# Basic Regression {#sec-regression}

```{r}
#| label: setup_regression
#| include: false

# for number learning checks
chap <- 5
lc <- 0

# `r paste0(chap, ".", (lc <- lc + 1))`

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  fig.align='center',
  warning = FALSE,
  message = FALSE
)

options(scipen = 99, digits = 3)

# In knitr::kable printing replace all NA's with blanks
options(knitr.kable.NA = '')

# Set random number generator see value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

Now that we are equipped with data visualization skills from @sec-viz, an understanding of the "tidy" data format from @sec-tidy, and data wrangling skills from @sec-wrangling, we now proceed with data modeling. The fundamental premise of data modeling is *to make explicit the relationship* between:

* an outcome variable $y$, also called a dependent variable and 
* an explanatory/predictor variable $x$, also called an independent variable or covariate. 

Another way to state this is using mathematical terminology: we will model the outcome variable $y$ *as a function* of the explanatory/predictor variable $x$. Why do we have two different labels, explanatory and predictor, for the variable $x$? That's because roughly speaking data modeling can be used for two purposes:

1. **Modeling for prediction**: You want to predict an outcome variable $y$ based on the information contained in a set of predictor variables. You don't care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about $y$, you're fine. For example, if we know many individuals' risk factors for lung cancer, such as smoking habits and age, can we predict whether or not they will develop lung cancer? Here we wouldn't care so much about distinguishing the degree to which the different risk factors contribute to lung cancer, but instead only on whether or not they could be put together to make reliable predictions.

1. **Modeling for explanation**: You want to explicitly describe the relationship between an outcome variable $y$ and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these. Continuing our example from above, we would now be interested in describing the individual effects of the different risk factors and quantifying the magnitude of these effects. One reason could be to design an intervention to reduce lung cancer cases in a population, such as targeting smokers of a specific age group with an advertisement for smoking cessation programs. In this book, we'll focus more on this latter purpose.

Data modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks and deep learning, and supervised learning. In this chapter, we'll focus on one particular technique: *linear regression*, one of the most commonly-used and easy-to-understand approaches to modeling. Recall our discussion in [Subsection -@sec-exploredataframes] on numerical and categorical variables. Linear regression involves:

* an outcome variable $y$ that is *numerical* and 
* explanatory variables $x_i$ (e.g. $x_1, x_2, ...$) that are either *numerical* or *categorical*.

With linear regression there is always only one numerical outcome variable $y$ but we have choices on both the number and the type of explanatory variables to use. We're going to cover the following regression scenarios:

* In this current chapter on basic regression, we'll always have only one explanatory variable.
    + In @sec-model1, this explanatory variable will be a single numerical explanatory variable $x$. This scenario is known as *simple linear regression*. 
    + In @sec-model2, this explanatory variable will be a categorical explanatory variable $x$.
* In the next chapter, @sec-multiple-regression on *multiple regression*, we'll have more than one explanatory variable:
    + We'll focus on two numerical explanatory variables, $x_1$ and $x_2$, in @sec-model3. 
    + We'll use one numerical and one categorical explanatory variable in @sec-model3. We'll also introduce *interaction models* here; there, the effect of one explanatory variable depends on the value of another. 

We'll study all four of these regression scenarios using real data, all easily accessible via R packages! 

## Packages Needed {.unnumbered}

Let's now load all the packages needed for this chapter (this assumes you've already installed them). In this chapter we introduce some new packages:

1. The `tidyverse` "umbrella" package. Recall from our discussion in @sec-tidyverse-package that loading the `tidyverse` package by running `library(tidyverse)` loads the following commonly used data science packages all at once:
    + `ggplot2` for data visualization
    + `dplyr` for data wrangling
    + `tidyr` for converting data to "tidy" format
    + `readr` for importing spreadsheet data into R
    + As well as the more advanced `purrr`, `tibble`, `stringr`, and `forcats` packages

1. The `skimr` [@R-skimr] package, which provides a simple-to-use function to quickly compute a wide array of commonly-used summary statistics.

1. The `gapminder` package, which provides excerpts of data available from [Gapminder.org](https://gapminder.org)

1. The `moderndive` package, which includes data sets we will analyze

If needed, read @sec-packages for information on how to install and load R packages.

```{r}
#| eval: false

library(tidyverse)
library(skimr)
library(gapminder)
library(moderndive)
```

```{r}
#| echo: false

library(tidyverse)
library(moderndive)

# DO NOT load the skimr package as a whole as it will break all kable() code for 
# the remaining chapters in the book.
# Furthermore all skimr::skim() output in this Chapter has been hard coded. 
# library(skimr)

library(gapminder)
```

```{r}
#| echo: false

# Packages needed internally, but not in text.
library(mvtnorm)
library(broom)
library(kableExtra)
library(patchwork)
```

## One numerical explanatory variable {#sec-model1}

Why do some professors and instructors at universities and colleges get high teaching evaluations from students while others don't? What factors can explain these differences? Are there biases? These are questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which professors and instructors should get promotions. Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer this question: what factors can explain differences in instructor's teaching evaluation scores? To this end, they collected information on $n = 463$ instructors. A full description of the study can be found at [openintro.org](https://www.openintro.org/stat/data/?data=evals).

We'll keep things simple for now and try to explain differences in instructor evaluation scores as a function of one numerical variable: their "beauty score." The specifics on how this score was calculated will be described shortly. 

Could it be that instructors with higher beauty scores also have higher teaching evaluations? Could it be instead that instructors with higher beauty scores tend to have lower teaching evaluations? Or could it be there is no relationship between beauty score and teaching evaluations? 

We'll achieve ways to address these questions by modeling the relationship between these two variables with a particular kind of linear regression called *simple linear regression*. Simple linear regression is the most basic form of linear regression. With it we have

1. A numerical outcome variable $y$. In this case, an instructor's teaching score.

1. A single numerical explanatory variable $x$. In this case, an instructor's beauty score.

### Exploratory data analysis {#sec-model1EDA}

A crucial step before doing any kind of modeling or analysis is performing an *exploratory data analysis*, or EDA, of all our data. Exploratory data analysis can give you a sense of the distribution of the data and whether there are outliers and/or missing values. Most importantly, it can inform how to build your model. There are many approaches to exploratory data analysis; here are three:

1. Most fundamentally: just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step!

1. Computing summary statistics like means, medians, and standard deviations.

1. Creating data visualizations.

Let's load the `evals` data (which is built into the `moderndive` package), `select` only a subset of the variables, and look at the raw values. Recall you can look at the raw values by running `View()` in the console in RStudio to pop-up the spreadsheet viewer with the data frame of interest as the argument to `View()`. Here, however, we present only a snapshot of five randomly chosen rows:

```{r}
evals_ch5 <- evals %>%
  select(score, bty_avg, age)
```

```{r}
#| eval: false

evals_ch5 %>% 
  slice_sample(n = 5)
```

```{r}
#| echo: false

set.seed(76)
evals_ch5 %>%
  slice_sample(n = 5) %>%
  knitr::kable(
    digits = 3,
    caption = "Random sample of 5 instructors",
    booktabs = TRUE,
    format = "markdown"
  ) %>% 
  kable_styling(
    font_size = ifelse(knitr:::is_latex_output(), 10, 16), 
                latex_options = c("HOLD_position")
    )
```

While a full description of each of these variables can be found at [openintro.org](https://www.openintro.org/stat/data/?data=evals), let's summarize what each of these variables represents.

1. `score`: Numerical variable of the average teaching score based on students' evaluations between 1 and 5. This is the outcome variable $y$ of interest.


1. `bty_avg`: Numerical variable of average "beauty" rating based on a panel of 6 students' scores between 1 and 10. This is the numerical explanatory variable $x$ of interest. Here 1 corresponds to a low beauty rating and 10 to a high beauty rating.

1. `age`: A numerical variable of age in years as an integer value.

An alternative way to look at the raw data values is by choosing a random sample of the rows in `evals_ch5` by piping it into the `slice_sample()` function from the `dplyr` package. Here we set the `n` argument to be `5`, indicating that we want a random sample of 5 rows. We display the results in @tbl-five-random-courses. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r}
#| eval: false

evals_ch5 %>%
  slice_sample(n = 5)
```

```{r}
#| label: tbl-five-random-courses
#| tbl-cap: "A random sample of 5 out of the 463 courses at UT Austin"
#| echo: false

evals_ch5 %>%
  slice_sample(n = 5) %>%
  knitr::kable(
    digits = 3,
    caption = "A random sample of 5 out of the 463 courses at UT Austin",
    booktabs = TRUE,
    format = "markdown"
  ) %>%
  kable_styling(
    font_size = ifelse(knitr:::is_latex_output(), 10, 16),
    latex_options = c("hold_position")
    )
```

Now that we've looked at the raw values in our `evals_ch5` data frame and got a preliminary sense of the data, let's move on to the next common step in an exploratory data analysis: computing summary statistics. Let's start by computing the mean and median of our numerical outcome variable `score` and our numerical explanatory variable "beauty" score denoted as `bty_avg`. We'll do this by using the `summarize()` function from `dplyr` along with the `mean()` and `median()` summary functions we saw in @sec-summarize.

```{r}
evals_ch5 %>%
  summarize(
    mean_bty_avg = mean(bty_avg),
    mean_score = mean(score),
    median_bty_avg = median(bty_avg), 
    median_score = median(score)
    )
```

However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? Typing out all these summary statistic functions in `summarize()` would be long and tedious. Instead, let's use the convenient `skim()` function from the `skimr` package. This function takes in a data frame, "skims" it, and returns commonly used summary statistics. Let's take our `evals_ch5` data frame, `select()` only the outcome and explanatory variables teaching `score` and `bty_avg`, and pipe them into the `skim()` function:


```{r}
#| eval: false
evals_ch5 %>%
  select(score, bty_avg) %>%
  skim()
```

```
── Data Summary ────────────────────────
                           Values    
Name                       Piped data
Number of rows             463       
Number of columns          2         
_______________________              
Column type frequency:               
  numeric                  2         
________________________             
Group variables            None      

── Variable type: numeric ──────────────────────────────────────────────────────
  skim_variable n_missing complete_rate mean    sd   p0  p25  p50 p75 p100 hist 
1 score                 0             1 4.17 0.544 2.3  3.8  4.3  4.6 5    ▁▁▅▇▇
2 bty_avg               0             1 4.42 1.53  1.67 3.17 4.33 5.5 8.17 ▃▇▇▃▂ 
```
