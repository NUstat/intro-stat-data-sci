# Exercise solutions {#sec-exr-sol}

```{r}
#| warning: false
#| message: false

library(ISDSdatasets)
library(tidyverse)
library(moderndive)
library(lubridate)
library(patchwork)
```

## Chapter 1 {#sec-ex01-sol}


@exr-ch01-c01 b. Quarto Document

@exr-ch01-c02 a. error

@exr-ch01-c03 a. TRUE

@exr-ch01-c04 a. TRUE

@exr-ch01-c05 b. FALSE

@exr-ch01-c06 e. 15

@exr-ch01-c07 b. Data on a flight

@exr-ch01-c08 c. quantitative

@exr-ch01-app1

```{r}
z <- 12*31
add_on <- 12
z + add_on
```

@exr-ch01-app2

```{r}
#| eval: false
glimpse(titanic)
```

The dataset has 418 rows (passengers) and 11 variables. The variables identify various passenger information such as name, age, sex, ticket class, number of siblings/spouses on board, fare cost, port the left from, and whether or not they survived.


@exr-ch01-adv1

```{r}
head(titanic)
```

The `head()` function shows the first 6 rows of the dataset. Based on this, I expect the `tail()` function to show the last 6 rows of the dataset.

@exr-ch01-adv2

```{r}
unique(titanic$Embarked)
```

There are 3 unique ports of embarkation: Q, S, C (Queenstown, Southampton, Cherbourg). 

## Chapter 2 {#sec-ex02-sol}

@exr-ch02-c01 b. geom_line(), c. geom_col(), e. geom_histogram()

@exr-ch02-c02 d. geom_point()

@exr-ch02-c03 b. changing the transparency, e. jittering the points

@exr-ch02-c04 b. When you want to split a particular visualization of variables by another variable

@exr-ch02-c05 b. geom_col()

@exr-ch02-c06 a. grouped boxplot

@exr-ch02-c07 b. linegraph

@exr-ch02-c08 c. scatterplot

@exr-ch02-c09 d. boxplot

@exr-ch02-c10 There is a strong positive non-linear (exponential) relationship. We can see by the blue line (line of best fit) that the data does not match a linear line. This tells us that as someone spends more time in the grocery store, they also spend more money.

@exr-ch02-c11 The histogram is unimodal and left skewed.

@exr-ch02-c12 a.

@exr-ch02-app1

```{r}
#| include: false
library(ggplot2)
library(dplyr)
library(ISDSdatasets)

covid_sub <- covid_states %>% 
  filter(
    state_abbr == "IL" | state_abbr == "FL",
    date >= "2021-07-01", date <= "2021-08-31",
    wday != "Sat", wday != "Sun"
    )
```

```{r}
ggplot(
  covid_sub, 
  aes(x = date, y = new_confirmed, color = state_abbr)
  ) +
  geom_line()
```

Florida has significantly more covid cases than Illinois in the months of July and August 2021. The spikes to 0 could indicate testing did not occur on those days. Illinois has a positive "fairly linear" trend. Florida has a nonlinear trend where cases are increasing until around August 15th, at which point cases start to decrease.

@exr-ch02-app2

```{r}
ggplot(nba, aes(x = factor(win)) ) +
  geom_bar() +
  facet_wrap(~ player)
```

A win is indicated with a "1". All three players have more wins than losses. LeBron has the most total wins and Jordan has the least total losses. But Jordan having less wins and losses is relative because he also played the least number of games.

@exr-ch02-app3

```{r}
ggplot(nba, aes(x = ft_percent, y = fg_percent)) +
  geom_jitter(alpha=0.1)
```

The relationship between the field goal percentage and free throw percentage seems to have no association. This means that whether a player made all of their free throws or only 50% of their free throws, it will not impact their shooting during the game (field goal percent). If we draw a circle around the points it is fairly horizontal (indicating no positive or negative trend) and a large oval. 

@exr-ch02-app4

```{r}
ggplot(nba, aes(x = player, y = pts)) +
  geom_boxplot() +
  facet_wrap(~ season)
```

```{r}
#Alternate way to visualize
ggplot(nba, aes(x = pts)) +
  geom_histogram() +
  facet_grid(player ~ season, scales = "free")
```

@exr-ch02-adv1

```{r}
ggplot(
  covid_sub, 
  aes(x = date, y = new_confirmed, color = state_abbr)
  ) +
  geom_line() +
  theme_minimal() +
  labs(x = NULL) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


## Chapter 3 {#sec-ex03-sol}

@exr-ch03-c01 c.  `x %>% c() %>% b() %>% a()`

@exr-ch03-c02 d. `arrange()` , g. `filter()` , c. `mutate()`

@exr-ch03-c03 a. 12 row and 5 columns

@exr-ch03-c04 b.  To make exploring a data frame easier by only outputting the variables of interest

@exr-ch03-c05 a.  increase

@exr-ch03-c06 c.  median and interquartile range; mean and standard deviation

@exr-ch03-c07 b.  $mean < median$

@exr-ch03-c08 a. These key variables uniquely identify the observational units

@exr-ch03-c09 d.

@exr-ch03-c10 d.

@exr-ch03-c11 a. e. (there is no variable called `passenger`)

@exr-ch03-app1

```{r}
nba %>%
  group_by(player) %>%
  summarize(avg_ft= mean(ft, na.rm=TRUE))
```

@exr-ch03-app2
```{r}
nba %>%
  group_by(player) %>%
  mutate(spread = pts_tm - pts_opp ) %>%
  ungroup() %>%
  slice_max(spread) %>% 
  select(player, pts_tm, pts_opp, spread)

# Alternate code
nba_spread <- nba %>%
  group_by(player) %>%
  mutate(spread = pts_tm - pts_opp ) %>% 
  select(player, pts_tm, pts_opp, spread)

nba_spread %>% arrange(desc(spread))
```
Kobe Bryant had the largest win spread, winning the game by 55 points.

@exr-ch03-app3
```{r}
titanic %>%
  group_by(Pclass) %>% 
  summarize(fare_calc= sum(Fare, na.rm=TRUE)) %>%
  arrange(desc(fare_calc))
```

@exr-ch03-app4

```{r}
covid_join <- covid_states %>%
 group_by(location_key, state) %>%
 summarize(total_new_confirmed=sum(new_confirmed, na.rm=TRUE),
           total_new_deceased=sum(new_deceased, na.rm=TRUE),
           total_new_recovered=sum(new_recovered, na.rm=TRUE),
           total_new_tested=sum(new_tested, na.rm=TRUE))

covid_joined <- covid_join %>%
  inner_join(covid_dem, by= "location_key")

covid_joined
```

@exr-ch03-adv1

```{r}
covid_pop <- covid_states %>% 
  filter(state_abbr %in% c("CA", "TX", "FL", "NY"))
```

@exr-ch03-adv2
```{r}
library(lubridate)
covid_confirmed <- covid_pop %>%
  filter(date<="2021-12-31", date>="2021-01-01") %>%
  mutate(week = week(date)) %>% 
  group_by(week, state) %>% 
  summarize(total_confirmed = sum(new_confirmed))

ggplot(data=covid_confirmed, 
       aes(x=week, y=total_confirmed, color=state)) +
  geom_line()
```

The linegraph shows that the states had the similar pattern overall. The peaks seem to correspond to weeks of holidays and school starting where people are gathering.


## Chapter 4 {#sec-ex04-sol}

::: callout-caution
## Under Construction

Currently working on exercise solutions.
:::



## Chapter 5 {#sec-ex05-sol}



@exr-ch05-c01 c) -0.7

@exr-ch05-c02 e) Exactly 1

@exr-ch05-c03 b) Between -1 and 0

@exr-ch05-c04 a) explanatory variable & d) independent variable

@exr-ch05-c05 b) predictor variable & c) outcome variable & e) dependent variable

@exr-ch05-c06 c) $b_0$ & e) the value of $\hat{y}$ when $x=0$ & f) intercept

@exr-ch05-c07 d) For every increase of 1 unit in x, there is an associated increase of, on average, 3.86 units of y.

@exr-ch05-c08 a) TRUE

@exr-ch05-c09 b) FALSE

@exr-ch05-c10 a) TRUE

@exr-ch05-c11 d) No, the positive correlation does not necessarily imply causation.


@exr-ch05-app1

**a)**

```{r}
#| eval: false
skim(covid_states)
```


**b)**

```{r}
covid_states %>% 
  select(new_tested, new_confirmed) %>% 
  cor(use = "complete.obs")
```
The correlation is 0.514.


**c)**

```{r}
ggplot(covid_states, aes(x = new_tested, y = new_confirmed)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

**d)**
```{r}
model_covid <- lm(new_confirmed ~ new_tested, data = covid_states)

summary(model_covid)$coefficients
```
$$\widehat{newconfirmed} = 320.16 + 0.057*newtested$$
**e)**

**Intercept**: When there are 0 people newly tested for COVID, we expect there to 320.16 newly confirmed cases.

**Slope:**: For every 1 additional person newly tested for COVID, we expect the number of new confirmed cases to increase by 0.057.


**f)**
```{r}
320.16 + 0.057*20000
```
We would expect 1,460 people to have COVID.

@exr-ch05-app2
```{r}
model_pts <- lm(pts_tm ~ pts, data = nba)

summary(model_pts)
```

**a)** 0.272
```{r}
nba %>% 
  summarize(cor = cor(pts_tm, pts, use = "complete.obs"))
```

**b)**
```{r}
ggplot(nba, aes(x = pts, y = pts_tm)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

**c)**
$$\widehat{pts tm} = 92.80 + 0.345*pts$$
**d)**

**Intercept**: When the individual player scored 0 points in a game, we expect their team to score 92.80 points.

**Slope:**: For every 1 additional point the individual player scores, we expect the number of total points the team scores to increase by 0.345.

**e)** We predict the team will have a total of 110 points.
```{r}
92.80 + 0.345*50
```


@exr-ch05-app3

```{r}
model_fare <- lm(Fare ~ Embarked, data = titanic)

summary(model_fare)$coefficients
```
**a)** $$\widehat{Fare} = 66.26 - 55.32*1_Q(x)-38.03*1_S(x)$$

**b)** The expected fare for passengers that embarked from Cherbourg (C) is 66.26.

**c)** On average, Cherbourg (C) had the highest ticket cost.

**d)**

```{r}
titanic_error <- titanic %>% 
  filter(!is.na(Fare)) %>% 
  mutate(residuals = residuals(model_fare),
         fitted = fitted.values(model_fare)) %>% 
  select(PassengerId, Survived, Fare, 
         Embarked, residuals, fitted)

titanic_error %>% 
  slice_max(residuals, n=1)
titanic_error %>% 
  slice_min(residuals, n=1)
```

The worst prediction will be the residual that is farthest from 0. We checked both the min (farthest negative) and max (farthest positive) residual. Passenger 1235 had the worst prediction with an observed value of 512.33 and predicted value of 66.26 (residual of 446.07).

@exr-ch05-adv1
```{r}
#| warning: false
#| message: false

# coefficients
summary(model_pts)$coefficients
# or
model_pts$coefficients

# r.squared
summary(model_pts)$r.squared
```


## Chapter 6 {#sec-ex06-sol}



@exr-ch06-c01 c. The parallel slopes model. Since two models are very similar, the additional complexity of the interaction model isnâ€™t necessary

@exr-ch06-c02 d. 0.47

@exr-ch06-c03 b. False

@exr-ch06-c04 a. True

@exr-ch06-c05 a. Splitting up your data can result in unequal balance in representation of some groups compared to others. & d. Splitting up your data by a confounding variable can allow you to see trends in the data that were hidden in the aggregated version of the data.


@exr-ch06-app1

```{r}
covid_pred <- lm(new_confirmed ~ new_recovered + new_deceased, data = covid_states)

summary(covid_pred)$coefficients
```

@exr-ch06-app2

```{r}
#| warning: false
#| message: false

covid_merge <- covid_states %>% 
  left_join(covid_dem)

covid_pred2 <- lm(new_confirmed ~ population + new_tested, 
                  data = covid_merge)

summary(covid_pred2)$coefficients
```


@exr-ch06-app3

```{r}
model_pts_parallel <- lm(pts_tm ~ pts + location, 
                   data = nba)

summary(model_pts_parallel)$coefficients

ggplot(nba, aes(x = pts, y = pts_tm, color = location)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE)
```

**b_0** When the star player scores 0 points and the game is away, the team is predicted to score 90.9 points.

**b_1** For every additional point the star player scores, the associated expected increase in team points is 0.345, regardless if the game is home or away.

**b_2** The team is expected to score on average 3.74 more points when the game is home compared to when the game is away.


@exr-ch06-app4

```{r}
#| include: false
model_pts_parallel <- lm(pts_tm ~ pts + location, 
                   data = nba)

summary(model_pts_parallel)$coefficients

ggplot(nba, aes(x = pts, y = pts_tm, color = location)) +
  geom_point() +
  geom_parallel_slopes(se = FALSE)
```


```{r}
#| warning: false
#| message: false

model_pts_int <- lm(pts_tm ~ pts*location, 
                   data = nba)

summary(model_pts_int)$coefficients

ggplot(nba, aes(x = pts, y = pts_tm, color = location)) +
  geom_point() +
  geom_smooth(se = FALSE)
```

**b_0** When the star player scores 0 points and the game is away, the team is predicted to score 88.57 points.

**b_1** For every additional point the star player scores, the associated expected increase in team points is 0.431 for away games.

**b_2** When the star player scores 0 points, the team is expected to score on average 8.37 more points when the game is home compared to when the game is away.

**b_3** For every additional point the star player scores, the associated expected increase in team points will be 0.169 less for home games than away games.

@exr-ch06-adv1

```{r}
#| warning: false
#| message: false

# parallel slopes
mean(model_pts_parallel$residuals^2)

# interaction
mean(model_pts_int$residuals^2)
```
The MSE of the parallel slopes is 136.6 and the MSE of the interaction is 135.9. Since the MSE for the interaction model is smaller it appears to be slightly better.

@exr-ch06-adv2

**Many different possible answers**

```{r}
#| warning: false
#| message: false

model_better <- lm(pts_tm ~ pts_opp + pts, data = nba)

mean(model_better$residuals^2)
```
This model which uses the opponents points and star player points has an MSE of 116.9 which is better.

## Chapter 7 {#sec-ex07-sol}


@exr-ch07-c01 a) There is a non-zero probability of being selected into the treatment or control group for every unit & c) A random process is used for selection & e) A random process is used for administration of the treatments

@exr-ch07-c02 d) rbernoulli(n = 1000, p = 0.25)

@exr-ch07-c03 b) False

@exr-ch07-c04 a) True

@exr-ch07-c05 a) Try to ensure that treatment and control groups are as similar as possible on all variables related to treatment assignment & c) Look for variables you can use to control for confounding & d) State your assumptions and limitations

@exr-ch07-c06 b) private colleges are only correlated with higher GPAs, because this would be an observational study

@exr-ch07-c07 b) video games are only correlated with violent behavior, because this would be an observational study

@exr-ch07-c08 Perhaps class size is a confounding variable for gpa, smaller sizes could lead to more individualized attention and higher grades. Perhaps parental control is a confounding variable for video games, children that play violent video games probably have less parental guidance or household rules leading to poor behavior choices.

@exr-ch07-c09 d) No, because the treatment and control groups were not randomized

@exr-ch07-c10 a confounding (or lurking) variable


@exr-ch07-app1 No the administration cannot conclude the after-school program caused student improvement. While this was a randomized selection of a subset of students we cannot generalize to all students because all students were not considered. Also, this was a before and after study where it is likely the material from the fall semester is likely different from the material in the spring semester. Perhaps these students were just better at the topics covered in the spring.


@exr-ch07-app2 Geography might impact the results because maybe the west has more rural cities than the east or perhaps there are different demographics of people that live in each region. Certain types of people/demographics might favor the 'traditional' label and deter from the new label because they don't recognize it while other types of people will see the new label and but it because it is 'new'. A way to reduce the impact of geography is to use "matching". Find a list of cities in the east that match on average with the cities in the west (perhaps New York City is very similar on average to Los Angeles etc.). Then randomly sample these pairs of cities to compare sales results (to measure if it is receptive).


## Chapter 8 {#sec-ex08-sol}


@exr-ch08-c01 d) a population parameter

@exr-ch08-c02 
b) $\hat{\mu}$ & d) $\bar{x}$ & f) $\hat{\pi}$ & g) $p$ & h) $\hat{p}$ & i) $s$ & k) $\hat{\sigma}$  

<!-- @exr-ch08-c03  -->
<!-- a) $\mu$ &  e) $\pi$ & j) $\sigma$ -->

@exr-ch08-c03 all of them (a, b, c, d)

@exr-ch08-c04 b) Cluster sampling

@exr-ch08-c05 d) Systematic sampling

@exr-ch08-c06 c) Stratified sampling

@exr-ch08-c07 c) Cluster sampling (with unequal probability) choosing towns is based on random cluster selection

@exr-ch08-c08 b) False

@exr-ch08-c09 c) An observational study with random sampling but no random assignment

@exr-ch08-c10 d) No, you cannot make causal or generalizable claims from the results of your survey


@exr-ch08-app1
population: US citizens

parameter: proportion (most likely a Yes or No question)

undercoverage: citizens that do not own a house, if there are multiple citizens in one household only one person will receive the survey.


@exr-ch08-app2

sampling method: stratified sampling

Compared to simple random sampling, stratified sample is guaranteed to represent people from all 50 states. 

While the stratified sampling would allow for better representation of people in different states, the same limitations in regards to citizens without addresses or if multiple citizens live in one household.


## Chapter 9 {#sec-ex09-sol}


@exr-ch09-c01
b) unimodal & h) symmetric

@exr-ch09-c02
b) False

@exr-ch09-c03
c) `1 - pnorm(q = 60, mean = 64, sd = 3, lower.tail = FALSE)`

@exr-ch09-c04
b) `pnorm(q = 72, mean = 64, sd = 3) - pnorm(q = 60, mean = 64, sd = 3)`
c) `1 - pnorm(q = -1.33) - pnorm(q = 2.67, lower.tail = FALSE)`

@exr-ch09-c05
a) apple

@exr-ch09-c06
a) True

@exr-ch09-c07

d) The sampling distribution of the sample mean and the sampling distribution of the difference in sample means both follow the T distribution

f) The regression slope and regression intercept both follow the T distribution

@exr-ch09-c08
a) True

@exr-ch09-c09
b) False

@exr-ch09-c10
d) unbiased and precise

@exr-ch09-c11

b) False

@exr-ch09-c12

- normal/t distribution
- normal/t distribution
- chi-squared distribution


@exr-ch09-c13

a) 99.7% (3 standard deviations)

b)
```{r}
pnorm(q = 13, mean = 10.5, sd = 1.5, 
      lower.tail = FALSE)
```
4.78% of men have a shoe size larger than 13.

c) 
```{r}
pnorm(q = 12, mean = 10.5, sd = 1.5) - pnorm(q = 10, mean = 10.5, sd = 1.5)
```
47.19% of males have a shoe size between 10 and 12. So assuming this is a random male where everyone has an equal chance of selection there is a 47.19% chance.

d) 
```{r}
qnorm(p = 0.6, mean = 10.5, sd = 1.5, lower.tail = FALSE)
```
His shoe size is 10.12 (which is not an actual shoe size so his shoe size would be 10)

@exr-ch09-c15

```{r}
weather <- tibble(
  daily_weather = c(rep("sunny",476), 
                    rep("cloudy",558), 
                    rep("partly cloudy",487), 
                    rep("rainy",312), 
                    rep("thuderstorms",28), 
                    rep("snowy",329))
  )
```

```{r}
set.seed(52)
samples_1 <- weather %>% 
  rep_sample_n(size = 30, reps = 50)

samples_2 <- weather %>% 
  rep_sample_n(size = 30, reps = 5000)

samples_3 <- weather %>% 
  rep_sample_n(size = 50, reps = 5000)
```

```{r}
summary_1 <- samples_1 %>% 
  group_by(replicate) %>% 
  summarize(sunny = sum(daily_weather == "sunny"),
            prop = sunny/n())

summary_2 <- samples_2 %>% 
  group_by(replicate) %>% 
  summarize(sunny = sum(daily_weather == "sunny"),
            prop = sunny/n())

summary_3 <- samples_3 %>% 
  group_by(replicate) %>% 
  summarize(sunny = sum(daily_weather == "sunny"),
            prop = sunny/n())
```

```{r}
plot_1 <- ggplot(summary_1, aes(x = prop)) +
  geom_histogram(color = "white", bins = 5)

plot_2 <- ggplot(summary_2, aes(x = prop)) +
  geom_histogram(color = "white", bins = 17)

plot_3 <- ggplot(summary_3, aes(x = prop)) +
  geom_histogram(color = "white", bins = 22)

plot_1 + plot_2 + plot_3
```

## Chapter 10 {#sec-ex10-sol}

@exr-ch10-c01
Estimate $\\pm$ Critical Value*SE(Estimate)

@exr-ch10-c03
b) False

@exr-ch10-c05
a) decrease

@exr-ch10-c07
d) qt(p = 0.05, df = 14)

@exr-ch10-c09
0.01804

@exr-ch10-app2

Let's consider only the games that Kobe Bryant **played in**.

```{r}
nba_sample %>% 
  filter(!is.na(gs)) %>% 
  count(win, player)

prop.test(x = 56, n = (56 + 33), conf.level = 0.99, correct = FALSE)
```
We are 99% confident that the proportion of career wins for Kobe Bryant is between 0.493 and 0.748.

@exr-ch10-app4

```{r}
nba_mj <- nba_sample %>% 
  filter(player == "Michael Jordan")

nba_kobe <- nba_sample %>% 
  filter(player == "Kobe Bryant")

t.test(x = nba_mj$ft_percent, y = nba_kobe$ft_percent, conf.level = 0.95)
```

We are 95% confident that the difference in free throw percentage between Michael Jordan and Kobe Bryant is between -0.0155 and 0.071.

@exr-ch10-adv2

```{r}
# 2
nba %>% 
  filter(!is.na(gs), player == "Kobe Bryant") %>% 
  count(win)
971/(971+595)
```
Yes, 0.620 is between our 99% CI [0.493, 0.748]



```{r}
# 4
nba %>% 
  group_by(player) %>% 
  summarize(mean = mean(ft_percent, na.rm = TRUE))

0.8279-0.8286
```
Yes, -0.0007 is between our 95% CI [-0.0155, 0.071]

## Chapter 11 {#sec-ex11-sol}

::: callout-caution
## Under Construction

Currently working on exercise solutions.
:::


## Chapter 12 {#sec-ex12-sol}


@exr-ch12-c01
a) Type I Error


@exr-ch12-c02
d) No, one type of error will be minimized at the expense of the other type

@exr-ch12-c03
b) False

@exr-ch12-c04
a) True

@exr-ch12-c05
a) $\alpha$
b) $n$

@exr-ch12-c06
b) Type II Error

@exr-ch12-c07


@exr-ch12-c08
@exr-ch12-c09


@exr-ch12-c10