[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistics and Data Science",
    "section": "",
    "text": "This is the website for Introduction to Statistics and Data Science. This book starts you down the path of learning how to think with data using R. You’ll learn the basics of how to engage, explore, and examine many types of data arising from several contexts. Hopefully you’ll have fun and see how valuable it is to be able to critically think with data.\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that this is a “development version” of this book for the new design of STAT 202. Meaning this is a work in progress being edited and updated as we go.\nWe would appreciate any feedback on typos and errors.\n\n\nThis open textbook is produced with support from Northwestern University Libraries and The Alumnae of Northwestern University."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Statistics and Data Science",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Zero v1.0 Universal License. If you’d like to give back, please consider reporting a typo or leaving a pull request at github.com/NUstat/intro-stat-data-sci."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Help! I’m new to R and RStudio and I need to learn about them! However, I’m completely new to coding! What do I do?\nIf you’re asking yourself this question, then you’ve come to the right place! Start with our “Introduction for Students”."
  },
  {
    "objectID": "preface.html#introduction-for-students",
    "href": "preface.html#introduction-for-students",
    "title": "Preface",
    "section": "Introduction for students",
    "text": "Introduction for students\nThis book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way statisticians, data scientists, data journalists, and other researchers would.\nIn Figure 1 we present a flowchart of what you’ll cover in this book. You’ll first get started with data in Chapter 1, where you’ll learn about the difference between R and RStudio, start coding in R, understand what R packages are, and explore your first dataset: all domestic departure flights from a New York City airport in 2013. Then\n\nData Exploration: You’ll assemble your data science toolbox using tidyverse packages. In particular:\n\nCh. 2: Visualizing data via the ggplot2 package.\nCh. 3: Wrangling data via the dplyr package.\nCh. 4: Understanding the concept of “tidy” data as a standardized data input format for all packages in the tidyverse\n\nData Modeling: Using these data science tools, you’ll start performing data modeling. In particular:\n\nCh. 5: Constructing basic regression models.\nCh. 6: Constructing multiple regression models.\n\nStatistical Theory: Now you’ll learn about the role of randomization in making inferences and the general frameworks used to make inferences in statistics. In particular:\n\nCh. 7: Randomization and causality.\nCh. 8: Populations and generalizability.\nCh. 9: Sampling distributions.\n\nStatistical Inference: You’ll learn to combine your newly acquired data analysis and modeling skills with statistical theory to make inferences. In particular:\n\nCh. 10: Building confidence intervals.\nCh. 11: Calculating p-values.\nCh. 12: Conducting hypothesis tests.\n\n\n\n\n\nFigure 1: Course Flowchart\n\n\n\nWhat you will learn from this book\nWe hope that by the end of this book, you’ll have learned\n\nHow to use R to explore data.\nHow to generate research questions and hypotheses.\nHow to think like a statistician and the role of chance in your data.\nHow to answer statistical questions using tools like confidence intervals and hypothesis tests.\nHow to effectively create “data stories” using these tools.\n\nWhat do we mean by data stories? We mean any analysis involving data that engages the reader in answering questions with careful visuals and thoughtful discussion, such as How strong is the relationship between per capita income and crime in Chicago neighborhoods? and How many f**ks does Quentin Tarantino give (as measured by the amount of swearing in his films)?. Further discussions on data stories can be found in this Think With Google article.\nFor other examples of data stories constructed by students like yourselves, look at the final projects for two courses that have previously used a version of this book:\n\nMiddlebury College MATH 116 Introduction to Statistical and Data Sciences using student collected data.\nPacific University SOC 301 Social Statistics using data from the fivethirtyeight R package.\n\nThis book will help you develop your “data science toolbox”, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. With these tools, you’ll be able to perform the entirety of the “data/science pipeline” while building data communication skills.\nIn particular, this book will lean heavily on data visualization. In today’s world, we are bombarded with graphics that attempt to convey ideas. We will explore what makes a good graphic and what the standard ways are to convey relationships with data. You’ll also see the use of visualization to introduce concepts like mean, median, standard deviation, distributions, etc. In general, we’ll use visualization as a way of building almost all of the ideas in this book.\nTo impart the statistical lessons in this book, we have intentionally minimized the number of mathematical formulas used and instead have focused on developing a conceptual understanding via data visualization, statistical computing, and simulations. We hope this is a more intuitive experience than the way statistics has traditionally been taught in the past and how it is commonly perceived.\nFinally, you’ll learn the importance of literate programming. By this we mean you’ll learn how to write code that is useful not just for a computer to execute but also for readers to understand exactly what your analysis is doing and how you did it. This is part of a greater effort to encourage reproducible research (see subsection Reproducible research for more details). Hal Abelson coined the phrase that we will follow throughout this book:\n\n“Programs must be written for people to read, and only incidentally for machines to execute.”\n\nWe understand that there may be challenging moments as you learn to program. We still continue to struggle and find ourselves often using web searches to find answers and reach out to colleagues for help. In the long run though, we all can solve problems faster and more elegantly via programming. We wrote this book as our way to help you get started and you should know that there is a huge community of R users that are always happy to help everyone along as well. This community exists in particular on the internet on various forums and websites such as stackoverflow.com.\n\n\nData/science pipeline\nYou may think of statistics as just being a bunch of numbers. We commonly hear the phrase “statistician” when listening to broadcasts of sporting events. Statistics (in particular, data analysis), in addition to describing numbers like with baseball batting averages, plays a vital role in all of the sciences. You’ll commonly hear the phrase “statistically significant” thrown around in the media. You’ll see articles that say “Science now shows that chocolate is good for you.” Underpinning these claims is data analysis and a theoretical model relating the data collected in a sample to a larger population. By the end of this book, you’ll be able to better understand whether these claims should be trusted or whether we should be wary. Inside data analysis are many sub-fields that we will discuss throughout this book (though not necessarily in this order):\n\ndata collection\ndata wrangling\ndata visualization\ndata modeling\nstatistical inference\ncorrelation and regression\ninterpretation of results\ndata communication/storytelling\n\nThese sub-fields are summarized in what Grolemund and Wickham term the “Data/Science Pipeline” in Figure 2.\n\n\n\nFigure 2: Data/Science Pipeline\n\n\nWe will begin by digging into the gray Understand portion of the cycle with data visualization, then with a discussion on what is meant by tidy data and data wrangling, and then conclude by talking about interpreting and discussing the results of our models via Communication. These steps are vital to any statistical analysis. But why should you care about statistics? “Why did they make me take this class?”\nThere’s a reason so many fields require a statistics course. Scientific knowledge grows through an understanding of statistical significance and data analysis. You needn’t be intimidated by statistics. It’s not the beast that it used to be and, paired with computation, you’ll see how reproducible research in the sciences particularly increases scientific knowledge.\n\n\nReproducible research\n\n“The most important tool is the mindset, when starting, that the end product will be reproducible.” – Keith Baggerly\n\nAnother goal of this book is to help readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their analyses reproducible from the very beginning. This means we’ll be trying to help you build new habits. This will take practice and be difficult at times. You’ll see just why it is so important for you to keep track of your code and well-document it to help yourself later and any potential collaborators as well.\nCopying and pasting results from one program into a word processor is not the way that efficient and effective scientific research is conducted. It’s much more important for time to be spent on data collection and data analysis and not on copying and pasting plots back and forth across a variety of programs.\nIn a traditional analysis if an error was made with the original data, we’d need to step through the entire process again: recreate the plots and copy and paste all of the new plots and our statistical analysis into your document. This is error prone and a frustrating use of time. We’ll see how to use R Markdown to get away from this tedious activity so that we can spend more time doing science.\n\n“We are talking about computational reproducibility.” - Yihui Xie\n\nReproducibility means a lot of things in terms of different scientific fields. Are experiments conducted in a way that another researcher could follow the steps and get similar results? In this book, we will focus on what is known as computational reproducibility. This refers to being able to pass all of one’s data analysis, data-sets, and conclusions to someone else and have them get exactly the same results on their machine. This allows for time to be spent interpreting results and considering assumptions instead of the more error prone way of starting from scratch or following a list of steps that may be different from machine to machine."
  },
  {
    "objectID": "01-getting-started.html",
    "href": "01-getting-started.html",
    "title": "1  Getting Started with Data in R",
    "section": "",
    "text": "Before we can start exploring data in R, there are some key concepts to understand first:\nWe’ll introduce these concepts in upcoming Sections 1.1 - 1.3 If you are already somewhat familiar with these concepts, feel free to skip to Section 1.4 where we’ll introduce our first data set: all domestic flights departing a New York City airport in 2013. This is a dataset we will explore in depth in this book."
  },
  {
    "objectID": "01-getting-started.html#sec-r-rstudio",
    "href": "01-getting-started.html#sec-r-rstudio",
    "title": "1  Getting Started with Data in R",
    "section": "1.1 What are R and RStudio?",
    "text": "1.1 What are R and RStudio?\nFor much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest:\n\nR is like a car’s engine.\nRStudio is like a car’s dashboard.\n\n\n\n\n\n\n\n\nR: Engine\nRStudio: Dashboard\n\n\n\n\n\n\n\n\n\nMore precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\n\n1.1.1 Using RStudio Cloud\nRStudio Cloud (https://rstudio.cloud) is a hosted version of RStudio that allows you to begin coding directly from your browser - there is no software to install and nothing to configure on your computer.\nTo begin using RStudio Cloud use the link provided by your instructor to gain access to the classroom workspace. You will be prompted to create a free account or log in if you have an existing account.\nAfter you open RStudio Cloud, you should now have access to the classroom under ‘Spaces’ on the left hand side (in this case ‘STAT202’).\n\nThroughout this course you will be working on various activities. Once the instructor has made an activity available you will click on the classroom Workspace (STAT202) to access the available projects. To begin working on an activity click ‘Start’. Once that activity project is open navigate to the ‘File’ pane and open the Quarto ‘.qmd’ file.\n\nYou can use RStudio Cloud for personal use as well by creating projects in ‘Your Workspace’. However, RStudio Cloud limits the number of projects and amount of accessible time so it is recommended that you later install the software on your own computer.\n\n\n1.1.2 Installing R and RStudio on your personal computer\n\nNote about RStudio Server or RStudio Cloud: If your instructor has provided you with a link and access to RStudio Server or RStudio Cloud, then you can skip this section. We do recommend after a few months of working on RStudio Server/Cloud that you return to these instructions to install this software on your own computer though. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio second.\n\n\nYou must do this first: Download and install R.\n\nIf you are a Windows user: Click on “Download R for Windows”, then click on “base”, then click on the Download link.\nIf you are macOS user: Click on “Download R for (Mac) OS X”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of August 10, 2019 was R-3.6.1.\n\nYou must do this second: Download and install RStudio.\n\nScroll down to “Installers for Supported Platforms” near the bottom of the page.\nClick on the download link corresponding to your computer’s operating system.\n\n\n\n\n1.1.3 Using R via RStudio\nRecall our car analogy from above. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs AKA applications you can open. We will always work in RStudio and not R. In other words:\n\n\n\n\n\n\n\nR: Do not open this\nRStudio: Open this\n\n\n\n\n\n\n\n\n\nAfter you open RStudio, you should see the following:\n\nNote the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, you’ll come to learn what purpose each of these panes serve."
  },
  {
    "objectID": "01-getting-started.html#sec-code",
    "href": "01-getting-started.html#sec-code",
    "title": "1  Getting Started with Data in R",
    "section": "1.2 How do I code in R?",
    "text": "1.2 How do I code in R?\nNow that you’re set up with R and RStudio, you are probably asking yourself “OK. Now how do I use R?” The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book.\nWhile it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively.\n\n1.2.1 Creating your first Quarto document\nQuarto allows you to easily create a document which combines your code, the results from your code, as well as any text that accompanies the analysis. To create a new Quarto file, in RStudio select File>New File>Quarto Document. Then, you will see a window pop-up titled New Quarto Document. Here, you specify the type of file you wish to create. HTML is generally the recommended document type since it does not have traditional page separators like PDF and Word do. You can also choose a title and author for your document using their respective fields. Finally, select Create to create your new Quarto file. You will see it appear as a tab in your RStudio session. Click the save icon to save your new document.\nThe following is an example of a Quarto document:\n\n\nSave your document.\nClick Render to compile your Quarto document into the file type that you specified. The file will be saved in your Files pane. This will also save your document.\nInsert a new code chunk in your document where the cursor is located. You will often have many code chunks in your document.\nRun the current code chunk.\n\nWhen you create your Quarto file and Render it into a document, the chunks are run in order and any output from them is shown in the document, in the order and location that their respective chunk appears. Sometimes you may wish to type code or analyze data without it printing in the document. If that is the case, you type the code in the Console rather than in the .qmd file.\nWhile you read through this book, it will be helpful to have a Quarto document open so you can copy code provided and paste it into a code chunk to run.\n\n\n1.2.2 Basic programming concepts and terminology\nWe now introduce some basic programming concepts and terminology. Instead of asking you to learn all these concepts and terminology right now, we’ll guide you so that you’ll “learn by doing.” Note that in this book we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, “learning by doing” and lots of repetition.\n\nBasics:\n\nConsole: Where you enter in commands. \nRunning code: The act of telling R to perform an action by giving it commands in the console.\nObjects: Where values are saved in R. In order to do useful and interesting things in R, we will want to assign a name to an object. For example we could do the following assignments: x <- 44 - 20 and three <- 3. This would allow us to run x + three which would return 27.\nData types: Integers, doubles/numerics, logicals, and characters.\n\n\nIn RStudio try typing the following code into the console or code chunk.\n\nx <- 44-20\nthree <- 3\nx+three\n\n[1] 27\n\n\nYou should see x and three appear as stored objects in the Environment pane. Anything you store in the Environment pane can be referenced and used later. R can also be used as a calculator, notice how it evaluates x+three.\n\nVectors: A series of values. These are created using the c() function, where c() stands for “combine” or “concatenate”. For example: c(6, 11, 13, 31, 90, 92).\nFactors: Categorical data are represented in R as factors.\nData frames: Data frames are like rectangular spreadsheets: they are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section Section 1.4.\nConditionals:\n\nTesting for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error.\nBoolean algebra: TRUE/FALSE statements and mathematical operators such as < (less than), <= (less than or equal), and != (not equal to).\nLogical operators: & representing “and” as well as | representing “or.” Ex: (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE.\n\nFunctions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values.\n\nThis list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. We feel that you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.\n\n\n1.2.3 Errors, warnings, and messages\nOne thing that intimidates new R and RStudio users is how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad.\nR will show red text in the console pane in three different situations:\n\nErrors: When the red text is a legitimate error, it will be prefaced with “Error in…” and try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 1.3.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first.\nWarnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter 2 if you create a scatterplot based on a dataset where one of the values is missing, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining values, but it is warning you that one of the points isn’t there.\nMessages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 1.3.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Chapter 4. These are helpful diagnostic messages and they don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages().\n\nRemember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather:\n\nIf the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong!\nIf the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention.\nOtherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine.\n\n\n\n1.2.4 Tips on learning to code\nLearning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn.\nHere are a few useful tips to keep in mind as you learn to program:\n\nRemember that computers are not actually that smart: You may think your computer or smartphone are “smart,” but really people spent a lot of time and energy designing them to appear “smart.” Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way.\nTake the “copy, paste, and tweak” approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Don’t be afraid to play around!\nThe best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in.\nPractice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Don’t worry however; we’ll give you plenty of opportunities to do so!"
  },
  {
    "objectID": "01-getting-started.html#sec-packages",
    "href": "01-getting-started.html#sec-packages",
    "title": "1  Getting Started with Data in R",
    "section": "1.3 What are R packages?",
    "text": "1.3 What are R packages?\nAnother point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization in Chapter 2, the dplyr package for data wrangling in Chapter 3, and the moderndive package that accompanies this book.\nA good analogy for R packages is they are like apps you can download onto a mobile phone:\n\n\n\n\n\n\n\nR: A new phone\nR Packages: Apps you can download\n\n\n\n\n\n\n\n\n\nSo R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play.\nLet’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to:\n\nInstall the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set. You might do this again in the future any time there is an update to the app.\nOpen the app: After you’ve installed Instagram, you need to open the app.\n\nOnce Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to:\n\nInstall the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version.\n“Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio.\n\nLet’s now show you how to perform these two steps for the ggplot2 package for data visualization.\n\n1.3.1 Package installation\n\nNote about RStudio Server/Cloud: If your instructor has provided you with a link and access to RStudio Server/Cloud, you probably will not need to install packages, as they have likely been pre-installed for you by your instructor. That being said, it is still a good idea to know this process for later on when you are not using RStudio Server/Cloud, but rather RStudio Desktop on your own computer.\n\nThere are two ways to install an R package. For example, to install the ggplot2 package:\n\nEasy way: In the Files pane of RStudio:\n\nClick on the “Packages” tab\nClick on “Install”\nType the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2\nClick “Install”\n\n\nSlightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the Console pane of RStudio and hitting enter. Note you must include the quotation marks.\n\nMuch like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps.\n\n\n\n\n\n\n🎯 Learning Check 1.1\n\n\n\n\n\nRepeat the above installing steps for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for writing reports in R.\n\n\n\n\n\n1.3.2 Package loading\nRecall that after you’ve installed a package, you need to “load” it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by “run the following code”? Either type or copy & paste the following code into the Console pane and then hit the enter key.\n\nlibrary(ggplot2)\n\nIf after running the above code, a blinking cursor returns next to the > “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red “error message” that reads…\nError in library(ggplot2) : there is no package called ‘ggplot2’\n… it means that you didn’t successfully install it. In that case, go back to the previous subsection “Package installation” and install it.\n\n\n\n\n\n\n🎯 Learning Check 1.2\n\n\n\n\n\n“Load” the dplyr, nycflights13, and knitr packages as well by repeating the above steps.\n\n\n\n\n\n1.3.3 Package use\nOne extremely common mistake new R users make when wanting to use particular packages is that they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to:\nError: could not find function\nR is telling you that you are trying to use a function in a package that has not yet been “loaded.” Almost all new users forget do this when starting out, and it is a little annoying to get used to. However, you’ll remember with practice."
  },
  {
    "objectID": "01-getting-started.html#sec-nycflights13",
    "href": "01-getting-started.html#sec-nycflights13",
    "title": "1  Getting Started with Data in R",
    "section": "1.4 Explore your first dataset",
    "text": "1.4 Explore your first dataset\nLet’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format; this is probably the most common way data are collected and saved in many fields. Remember from Subsection 1.2.2 that these “spreadsheet”-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this book.\nLet’s first load all the packages needed for this chapter, assuming you’ve already installed them. Read Section 1.3 for information on how to install and load R packages if you haven’t already.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(knitr)\n\nAt the beginning of all subsequent chapters in this text, we’ll always have a list of packages that you should have installed and loaded to work with that chapter’s R code.\n\n1.4.1 nycflights13 package\nMany of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect in many people’s lives. If you live in or are visiting a relatively large city and you walk around that city’s airport, you see gates showing flight information from many different airlines. And you will frequently see that some flights are delayed because of a variety of conditions. Are there ways that we can avoid having to deal with these flight delays?\nWe’d all like to arrive at our destinations on time whenever possible. (Unless you secretly love hanging out at airports. If you are one of these people, pretend for the moment that you are very much anticipating being at your final destination.) Throughout this book, we’re going to analyze data related to flights contained in the nycflights13 package (Wickham 2021). Specifically, this package contains five data sets saved in five separate data frames with information about all domestic flights departing from New York City in 2013. These include Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia (LGA) airports:\n\nflights: Information on all 336,776 flights\nairlines: A table matching airline names and their two letter IATA airline codes (also known as carrier codes) for 16 airline companies\nplanes: Information about each of 3,322 physical aircraft used.\nweather: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughtly corresponding to the 365 \\(\\times\\) 24 \\(\\times\\) 3 = 26,280 possible hourly measurements one can observe at three locations over the course of a year.\nairports: Airport names, codes, and locations for 1,458 destination airports.\n\n\n\n1.4.2 flights data frame\nWe will begin by exploring the flights data frame that is included in the nycflights13 package and getting an idea of its structure. Run the following code in your console (either by typing it or cutting & pasting it): it loads in the flights dataset into your Console. Note depending on the size of your monitor, the output may vary slightly.\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nLet’s unpack this output:\n\nA tibble: 336,776 x 19: A tibble is a kind of data frame used in R. This particular data frame has\n\n336,776 rows\n19 columns corresponding to 19 variables describing each observation\n\nyear month day dep_time sched_dep_time dep_delay arr_time are different columns, in other words variables, of this data frame.\nWe then have the first 10 rows of observations corresponding to 10 flights.\n... with 336,766 more rows, and 11 more variables: indicating to us that 336,766 more rows of data and 11 more variables could not fit in this screen.\n\nUnfortunately, this output does not allow us to explore the data very well. Let’s look at different tools to explore data frames.\n\n\n1.4.3 Exploring data frames\nAmong the many ways of getting a feel for the data contained in a data frame such as flights, we present three functions that take as their “argument”, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame:\n\nUsing the View() function built for use in RStudio. We will use this the most.\nUsing the glimpse() function, which is included in the dplyr package.\nUsing the kable() function, which is included in the knitr package.\nUsing the $ operator to view a single variable in a data frame.\n\n1. View():\nRun View(flights) in your Console in RStudio, either by typing it or cutting & pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital “V” in View. R is case-sensitive so you’ll receive an error is you run view(flights) instead of View(flights).\n\n\n\n\n\n\n🎯 Learning Check 1.3\n\n\n\n\n\nWhat does any ONE row in this flights dataset refer to?\n\nData on an airline\nData on a flight\nData on an airport\nData on multiple flights\n\n\n\n\nBy running View(flights), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like distance, day, and arr_delay are what we will call quantitative variables. These variables are numerical in nature. Other variables here are categorical.\nNote that if you look in the leftmost column of the View(flights) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. In other words, this will allow you to identify what object is being referred to in a given row. This is often called the observational unit. The observational unit in this example is an individual flight departing New York City in 2013. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables.\n2. glimpse():\nThe second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame:\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nWe see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type (see Subsection 1.2.2) of the variable is given immediately after each variable’s name inside < >. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to “character”, which is computer terminology for text data. Text data, such as the carrier or origin of a flight, are categorical variables. The time_hour variable is an example of one more type of data type: dttm. As you may suspect, this variable corresponds to a specific date and time of day. However, we won’t work with dates in this class and leave it to a more advanced book on data science.\n\n\n\n\n\n\n🎯 Learning Check 1.4\n\n\n\n\n\nWhat are some examples in this dataset of categorical variables? What makes them different than quantitative variables?\n\n\n\n3. kable():\nThe another way to explore the entirety of a data frame is using the kable() function from the knitr package. Let’s explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in your Console:\n\nairlines\nkable(airlines)\n\nAt first glance, it may not appear that there is much difference in the outputs. However when using tools for document production such as Quarto, the latter code produces output that is much more legible and reader-friendly.\n4. $ operator\nLastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console\n\nairlines\nairlines$name\n\nWe used the $ operator to extract only the name variable and return it as a vector of length 16. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions.\n\n\n1.4.4 Help files\nAnother nice feature of R is the help system. You can get help in R by entering a ? before the name of a function or data frame in question and you will be presented with a page showing the documentation. For example, let’s look at the help file for the flights data frame:\n\n?flights\n\nA help file should pop-up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away."
  },
  {
    "objectID": "01-getting-started.html#sec-gs-conclusion",
    "href": "01-getting-started.html#sec-gs-conclusion",
    "title": "1  Getting Started with Data in R",
    "section": "1.5 Conclusion",
    "text": "1.5 Conclusion\nWe’ve given you what we feel are the most essential concepts to know before you can start exploring data in R. Is this chapter exhaustive? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful!\n\n1.5.1 Additional resources\nIf you are completely new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out Chester Ismay’s short book Getting used to R, RStudio, and R Markdown (Ismay 2016), which includes screencast recordings that you can follow along and pause as you learn. While this book teaches R Markdown it it important to note that everything in R Markdown is transferable to Quarto. R Markdown and Quarto are both tools used for reproducible research but R Markdown is fundamentally tied to R while Quarto is a multi-language platform. For a getting started guide on Quarto, we suggest the Quarto Getting Started webpage"
  },
  {
    "objectID": "01-getting-started.html#sec-pp01",
    "href": "01-getting-started.html#sec-pp01",
    "title": "1  Getting Started with Data in R",
    "section": "1.6 Practice Problems",
    "text": "1.6 Practice Problems\n\n\n\n\n\n\nConcept\n\n\n\n\n\n\n\nWhich type of document do we use to both code and write explanations?\n\nR Script\nQuarto Document\nHTML file\nR Notebook\n\n\n\n\nWhich type of red text in the console pane generally means that your code will not run?\n\nerror\nwarning\nmessage\n\nIf you place the operator ? before the name of a function or data frame, then you will be presented with a page showing the documentation for the respective function or data frame.\n\nTRUE\nFALSE\n\nIf you type \\(8/2 == 4\\) into the console, what will the output be?\n\nTRUE\nFALSE\nNA\n0\n4\n\nIf you type \\(3^2 != 9\\) into the console, what will the output be?\n\nTRUE\nFALSE\nNA\n0\n9\n\nIf you type \\(5*3\\) into the console, what will the output be?\n\nTRUE\nFALSE\nNA\n8\n15\n\nWhat does any ONE row in this flights dataset refer to?\n\nData on an airline\nData on a flight\nData on an airport\nData on multiple flights\n\nIn the flights dataset, air_time and arr_delay are which type of variables?\n\nstring\ncategorical\nquantitative\ncharacter\ndataframe\n\n\n\n\n\n\n\n\nApplication\n\n\n\n\n\n\n\nIn a code chunk, first define a variable z to be the product of 12 and 31, then define a variable called add_on to be the number 12. Print the output of z + add_on.\nConsider the titanic data set included in the package ISDSdatasets. This is one of the most popular data sets used for understanding machine learning basics, and you will likely see this data set in the future if you continue on in your studies to machine learning.\n\nUse the `glimpse()` function from the `dplyr` package to explore and describe the dataset.\n\n\n\n\n\n\nAdvanced\n\n\n\n\n\n\nFor the following problems we will use the titanic data set to learn additional data exploration techniques.\n\nUse the function head() on the titanic dataset. What does it do? Based on this, what do you expect the function tail() does?\nThe function unique(), when used on a specific variable within a data set, returns a vector of the values of the variable with duplicate elements removed. Try using the function unique() on the variable Embarked.\n\n\n\n\n\nIsmay, Chester. 2016. Getting Used to r, RStudio, and r Markdown. http://ismayc.github.io/rbasics-book.\n\n\nWickham, Hadley. 2021. Nycflights13: Flights That Departed NYC in 2013. https://github.com/hadley/nycflights13."
  },
  {
    "objectID": "02-visualization.html",
    "href": "02-visualization.html",
    "title": "2  Data Visualization",
    "section": "",
    "text": "We begin the development of your data science toolbox with data visualization. By visualizing our data, we gain valuable insights that we couldn’t initially see from just looking at the raw data in spreadsheet form. We will use the ggplot2 package as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as The Grammar of Graphics (Wilkinson 2005).\nAt the most basic level, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way for us to get a sense for how quantitative variables compare in terms of their center (where the values tend to be located) and their spread (how they vary around the center). Graphics should be designed to emphasize the findings and insight you want your audience to understand. This does however require a balancing act. On the one hand, you want to highlight as many meaningful relationships and interesting findings as possible; on the other you don’t want to include so many as to overwhelm your audience.\nAs we will see, plots/graphics also help us to identify patterns and outliers in our data. We will see that a common extension of these ideas is to compare the distribution of one quantitative variable (i.e., what the spread of a variable looks like or how the variable is distributed in terms of its values) as we go across the levels of a different categorical variable."
  },
  {
    "objectID": "02-visualization.html#packages-needed",
    "href": "02-visualization.html#packages-needed",
    "title": "2  Data Visualization",
    "section": "Packages Needed",
    "text": "Packages Needed\nLet’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section 1.3 for information on how to install and load R packages.\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "02-visualization.html#sec-grammarofgraphics",
    "href": "02-visualization.html#sec-grammarofgraphics",
    "title": "2  Data Visualization",
    "section": "2.1 The Grammar of Graphics",
    "text": "2.1 The Grammar of Graphics\nWe begin with a discussion of a theoretical framework for data visualization known as “The Grammar of Graphics,” which serves as the foundation for the ggplot2 package. Think of how we construct sentences in English to form sentences by combining different elements, like nouns, verbs, particles, subjects, objects, etc. However, we can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “The Grammar of Graphics” define a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2005) and has been implemented in a variety of data visualization software including R.\n\n2.1.1 Components of the Grammar\nIn short, the grammar tells us that:\n\nA statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects.\n\nSpecifically, we can break a graphic into three essential components:\n\ndata: the data set composed of variables that we map.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.\naes: aesthetic attributes of the geometric object. For example, x-position, y-position, color, shape, and size. Each assigned aesthetic attribute can be mapped to a variable in our data set.\n\nYou might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example.\n\n\n2.1.2 Gapminder data\n\n\n\nIn February 2006, a statistician named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for the 142 countries included from 2007, let’s consider only the first 6 countries when listed alphabetically in Table 2.1.\n\n\n\n\nTable 2.1: Gapminder 2007 Data: First 6 of 142 countries\n\n\nCountry\nContinent\nLife Expectancy\nPopulation\nGDP per Capita\n\n\n\n\nAfghanistan\nAsia\n43.8\n31889923\n975\n\n\nAlbania\nEurope\n76.4\n3600523\n5937\n\n\nAlgeria\nAfrica\n72.3\n33333216\n6223\n\n\nAngola\nAfrica\n42.7\n12420476\n4797\n\n\nArgentina\nAmericas\n75.3\n40301927\n12779\n\n\nAustralia\nOceania\n81.2\n20434176\n34435\n\n\n\n\n\n\nEach row in this table corresponds to a country in 2007. For each row, we have 5 columns:\n\nCountry: Name of country.\nContinent: Which of the five continents the country is part of. (Note that “Americas” includes countries in both North and South America and that Antarctica is excluded.)\nLife Expectancy: Life expectancy in years.\nPopulation: Number of people living in the country.\nGDP per Capita: Gross domestic product (in US dollars).\n\nNow consider Figure 2.1, which plots this data for all 142 countries in the data.\n\n\n\n\n\nFigure 2.1: Life Expectancy over GDP per Capita in 2007\n\n\n\n\nLet’s view this plot through the grammar of graphics:\n\nThe data variable GDP per Capita gets mapped to the x-position aesthetic of the points.\nThe data variable Life Expectancy gets mapped to the y-position aesthetic of the points.\nThe data variable Population gets mapped to the size aesthetic of the points.\nThe data variable Continent gets mapped to the color aesthetic of the points.\n\nWe’ll see shortly that data corresponds to the particular data frame where our data is saved and a “data variable” corresponds to a particular column in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. Other plots involve lines while others involve bars.\nLet’s summarize the three essential components of the Grammar in Table 2.2.\n\n\n\n\nTable 2.2: Summary of Grammar of Graphics for this plot\n\n\ndata variable\naes\ngeom\n\n\n\n\nGDP per Capita\nx\npoint\n\n\nLife Expectancy\ny\npoint\n\n\nPopulation\nsize\npoint\n\n\nContinent\ncolor\npoint\n\n\n\n\n\n\n\n\n2.1.3 Other components\nThere are other components of the Grammar of Graphics we can control as well. As you start to delve deeper into the Grammar of Graphics, you’ll start to encounter these topics more frequently. In this book however, we’ll keep things simple and only work with the two additional components listed below:\n\nfaceting breaks up a plot into small multiples corresponding to the levels of another variable (Section 2.6)\nposition adjustments for barplots (Section 2.8)\n\nOther more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (Grolemund and Wickham 2016). Generally speaking, the Grammar of Graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them.\n\n\n2.1.4 ggplot2 package\nIn this book, we will be using the ggplot2 package for data visualization, which is an implementation of the Grammar of Graphics for R (Wickham et al. 2022). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the Grammar of Graphics are specified in the ggplot() function included in the ggplot2 package, which expects at a minimum as arguments (i.e. inputs):\n\nThe data frame where the variables exist: the data argument.\nThe mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved.\n\nAfter we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include layers specifying the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 2.6.\nLet’s now put the theory of the Grammar of Graphics into practice."
  },
  {
    "objectID": "02-visualization.html#sec-five-ng",
    "href": "02-visualization.html#sec-five-ng",
    "title": "2  Data Visualization",
    "section": "2.2 Five Named Graphs - The 5NG",
    "text": "2.2 Five Named Graphs - The 5NG\nIn order to keep things simple, we will only focus on five types of graphics in this book, each with a commonly given name. We term these “five named graphs” the 5NG:\n\nscatterplots\nlinegraphs\nboxplots\nhistograms\nbarplots\n\nWe will discuss some variations of these plots, but with this basic repertoire of graphics in your toolbox you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables and while others are only appropriate for quantitative variables. You’ll want to quiz yourself often as we go along on which plot makes sense a given a particular problem or data set."
  },
  {
    "objectID": "02-visualization.html#sec-scatterplots",
    "href": "02-visualization.html#sec-scatterplots",
    "title": "2  Data Visualization",
    "section": "2.3 5NG#1: Scatterplots",
    "text": "2.3 5NG#1: Scatterplots\nThe simplest of the 5NG are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the Grammar of Graphics. Specifically, we will visualize the relationship between the following two numerical variables in the flights data frame included in the nycflights13 package:\n\ndep_delay: departure delay on the horizontal “x” axis and\narr_delay: arrival delay on the vertical “y” axis\n\nfor Alaska Airlines flights leaving NYC in 2013. This requires paring down the data from all 336,776 flights that left NYC in 2013, to only the 714 Alaska Airlines flights that left NYC in 2013.\nWhat this means computationally is: we’ll take the flights data frame, extract only the 714 rows corresponding to Alaska Airlines flights, and save this in a new data frame called alaska_flights. Run the code below to do this:\n\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\n\nFor now we suggest you ignore how this code works; we’ll explain this in detail in Chapter 3 when we cover data wrangling. However, convince yourself that this code does what it is supposed to by running View(alaska_flights): it creates a new data frame alaska_flights consisting of only the 714 Alaska Airlines flights.\nWe’ll see later in Chapter 3 on data wrangling that this code uses the dplyr package for data wrangling to achieve our goal: it takes the flights data frame and filters it to only return the rows where carrier is equal to \"AS\", Alaska Airlines’ carrier code. Other examples of carrier codes include “AA” for American Airlines and “UA” for United Airlines. Recall from Section 1.2 that testing for equality is specified with == and not =. Fasten your seat belts and sit tight for now however, we’ll introduce these ideas more fully in Chapter 3.\n\n\n\n\n\n\n🎯 Learning Check 2.1\n\n\n\n\n\nTake a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ?\n\n\n\n\n2.3.1 Scatterplots via geom_point\nLet’s now go over the code that will create the desired scatterplot, keeping in mind our discussion on the Grammar of Graphics in Section 2.1. We’ll be using the ggplot() function included in the ggplot2 package.\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()\n\nLet’s break this down piece-by-piece:\n\nWithin the ggplot() function, we specify two of the components of the Grammar of Graphics as arguments (i.e. inputs):\n\nThe data frame to be alaska_flights by setting data = alaska_flights.\nThe aesthetic mapping by setting aes(x = dep_delay, y = arr_delay). Specifically:\n\nthe variable dep_delay maps to the x position aesthetic\nthe variable arr_delay maps to the y position aesthetic\n\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case the geometric object are points, set by specifying geom_point().\n\nAfter running the above code, you’ll notice two outputs: a warning message and the graphic shown in Figure 2.2. Let’s first unpack the warning message:\n\n\nWarning: Removed 5 rows containing missing values (geom_point).\n\n\n\n\n\nFigure 2.2: Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013\n\n\n\n\nAfter running the above code, R returns a warning message alerting us to the fact that 5 rows were ignored due to them being missing. For 5 rows either the value for dep_delay or arr_delay or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Turning our attention to the resulting scatterplot in Figure 2.2, we see that a positive relationship exists between dep_delay and arr_delay: as departure delays increase, arrival delays tend to also increase. We also note the large mass of points clustered near (0, 0).\nBefore we continue, let’s consider a few more notes on the layers in the above code that generated the scatterplot:\n\nNote that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning.\nWhen adding layers to a plot, you are encouraged to start a new line after the + so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code.\nTo stress the importance of adding layers in particular the layer specifying the geometric object, consider Figure 2.3 where no layers are added. A not very useful plot!\n\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay))\n\n\n\n\nFigure 2.3: Plot with no layers\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.2\n\n\n\n\n\nWhat are some practical reasons why dep_delay and arr_delay have a positive relationship?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.3\n\n\n\n\n\nWhat variables (not necessarily in the flights data frame) would you expect to have a negative correlation (i.e. a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.4\n\n\n\n\n\nWhy do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaskan flights?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.5\n\n\n\n\n\nWhat are some other features of the plot that stand out to you?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.6\n\n\n\n\n\nCreate a new scatterplot using different variables in the alaska_flights data frame by modifying the example above.\n\n\n\n\n\n2.3.2 Over-plotting\nThe large mass of points near (0, 0) in Figure 2.2 can cause some confusion as it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to values being plotted on top of each other over and over again. It is often difficult to know just how many values are plotted in this way when looking at a basic scatterplot as we have here. There are two methods to address the issue of overplotting:\n\nBy adjusting the transparency of the points.\nBy adding a little random “jitter”, or random “nudges”, to each of the points.\n\nMethod 1: Changing the transparency\nThe first way of addressing overplotting is by changing the transparency of the points by using the alpha argument in geom_point(). By default, this value is set to 1. We can change this to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. Note how the following code is identical to the code in Section 2.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point():\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point(alpha = 0.2)\n\n\n\n\nFigure 2.4: Delay scatterplot with alpha = 0.2\n\n\n\n\nThe key feature to note in Figure 2.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line above to read geom_point(aes(alpha = 0.2)).\nMethod 2: Jittering the points\nThe second way of addressing overplotting is by jittering all the points, in other words give each point a small nudge in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame jitter_example with 4 rows of identical value 0 for both x and y:\n\n\n# A tibble: 4 × 2\n      x     y\n  <dbl> <dbl>\n1     0     0\n2     0     0\n3     0     0\n4     0     0\n\n\nWe display the resulting scatterplot in Figure 2.5; observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others.\n\n\n\n\n\nFigure 2.5: Regular scatterplot of jitter example data\n\n\n\n\nIn Figure 2.6 we instead display a jittered scatterplot where each point is given a random “nudge.” It is now plainly evident that this plot involves four points. Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in jitter_example remain unchanged.\n\n\n\n\n\nFigure 2.6: Jittered scatterplot of jitter example data\n\n\n\n\nTo create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). To specify how much jitter to add, we adjust the width and height arguments. This corresponds to how hard you’d like to shake the plot in units corresponding to those for both the horizontal and vertical variables (in this case minutes).\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_jitter(width = 30, height = 30)\n\n\n\n\nFigure 2.7: Jittered delay scatterplot\n\n\n\n\nObserve how the above code is identical to the code that created the scatterplot with overplotting in Subsection 2.3.1, but with geom_point() replaced with geom_jitter().\nThe resulting plot in Figure 2.7 helps us a little bit in getting a sense for the overplotting, but with a relatively large data set like this one (714 flights), it can be argued that changing the transparency of the points by setting alpha proved more effective. In terms of how much jitter one should add using the width and height arguments, it is important to add just enough jitter to break any overlap in points, but not so much that we completely alter the overall pattern in points.\n\n\n\n\n\n\n🎯 Learning Check 2.7\n\n\n\n\n\nWhy is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.8\n\n\n\n\n\nAfter viewing the Figure 2.4 above, give an approximate range of arrival delays and departure delays that occur the most frequently. How has that region changed compared to when you observed the same plot without the alpha = 0.2 set in Figure 2.2?\n\n\n\n\n\n2.3.3 Summary\nScatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful!\nWith medium to large data sets, you may need to play around with the different modifications one can make to a scatterplot. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships come about as you make subtle changes to your plots."
  },
  {
    "objectID": "02-visualization.html#sec-linegraphs",
    "href": "02-visualization.html#sec-linegraphs",
    "title": "2  Data Visualization",
    "section": "2.4 5NG#2: Linegraphs",
    "text": "2.4 5NG#2: Linegraphs\nThe next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature; in other words there is an inherent ordering to the variable. The most common example of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots. Linegraphs should be avoided when there is not a clear sequential ordering to the variable on the x-axis. Let’s illustrate linegraphs using another data set in the nycflights13 package: the weather data frame.\nLet’s get a sense for the weather data frame:\n\nExplore the weather data by running View(weather).\nRun ?weather to bring up the help file.\n\nWe can see that there is a variable called temp of hourly temperature recordings in Fahrenheit at weather stations near all three airports in New York City: Newark (origin code EWR), JFK, and La Guardia (LGA). Instead of considering hourly temperatures for all days in 2013 for all three airports however, for simplicity let’s only consider hourly temperatures at only Newark airport for the first 15 days in January.\nRecall in Section 2.3 we used the filter() function to only choose the subset of rows of flights corresponding to Alaska Airlines flights. We similarly use filter() here, but by using the & operator we only choose the subset of rows of weather where\n\nThe origin is \"EWR\" and\nthe month is January and\nthe day is between 1 and 15\n\n\nearly_january_weather <- weather %>% \n  filter(origin == \"EWR\" & month == 1 & day <= 15)\n\n\n\n\n\n\n\n🎯 Learning Check 2.9\n\n\n\n\n\nTake a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.10\n\n\n\n\n\nView() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement whereas the hour variable does not?\n\n\n\n\n2.4.1 Linegraphs via geom_line\nLet’s plot a linegraph of hourly temperatures in early_january_weather by using geom_line() instead of geom_point() like we did for scatterplots:\n\nggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) +\n  geom_line()\n\n\n\n\nFigure 2.8: Hourly Temperature in Newark for January 1-15, 2013\n\n\n\n\nMuch as with the ggplot() code that created the scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 2.2, let’s break down the above code piece-by-piece in terms of the Grammar of Graphics:\n\nWithin the ggplot() function call, we specify two of the components of the Grammar of Graphics as arguments:\n\nThe data frame to be early_january_weather by setting data = early_january_weather\nThe aesthetic mapping by setting aes(x = time_hour, y = temp). Specifically:\n\nthe variable time_hour maps to the x position aesthetic.\nthe variable temp maps to the y position aesthetic\n\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case the geometric object is a line, set by specifying geom_line().\n\n\n\n\n\n\n\n🎯 Learning Check 2.11\n\n\n\n\n\nWhy should linegraphs be avoided when there is not a clear ordering of the horizontal axis?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.12\n\n\n\n\n\nWhy are linegraphs frequently used when time is the explanatory variable on the x-axis?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.13\n\n\n\n\n\nPlot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013.\n\n\n\n\n\n2.4.2 Summary\nLinegraphs, just like scatterplots, display the relationship between two numerical variables. However it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e. the explanatory variable) has an inherent ordering, like some notion of time."
  },
  {
    "objectID": "02-visualization.html#sec-histograms",
    "href": "02-visualization.html#sec-histograms",
    "title": "2  Data Visualization",
    "section": "2.5 5NG#3: Histograms",
    "text": "2.5 5NG#3: Histograms\nLet’s consider the temp variable in the weather data frame once again, but unlike with the linegraphs in Section 2.4, let’s say we don’t care about the relationship of temperature to time, but rather we only care about how the values of temp distribute. In other words:\n\nWhat are the smallest and largest values?\nWhat is the “center” value?\nHow do the values spread out?\nWhat are frequent and infrequent values?\n\nOne way to visualize this distribution of this single variable temp is to plot them on a horizontal line as we do in Figure 2.9:\n\n\n\n\n\nFigure 2.9: Plot of Hourly Temperature Recordings from NYC in 2013\n\n\n\n\nThis gives us a general idea of how the values of temp distribute: observe that temperatures vary from around 11°F up to 100°F. Furthermore, there appear to be more recorded temperatures between 40°F and 60°F than outside this range. However, because of the high degree of overlap in the points, it’s hard to get a sense of exactly how many values are between, say, 50°F and 55°F.\nWhat is commonly produced instead of the above plot is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows:\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nThen for each bin, we draw a bar whose height marks the corresponding count.\n\nLet’s drill-down on an example of a histogram, shown in @fig-histogramexample.\n\n\n\n\n\nFigure 2.10: Example histogram\n\n\n\n\nObserve that there are three bins of equal width between 30°F and 60°F, thus we have three bins of width 10°F each: one bin for the 30-40°F range, another bin for the 40-50°F range, and another bin for the 50-60°F range. Since:\n\nThe bin for the 30-40°F range has a height of around 5000, this histogram is telling us that around 5000 of the hourly temperature recordings are between 30°F and 40°F.\nThe bin for the 40-50°F range has a height of around 4300, this histogram is telling us that around 4300 of the hourly temperature recordings are between 40°F and 50°F.\nThe bin for the 50-60°F range has a height of around 3500, this histogram is telling us that around 3500 of the hourly temperature recordings are between 50°F and 60°F.\n\nThe remaining bins all have a similar interpretation.\n\n2.5.1 Histograms via geom_histogram\nLet’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram()\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 rows containing non-finite values (stat_bin).\n\n\n\n\n\nFigure 2.11: Histogram of hourly temperatures at three NYC airports\n\n\n\n\nLet’s unpack the messages R sent us first. The first message is telling us that the histogram was constructed using bins = 30, in other words 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change this default number of bins. The second message is telling us something similar to the warning message we received when we ran the code to create a scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 2.2: that because one row has a missing NA value for temp, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case.\nNow’s let’s unpack the resulting histogram in Figure 2.11. Observe that values less than 25°F as well as values above 80°F are rather rare. However, because of the large number of bins, its hard to get a sense for which range of temperatures is covered by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram():\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(color = \"white\")\n\n\n\n\nFigure 2.12: Histogram of hourly temperatures at three NYC airports with white borders\n\n\n\n\nWe can now better associate ranges of temperatures to each of the bins. We can also vary the color of the bars by setting the fill argument. Run colors() to see all 657 possible choice of colors!\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n\n\n\nFigure 2.13: Histogram of hourly temperatures at three NYC airports with white borders\n\n\n\n\n\n\n2.5.2 Adjusting the bins\nObserve in both Figure 2.12 and Figure 2.13 that in the 50-75°F range there appear to be roughly 8 bins. Thus each bin has width 25 divided by 8, or roughly 3.12°F which is not a very easily interpretable range to work with. Let’s now adjust the number of bins in our histogram in one of two methods:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nUsing the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows:\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(bins = 40, color = \"white\")\n\n\n\n\nFigure 2.14: Histogram with 40 bins\n\n\n\n\nUsing the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 10°F.\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 10, color = \"white\")\n\n\n\n\nFigure 2.15: Histogram with binwidth 10\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.14\n\n\n\n\n\nWhat does changing the number of bins from 30 to 40 tell us about the distribution of temperatures?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.15\n\n\n\n\n\nWould you classify the distribution of temperatures as symmetric or skewed?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.16\n\n\n\n\n\nWhat would you guess is the “center” value in this distribution? Why did you make that choice?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.17\n\n\n\n\n\nIs this data spread out greatly from the center or is it close? Why?\n\n\n\n\n\n2.5.3 Summary\nHistograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question."
  },
  {
    "objectID": "02-visualization.html#sec-facets",
    "href": "02-visualization.html#sec-facets",
    "title": "2  Data Visualization",
    "section": "2.6 Facets",
    "text": "2.6 Facets\nBefore continuing the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization of variables by another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ.\nFor example, suppose we were interested in looking at how the histogram of hourly temperature recordings at the three NYC airports we saw in Section 2.5 differed by month. We would “split” this histogram by the 12 possible months in a given year, in other words plot histograms of temp for each month. We do this by adding facet_wrap(~ month) layer.\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  facet_wrap(~ month)\n\n\n\n\nFigure 2.16: Faceted histogram\n\n\n\n\nNote the use of the tilde ~ before month in facet_wrap(). The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'month' not found if you don’t include it before month here. We can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted plot to have 4 rows instead of 3. Add the nrow = 4 argument to facet_wrap(~ month)\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  facet_wrap(~ month, nrow = 4)\n\n\n\n\nFigure 2.17: Faceted histogram with 4 instead of 3 rows\n\n\n\n\nObserve in both Figure 2.16 and Figure 2.17 that as we might expect in the Northern Hemisphere, temperatures tend to be higher in the summer months, while they tend to be lower in the winter.\n\n\n\n\n\n\n🎯 Learning Check 2.18\n\n\n\n\n\nWhat other things do you notice about the faceted plot above? How does a faceted plot help us see relationships between two variables?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.19\n\n\n\n\n\nWhat do the numbers 1-12 correspond to in the plot above? What about 25, 50, 75, 100?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.20\n\n\n\n\n\nFor which types of data sets would these types of faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.21\n\n\n\n\n\nDoes the temp variable in the weather data set have a lot of variability? Why do you say that?"
  },
  {
    "objectID": "02-visualization.html#sec-boxplots",
    "href": "02-visualization.html#sec-boxplots",
    "title": "2  Data Visualization",
    "section": "2.7 5NG#4: Boxplots",
    "text": "2.7 5NG#4: Boxplots\nWhile faceted histograms are one visualization that allows us to compare distributions of a numerical variable split by another variable, another visualization that achieves this same goal are side-by-side boxplots. A boxplot is constructed from the information provided in the five-number summary of a numerical variable (see Appendix A). To keep things simple for now, let’s only consider hourly temperature recordings for the month of November in Figure 2.18.\n\n\n\n\n\nFigure 2.18: November temperatures\n\n\n\n\nThese 2141 observations have the following five-number summary:\n\nMinimum: 21.02°F\nFirst quartile AKA 25th percentile: 35.96°F\nMedian AKA second quartile AKA 50th percentile: 44.96°F\nThird quartile AKA 75th percentile: 51.98°F\nMaximum: 71.06°F\n\nLet’s mark these 5 values with dashed horizontal lines in Figure 2.19.\n\n\n\n\n\nFigure 2.19: November temperatures\n\n\n\n\nLet’s add the boxplot underneath these points and dashed horizontal lines in Figure 2.20.\n\n\n\n\n\nFigure 2.20: November temperatures\n\n\n\n\nWhat the boxplot does summarize the 2141 points by emphasizing that:\n\n25% of points (about 534 observations) fall below the bottom edge of the box, which is the first quartile of 35.96°F. In other words 25% of observations were colder than 35.96°F.\n25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 44.96°F. In other words 25% of observations were between 35.96 and 44.96°F and 50% of observations were colder than 44.96°F.\n25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 51.98°F. In other words 25% of observations were between 44.96 and 51.98°F and 75% of observations were colder than 51.98°F.\n25% of points fall over the top edge of the box. In other words 25% of observations were warmer than 51.98°F.\nThe middle 50% of points lie within the interquartile range between the first and third quartile of 51.98 - 35.96 = 16.02°F.\n\nLastly, for clarity’s sake let’s remove the points but keep the dashed horizontal lines in Figure 2.21.\n\n\n\n\n\nFigure 2.21: November temperatures\n\n\n\n\nWe can now better see the whiskers of the boxplot. They stick out from either end of the box all the way to the minimum and maximum observed temperatures of 21.02°F and 71.06°F respectively. However, the whiskers don’t always extend to the smallest and largest observed values. They in fact can extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box, in this case 1.5 \\(\\times\\) 16.02°F = 24.03°F from either end of the box. Any observed values outside this whiskers get marked with points called outliers, which we’ll see in the next section.\n\n2.7.1 Boxplots via geom_boxplot\nLet’s now create a side-by-side boxplot of hourly temperatures split by the 12 months as we did above with the faceted histograms. We do this by mapping the month variable to the x-position aesthetic, the temp variable to the y-position aesthetic, and by adding a geom_boxplot() layer:\n\nggplot(data = weather, mapping = aes(x = month, y = temp)) +\n  geom_boxplot()\n\n\n\n\nFigure 2.22: Invalid boxplot specification\n\n\n\n\nWarning messages:\n1: Continuous x aesthetic -- did you forget aes(group=...)? \n2: Removed 1 rows containing non-finite values (stat_boxplot). \nObserve in Figure 2.22 that this plot does not provide information about temperature separated by month. The warning messages clue us in as to why. The second warning message is identical to the warning message when plotting a histogram of hourly temperatures: that one of the values was recorded as NA missing. However, the first warning message is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots however require a categorical variable on the x-axis.\nWe can convert the numerical variable month into a categorical variable by using the factor() function. So after applying factor(month), month goes from having numerical values 1, 2, …, 12 to having labels “1”, “2”, …, “12.”\n\nggplot(data = weather, mapping = aes(x = factor(month), y = temp)) +\n  geom_boxplot()\n\n\n\n\nFigure 2.23: Temp by month boxplot\n\n\n\n\nThe resulting Figure 2.23 shows 12 separate “box and whiskers” plots with the features we saw earlier focusing only on November:\n\nThe “box” portions of this visualization represent the 1st quartile, the median AKA the 2nd quartile, and the 3rd quartile.\nThe “length” of each box, i.e. the value of the 3rd quartile minus the value of the 1st quartile, is the interquartile range. It is a measure of spread of the middle 50% of values, with longer boxes indicating more variability.\nThe “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed temperatures. The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability.\nThe dots representing values falling outside the whiskers are called outliers. These can be thought of as anomalous values.\n\nIt is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long. Looking at this plot we can see, as expected, that summer months (6 through 8) have higher median temperatures as evidenced by the higher solid lines in the middle of the boxes. We can easily compare temperatures across months by drawing imaginary horizontal lines across the plot. Furthermore, the height of the 12 boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of temperatures recorded in a given month.\n\n\n\n\n\n\n🎯 Learning Check 2.22\n\n\n\n\n\nWhat does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.23\n\n\n\n\n\nWhich months have the highest variability in temperature? What reasons can you give for this?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.24\n\n\n\n\n\nWe looked at the distribution of the numerical variable temp split by the numerical variable month that we converted to a categorical variable using the factor() function. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.25\n\n\n\n\n\nBoxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?\n\n\n\n\n\n2.7.2 Summary\nSide-by-side boxplots provide us with a way to compare and contrast the distribution of a quantitative variable across multiple levels of another categorical variable. One can see where the median falls across the different groups by looking at the center line in the boxes. To see how spread out the variable is across the different groups, look at both the width of the box and also how far the whiskers stretch out away from the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with points."
  },
  {
    "objectID": "02-visualization.html#sec-geombar",
    "href": "02-visualization.html#sec-geombar",
    "title": "2  Data Visualization",
    "section": "2.8 5NG#5: Barplots",
    "text": "2.8 5NG#5: Barplots\nBoth histograms and boxplots are tools to visualize the distribution of numerical variables. Another common task is visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories, also known as levels, of a categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot (also known as a barchart). One complication, however, is how your data is represented: is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges.\n\nfruits <- tibble(\n  fruit = c(\"apple\", \"apple\", \"orange\", \"apple\", \"orange\")\n  )\n\nfruits_counted <- tibble(\n  fruit = c(\"apple\", \"orange\"),\n  number = c(3, 2)\n  )\n\nWe see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually…\n\n\n# A tibble: 5 × 1\n  fruit \n  <chr> \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange\n\n\n… fruits_counted has a variable number which represents pre-counted values of each fruit.\n\n\n# A tibble: 2 × 2\n  fruit  number\n  <chr>   <dbl>\n1 apple       3\n2 orange      2\n\n\nDepending on how your categorical data is represented, you’ll need to use add a different geom layer to your ggplot() to create a barplot, as we now explore.\n\n2.8.1 Barplots via geom_bar or geom_col\nLet’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer.\n\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\n\n\n\n\nFigure 2.24: Barplot when counts are not pre-counted\n\n\n\n\nHowever, using the fruits_counted data frame where the fruit have been “pre-counted”, we map the fruit variable to the x-position aesthetic as with geom_bar(), but we also map the count variable to the y-position aesthetic, and add a geom_col() layer.\n\nggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) +\n  geom_col()\n\n\n\n\nFigure 2.25: Barplot when counts are pre-counted\n\n\n\n\nCompare the barplots in Figure 2.24 and Figure 2.25. They are identical because they reflect count of the same 5 fruit. However depending on how our data is saved, either pre-counted or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize is:\n\nIs not pre-counted in your data frame: use geom_bar().\nIs pre-counted in your data frame, use geom_col() with the y-position aesthetic mapped to the variable that has the counts.\n\nLet’s now go back to the flights data frame in the nycflights13 package and visualize the distribution of the categorical variable carrier. In other words, let’s visualize the number of domestic flights out of the three New York City airports each airline company flew in 2013. Recall from Section 1.4.3 when you first explored the flights data frame you saw that each row corresponds to a flight. In other words the flights data frame is more like the fruits data frame than the fruits_counted data frame above, and thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable carrier gets mapped to the x-position.\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\n\n\n\nFigure 2.26: Number of flights departing NYC in 2013 by airline using geom_bar()\n\n\n\n\nObserve in Figure 2.26 that United Air Lines (UA), JetBlue Airways (B6), and ExpressJet Airlines (EV) had the most flights depart New York City in 2013. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example: AA is American Airlines; B6 is JetBlue Airways; DL is Delta Airlines; EV is ExpressJet Airlines; MQ is Envoy Air; while UA is United Airlines.\nAlternatively, say you had a data frame flights_counted where the number of flights for each carrier was pre-counted like in Table 2.3.\n\n\n\n\nTable 2.3: Number of flights pre-counted for each carrier\n\n\ncarrier\nnumber\n\n\n\n\nUA\n58665\n\n\nB6\n54635\n\n\nEV\n54173\n\n\nDL\n48110\n\n\nAA\n32729\n\n\nMQ\n26397\n\n\nUS\n20536\n\n\n9E\n18460\n\n\nWN\n12275\n\n\nVX\n5162\n\n\nFL\n3260\n\n\nAS\n714\n\n\nF9\n685\n\n\nYV\n601\n\n\nHA\n342\n\n\nOO\n32\n\n\n\n\n\n\nIn order to create a barplot visualizing the distribution of the categorical variable carrier in this case, we would use geom_col() instead with x mapped to carrier and y mapped to number as seen below. The resulting barplot would be identical to Figure 2.26.\n\nggplot(data = flights_table, mapping = aes(x = carrier, y = number)) +\n  geom_col()\n\n\n\n\n\n\n\n🎯 Learning Check 2.26\n\n\n\n\n\nWhy are histograms inappropriate for visualizing categorical variables?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.27\n\n\n\n\n\nWhat is the difference between histograms and barplots?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.28\n\n\n\n\n\nHow many Envoy Air flights departed NYC in 2013?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.29\n\n\n\n\n\nWhat was the seventh highest airline in terms of departed flights from NYC in 2013? How could we better present the table to get this answer quickly?\n\n\n\n\n\n2.8.2 Must avoid pie charts!\nUnfortunately, one of the most common plots seen today for categorical data is the pie chart. While they may seem harmless enough, they actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book “Creating More Effective Graphs” (Robbins 2013), we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine relative size of one piece of the pie compared to another.\nLet’s examine the same data used in our previous barplot of the number of flights departing NYC by airline in Figure 2.26, but this time we will use a pie chart in Figure 2.27.\n\n\n\n\n\nFigure 2.27: The dreaded pie chart\n\n\n\n\nTry to answer the following questions:\n\nHow much larger the portion of the pie is for ExpressJet Airlines (EV) compared to US Airways (US),\nWhat the third largest carrier is in terms of departing flights, and\nHow many carriers have fewer flights than United Airlines (UA)?\n\nWhile it is quite difficult to answer these questions when looking at the pie chart in Figure 2.27, we can much more easily answer these questions using the barchart in Figure Figure 2.26. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons between categories must be made by comparing angles.\nThere may be one exception of a pie chart not to avoid courtesy Nathan Yau at FlowingData.com, but we will leave this for the reader to decide:\n\n\n\nThe only good pie chart\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.30\n\n\n\n\n\nWhy should pie charts be avoided and replaced by barplots?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.31\n\n\n\n\n\nWhy do you think people continue to use pie charts?\n\n\n\n\n\n2.8.3 Two categorical variables\nBarplots are the go-to way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of outgoing domestic flights from NYC by carrier and origin, or in other words the number of flights for each carrier and origin combination. For example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure 2.26:\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping; the fill aesthetic of any bar corresponds to the color used to fill the bars.\n\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar()\n\n\n\n\nFigure 2.28: Stacked barplot comparing the number of flights by carrier and origin\n\n\n\n\nFigure 2.28 is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of flights from each origin airport between the carriers.\nBefore we continue, let’s address some common points of confusion amongst new R users. First, note that fill is another aesthetic mapping much like x-position; thus it must be included within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make:\n\nggplot(data = flights, mapping = aes(x = carrier), fill = origin) +\n  geom_bar()\n\nSecond, the fill aesthetic corresponds to the color used to fill the bars, while the color aesthetic corresponds to the color of the outline of the bars. Observe in Figure 2.29 that mapping origin to color and not fill yields grey bars with different colored outlines.\n\nggplot(data = flights, mapping = aes(x = carrier, color = origin)) +\n  geom_bar()\n\n\n\n\nFigure 2.29: Stacked barplot with color aesthetic used instead of fill\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.32\n\n\n\n\n\nWhat kinds of questions are not easily answered by looking at the above figure?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.33\n\n\n\n\n\nWhat can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?\n\n\n\nAnother alternative to stacked barplots are side-by-side barplots, also known as a dodged barplot. The code to created a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot.\n\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nFigure 2.30: Side-by-side AKA dodged barplot comparing the number of flights by carrier and origin\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.34\n\n\n\n\n\nWhy might the side-by-side (AKA dodged) barplot be preferable to a stacked barplot in this case?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.35\n\n\n\n\n\nWhat are the disadvantages of using a side-by-side (AKA dodged) barplot, in general?\n\n\n\nLastly, another type of barplot is a faceted barplot. Recall in Section 2.6 we visualized the distribution of hourly temperatures at the 3 NYC airports split by month using facets. We apply the same principle to our barplot visualizing the frequency of carrier split by origin: instead of mapping origin\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar() +\n  facet_wrap(~ origin, ncol = 1)\n\n\n\n\nFigure 2.31: Faceted barplot comparing the number of flights by carrier and origin\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.36\n\n\n\n\n\nWhy is the faceted barplot preferred to the side-by-side and stacked barplots in this case?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.37\n\n\n\n\n\nWhat information about the different carriers at different airports is more easily seen in the faceted barplot?\n\n\n\n\n\n2.8.4 Summary\nBarplots are the preferred way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories called levels occur. They are easy to understand and make it easy to make comparisons across levels. When trying to visualize two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the joint distribution you are trying to emphasize, you will need to make a choice between these three types of barplots."
  },
  {
    "objectID": "02-visualization.html#sec-data-vis-conclusion",
    "href": "02-visualization.html#sec-data-vis-conclusion",
    "title": "2  Data Visualization",
    "section": "2.9 Conclusion",
    "text": "2.9 Conclusion\n\n2.9.1 Summary table\nLet’s recap all five of the Five Named Graphs (5NG) in Table 2.4 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package.\n\n\n\n\nTable 2.4: Summary of 5NG\n\n\n\n\n\n\n\n\nNamed graph\nShows\nGeometric object\nNotes\n\n\n\n\nScatterplot\nRelationship between 2 numerical variables\ngeom_point()\n\n\n\nLinegraph\nRelationship between 2 numerical variables\ngeom_line()\nUsed when there is a sequential order to x-variable e.g. time\n\n\nHistogram\nDistribution of 1 numerical variable\ngeom_histogram()\nFacetted histograms show the distribution of 1 numerical variable split by the values of another variable\n\n\nBoxplot\nDistribution of 1 numerical variable split by the values of another variable\ngeom_boxplot()\n\n\n\nBarplot\nDistribution of 1 categorical variable\ngeom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted\nStacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables\n\n\n\n\n\n\n\n\n2.9.2 Argument specification\nRun the following two segments of code. First this:\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\nthen this:\n\nggplot(flights, aes(x = carrier)) +\n  geom_bar()\n\nYou’ll notice that that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() by default assumes that the data argument comes first and the mapping argument comes second. So as long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =.\nGoing forward for the rest of this book, all ggplot() will be like the second segment above: with the data = and mapping = explicit naming of the argument omitted and the default ordering of arguments respected.\n\n\n2.9.3 Additional resources\nIf you want to further unlock the power of the ggplot2 package for data visualization, we suggest you that you check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular the many more than the 5 geom geometric objects we covered in this Chapter, while providing quick and easy to read visual descriptions.\nYou can access this cheatsheet by going to the RStudio Menu Bar -> Help -> Cheatsheets -> “Data Visualization with ggplot2”:\n\n\n\n\n\n\n\n2.9.4 What’s to come\nRecall in Figure 2.2 in Section 2.3 we visualized the relationship between departure delay and arrival delay for Alaska Airlines flights. This necessitated paring or filtering down the flights data frame to a new data frame alaska_flights consisting of only carrier == AS flights first:\n\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()\n\nFurthermore recall in Figure 2.8 in Section 2.4 we visualized hourly temperature recordings at Newark airport only for the first 15 days of January 2013. This necessitated paring or fitlering down the weather data frame to a new data frame early_january_weather consisting of hourly temperature recordings only for origin == \"EWR\", month == 1, and day less than or equal to 15 first:\n\nearly_january_weather <- weather %>% \n  filter(origin == \"EWR\" & month == 1 & day <= 15)\n\nggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) +\n  geom_line()\n\nThese two code segments were a preview of Chapter 3 on data wrangling where we’ll delve further into the dplyr package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, the two code segments used the filter() function to create new data frames (alaska_flights and early_january_weather) by choosing only a subset of rows of existing data frames (flights and weather). In this next chapter, we’ll formally introduce the filter() and other data wrangling functions as well as the pipe operator %>% which allows you to combine multiple data wrangling actions into a single sequential chain of actions. On to Chapter 3 on data wrangling!\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. 2016. R for Data Science. http://r4ds.had.co.nz/.\n\n\nRobbins, Naomi. 2013. Creating More Effective Graphs. Chart House.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and Computing). Secaucus, NJ, USA: Springer-Verlag New York, Inc."
  },
  {
    "objectID": "03-wrangling.html",
    "href": "03-wrangling.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "So far in our journey, we’ve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter 1 on and how to create data visualizations using the ggplot2 package in Chapter 2. In particular we studied what we term the “five named graphs” (5NG):\nWe created these visualizations using the “Grammar of Graphics”, which maps variables in a data frame to the aesthetic attributes of one the above 5 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure Figure 2.1.\nRecall however in Section 2.9.4 we discussed that for two of our visualizations we needed transformed/modified versions of existing data frames. Recall for example the scatterplot of departure and arrival delay only for Alaska Airlines flights. In order to create this visualization, we needed to first pare down the flights data frame to a new data frame alaska_flights consisting of only carrier == \"AS\" flights using the filter() function.\nIn this chapter, we’ll introduce a series of functions from the dplyr package that will allow you to take a data frame and\nNotice how we used computer code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling that we’ll introduce in this chapter has intuitively verb-named functions that are easy to remember.\nWe’ll start by introducing the pipe operator %>%, which allows you to combine multiple data wrangling verb-named functions into a single sequential chain of actions."
  },
  {
    "objectID": "03-wrangling.html#packages-needed",
    "href": "03-wrangling.html#packages-needed",
    "title": "3  Data Wrangling",
    "section": "Packages Needed",
    "text": "Packages Needed\nLet’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 1.3 for information on how to install and load R packages.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(nycflights13)"
  },
  {
    "objectID": "03-wrangling.html#sec-piping",
    "href": "03-wrangling.html#sec-piping",
    "title": "3  Data Wrangling",
    "section": "3.1 The pipe operator: %>%",
    "text": "3.1 The pipe operator: %>%\nBefore we start data wrangling, let’s first introduce a very nifty tool that gets loaded along with the dplyr package: the pipe operator %>%. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h():\n\nTake x then\nUse x as an input to a function f() then\nUse the output of f(x) as an input to a function g() then\nUse the output of g(f(x)) as an input to a function h()\n\nOne way to achieve this sequence of operations is by using nesting parentheses as follows:\n\nh(g(f(x)))\n\nThe above code isn’t so hard to read since we are applying only three functions: f(), then g(), then h(). However, you can imagine that this can get progressively harder and harder to read as the number of functions applied in your sequence increases. This is where the pipe operator %>% comes in handy. %>% takes one output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %>% as “then.” For example, you can obtain the same output as the above sequence of operations as follows:\n\nx %>% \n  f() %>% \n  g() %>% \n  h()\n\nYou would read this above sequence as:\n\nTake x then\nUse this output as the input to the next function f() then\nUse this output as the input to the next function g() then\nUse this output as the input to the next function h()\n\nSo while both approaches above would achieve the same goal, the latter is much more human-readable because you can read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling:\n\nThe starting value x will be a data frame. For example: flights.\nThe sequence of functions, here f(), g(), and h(), will be a sequence of any number of the 6 data wrangling verb-named functions we listed in the introduction to this chapter. For example: filter(carrier == \"AS\").\nThe result will be the transformed/modified data frame that you want. For example: a data frame consisting of only the subset of rows in flights corresponding to Alaska Airlines flights.\n\nMuch like when adding layers to a ggplot() using the + sign at the end of lines, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence with pipe operators %>% at the end of lines. So continuing our example involving Alaska Airlines flights, we form a chain using the pipe operator %>% and save the resulting data frame in alaska_flights:\n\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\n\nKeep in mind, there are many more advanced data wrangling functions than just the 6 listed in the introduction to this chapter; you’ll see some examples of these in Section 3.8. However, just with these 6 verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this book."
  },
  {
    "objectID": "03-wrangling.html#sec-filter",
    "href": "03-wrangling.html#sec-filter",
    "title": "3  Data Wrangling",
    "section": "3.2 filter() rows",
    "text": "3.2 filter() rows\n\n\n\nFigure 3.1: Diagram of filter()\n\n\nThe filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only those rows that match that criteria. We begin by focusing only on flights from New York City to Portland, Oregon. The dest code (or airport code) for Portland, Oregon is \"PDX\". Run the following and look at the resulting spreadsheet to ensure that only flights heading to Portland are chosen here:\n\nportland_flights <- flights %>% \n  filter(dest == \"PDX\")\n\nView(portland_flights)\n\nNote the following:\n\nThe ordering of the commands:\n\nTake the flights data frame flights then\nfilter the data frame so that only those where the dest equals \"PDX\" are included.\n\nWe test for equality using the double equal sign == and not a single equal sign =. In other words filter(dest = \"PDX\") will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it.\n\nYou can use other mathematical operations beyond just == to form criteria:\n\n> corresponds to “greater than”\n< corresponds to “less than”\n>= corresponds to “greater than or equal to”\n<= corresponds to “less than or equal to”\n!= corresponds to “not equal to”. The ! is used in many programming languages to indicate “not”.\n\nFurthermore, you can combine multiple criteria together using operators that make comparisons:\n\n| corresponds to “or”\n& corresponds to “and”\n\nTo see many of these in action, let’s filter flights for all rows that:\n\nDeparted from JFK airport and\nWere heading to Burlington, Vermont (\"BTV\") or Seattle, Washington (\"SEA\") and\nDeparted in the months of October, November, or December.\n\nRun the following:\n\nbtv_sea_flights_fall <- flights %>% \n  filter(origin == \"JFK\" & (dest == \"BTV\" | dest == \"SEA\") & month >= 10)\n\nView(btv_sea_flights_fall)\n\nNote that even though colloquially speaking one might say “all flights leaving Burlington, Vermont and Seattle, Washington,” in terms of computer operations, we really mean “all flights leaving Burlington, Vermont or leaving Seattle, Washington.” For a given row in the data, dest can be “BTV”, “SEA”, or something else, but not “BTV” and “SEA” at the same time. Furthermore, note the careful use of parentheses around the dest == \"BTV\" | dest == \"SEA\".\nWe can often skip the use of & and just separate our conditions with a comma. In other words the code above will return the identical output btv_sea_flights_fall as this code below:\n\nbtv_sea_flights_fall <- flights %>% \n  filter(origin == \"JFK\", (dest == \"BTV\" | dest == \"SEA\"), month >= 10)\n\nView(btv_sea_flights_fall)\n\nLet’s present another example that uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Here we are filtering rows corresponding to flights that didn’t go to Burlington, VT or Seattle, WA.\n\nnot_BTV_SEA <- flights %>% \n  filter(!(dest == \"BTV\" | dest == \"SEA\"))\n\nView(not_BTV_SEA)\n\nAgain, note the careful use of parentheses around the (dest == \"BTV\" | dest == \"SEA\"). If we didn’t use parentheses as follows:\n\nflights %>% \n  filter(!dest == \"BTV\" | dest == \"SEA\")\n\nWe would be returning all flights not headed to \"BTV\" or those headed to \"SEA\", which is an entirely different resulting data frame.\nNow say we have a large list of airports we want to filter for, say BTV, SEA, PDX, SFO, and BDL. We could continue to use the | or operator as so:\n\nmany_airports <- flights %>% \n  filter(dest == \"BTV\" | dest == \"SEA\" | dest == \"PDX\" | dest == \"SFO\" | dest == \"BDL\")\n\nView(many_airports)\n\nbut as we progressively include more airports, this will get unwieldy. A slightly shorter approach uses the %in% operator:\n\nmany_airports <- flights %>% \n  filter(dest %in% c(\"BTV\", \"SEA\", \"PDX\", \"SFO\", \"BDL\"))\n\nView(many_airports)\n\nWhat this code is doing is filtering flights for all flights where dest is in the list of airports c(\"BTV\", \"SEA\", \"PDX\", \"SFO\", \"BDL\"). Recall from Chapter 1 that the c() function “combines” or “concatenates” values in a vector of values. Both outputs of many_airports are the same, but as you can see the latter takes much less time to code.\nAs a final note we point out that filter() should often be among the first verbs you apply to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations your care about.\n\n\n\n\n\n\n🎯 Learning Check 3.1\n\n\n\n\n\nWhat’s another way of using the “not” operator ! to filter only the rows that are not going to Burlington VT nor Seattle WA in the flights data frame? Test this out using the code above."
  },
  {
    "objectID": "03-wrangling.html#sec-summarize",
    "href": "03-wrangling.html#sec-summarize",
    "title": "3  Data Wrangling",
    "section": "3.3 summarize() variables",
    "text": "3.3 summarize() variables\nThe next common task when working with data is to return summary statistics: a single numerical value that summarizes a large number of values, for example the mean/average or the median. Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value AKA the minimum, the largest value AKA the maximum, and the standard deviation; they are all summaries of a large number of values.\n\n\n\nSummarize diagram from Data Wrangling with dplyr and tidyr cheatsheet\n\n\n\n\n\nAnother summarize diagram from Data Wrangling with dplyr and tidyr\n\n\nLet’s calculate the mean and the standard deviation of the temperature variable temp in the weather data frame included in the nycflights13 package (See Appendix A). We’ll do this in one step using the summarize() function from the dplyr package and save the results in a new data frame summary_temp with columns/variables mean and the std_dev. Note you can also use the UK spelling of summarise().\nThe weather data frame’s many rows will bow be collapsed into a single row of just the summary values, in this case the mean and standard deviation:\n\nsummary_temp <- weather %>% \n  summarize(mean = mean(temp), std_dev = sd(temp))\n\nsummary_temp\n\n# A tibble: 1 × 2\n   mean std_dev\n  <dbl>   <dbl>\n1    NA      NA\n\n\nWhy are the values returned NA? As we saw in Section 2.3.1 when creating the scatterplot of departure and arrival delays for alaska_flights, NA is how R encodes missing values where NA indicates “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult? Perhaps there was an erroneous value that someone entered that has been correct to read as missing? You’ll often encounter issues with missing values when working with real data.\nGoing back to our summary_temp output above, by default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, then NA is returned. To work around this fact, you can set the na.rm argument to TRUE, where rm is short for “remove”; this will ignore any NA missing values and only return the summary value for all non-missing values.\nThe code below computes the mean and standard deviation of all non-missing values of temp. Notice how the na.rm=TRUE are used as arguments to the mean() and sd() functions individually, and not to the summarize() function.\n\nsummary_temp <- weather %>% \n  summarize(mean = mean(temp, na.rm = TRUE), \n            std_dev = sd(temp, na.rm = TRUE))\n\nsummary_temp\n\n# A tibble: 1 × 2\n   mean std_dev\n  <dbl>   <dbl>\n1  55.3    17.8\n\n\nHowever, one needs to be cautious whenever ignoring missing values as we’ve done above. In the upcoming Learning Checks we’ll consider the possible ramifications of blindly sweeping rows with missing values “under the rug.” This is in fact why the na.rm argument to any summary statistic function in R has is set to FALSE by default; in other words, do not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should by mindful of this missingness and any potential causes of this missingness throughout your analysis.\nWhat are other functions for summary statistics can we use inside the summarize() verb? We can use any function in R that takes many values and returns just one. Here are just a few:\n\nmean(): the mean AKA the average\nsd(): the standard deviation, which is a measure of spread\nmin() and max(): the minimum and maximum values respectively\nIQR(): Interquartile range\nsum(): the sum\nn(): a count of the number of rows/observations in each group. This particular summary function will make more sense when group_by() is covered in Section 3.4.\n\n\n\n\n\n\n\n🎯 Learning Check 3.2\n\n\n\n\n\nSay a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.3\n\n\n\n\n\nModify the above summarize() function to create summary_temp to also use the n() summary function: summarize(count = n()).\nWhat does the returned value correspond to?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.4\n\n\n\n\n\nWhy doesn’t the following code work?\n\nsummary_temp <- weather %>%   \n  summarize(mean = mean(temp, na.rm = TRUE)) %>% \n  summarize(std_dev = sd(temp, na.rm = TRUE))\n\nHiint: Run the code line by line instead of all at once, and then look at the data. In other words, run summary_temp <- weather %>% summarize(mean = mean(temp, na.rm = TRUE)) first."
  },
  {
    "objectID": "03-wrangling.html#sec-groupby",
    "href": "03-wrangling.html#sec-groupby",
    "title": "3  Data Wrangling",
    "section": "3.4 group_by() rows",
    "text": "3.4 group_by() rows\n\n\n\nGroup by and summarize diagram from Data Wrangling with dplyr and tidyr cheatsheet\n\n\nSay instead of the a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately? In other words, we would like to compute the mean temperature split by month AKA sliced by month AKA aggregated by month. We can do this by “grouping” temperature observations by the values of another variable, in this case by the 12 values of the variable month. Run the following code:\n\nsummary_monthly_temp <- weather %>% \n  group_by(month) %>% \n  summarize(mean = mean(temp, na.rm = TRUE), \n            std_dev = sd(temp, na.rm = TRUE))\n\nsummary_monthly_temp\n\n# A tibble: 12 × 3\n   month  mean std_dev\n   <int> <dbl>   <dbl>\n 1     1  35.6   10.2 \n 2     2  34.3    6.98\n 3     3  39.9    6.25\n 4     4  51.7    8.79\n 5     5  61.8    9.68\n 6     6  72.2    7.55\n 7     7  80.1    7.12\n 8     8  74.5    5.19\n 9     9  67.4    8.47\n10    10  60.1    8.85\n11    11  45.0   10.4 \n12    12  38.4    9.98\n\n\nThis code is identical to the previous code that created summary_temp, but with an extra group_by(month) added before the summarize(). Grouping the weather dataset by month and then applying the summarize() functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year.\nIt is important to note that the group_by() function doesn’t change data frame by itself. Rather it changes the meta-data, or data about the data, specifically the group structure. It is only after we apply the summarize() function that the data frame changes. For example, let’s consider the diamonds data frame included in the ggplot2 package. Run this code, specifically in the console:\n\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# … with 53,930 more rows\n\n\nObserve that the first line of the output reads # A tibble: 53,940 x 10. This is an example of meta-data, in this case the number of observations/rows and variables/columns in diamonds. The actual data itself are the subsequent table of values.\nNow let’s pipe the diamonds data frame into group_by(cut). Run this code, specifically in the console:\n\ndiamonds %>% \n  group_by(cut)\n\n# A tibble: 53,940 × 10\n# Groups:   cut [5]\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# … with 53,930 more rows\n\n\nObserve that now there is additional meta-data: # Groups: cut [5] indicating that the grouping structure meta-data has been set based on the 5 possible values AKA levels of the categorical variable cut: \"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\". On the other hand observe that the data has not changed: it is still a table of 53,940 \\(\\times\\) 10 values.\nOnly by combining a group_by() with another data wrangling operation, in this case summarize() will the actual data be transformed.\n\ndiamonds %>% \n  group_by(cut) %>% \n  summarize(avg_price = mean(price))\n\n# A tibble: 5 × 2\n  cut       avg_price\n  <ord>         <dbl>\n1 Fair          4359.\n2 Good          3929.\n3 Very Good     3982.\n4 Premium       4584.\n5 Ideal         3458.\n\n\nIf we would like to remove this group structure meta-data, we can pipe the resulting data frame into the ungroup() function. Observe how the # Groups: cut [5] meta-data is no longer present. Run this code, specifically in the console:\n\ndiamonds %>% \n  group_by(cut) %>% \n  ungroup()\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# … with 53,930 more rows\n\n\nLet’s now revisit n() thw counting summary function introduced in the previous section. For example, suppose we’d like to count how many flights departed each of the three airports in New York City:\n\nby_origin <- flights %>% \n  group_by(origin) %>% \n  summarize(count = n())\n\nby_origin\n\n# A tibble: 3 × 2\n  origin  count\n  <chr>   <int>\n1 EWR    120835\n2 JFK    111279\n3 LGA    104662\n\n\nWe see that Newark (\"EWR\") had the most flights departing in 2013 followed by \"JFK\" and lastly by LaGuardia (\"LGA\"). Note there is a subtle but important difference between sum() and n(); While sum() returns the sum of a numerical variable, n() returns counts of the the number of rows/observations.\n\n3.4.1 Grouping by more than one variable\nYou are not limited to grouping by one variable! Say you wanted to know the number of flights leaving each of the three New York City airports for each month, we can also group by a second variable month: group_by(origin, month). We see there are 36 rows to by_origin_monthly because there are 12 months for 3 airports (EWR, JFK, and LGA).\n\nby_origin_monthly <- flights %>% \n  group_by(origin, month) %>% \n  summarize(count = n())\n\nby_origin_monthly\n\n# A tibble: 36 × 3\n# Groups:   origin [3]\n   origin month count\n   <chr>  <int> <int>\n 1 EWR        1  9893\n 2 EWR        2  9107\n 3 EWR        3 10420\n 4 EWR        4 10531\n 5 EWR        5 10592\n 6 EWR        6 10175\n 7 EWR        7 10475\n 8 EWR        8 10359\n 9 EWR        9  9550\n10 EWR       10 10104\n# … with 26 more rows\n\n\nWhy do we group_by(origin, month) and not group_by(origin) and then group_by(month)? Let’s investigate:\n\nby_origin_monthly_incorrect <- flights %>% \n  group_by(origin) %>% \n  group_by(month) %>% \n  summarize(count = n())\n\nby_origin_monthly_incorrect\n\n# A tibble: 12 × 2\n   month count\n   <int> <int>\n 1     1 27004\n 2     2 24951\n 3     3 28834\n 4     4 28330\n 5     5 28796\n 6     6 28243\n 7     7 29425\n 8     8 29327\n 9     9 27574\n10    10 28889\n11    11 27268\n12    12 28135\n\n\nWhat happened here is that the second group_by(month) overrode the group structure meta-data of the first group_by(origin), so that in the end we are only grouping by month. The lesson here is if you want to group_by() two or more variables, you should include all these variables in a single group_by() function call.\n\n\n\n\n\n\n🎯 Learning Check 3.5\n\n\n\n\n\n\n\n\n\n(LC3.6) Recall from Chapter @ref(viz) when we looked at plots of temperatures by months in NYC. What does the standard deviation column in the summary_monthly_temp data frame tell us about temperatures in New York City throughout the year?\n\n\n\n\n\n\n🎯 Learning Check 3.7\n\n\n\n\n\nWhat code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.8\n\n\n\n\n\nRecreate by_monthly_origin, but instead of grouping via group_by(origin, month), group variables in a different order group_by(month, origin).\nWhat differs in the resulting dataset?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.9\n\n\n\n\n\nHow could we identify how many flights left each of the three airports for each carrier?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.10\n\n\n\n\n\nHow does the filter operation differ from a group_by followed by a summarize?"
  },
  {
    "objectID": "03-wrangling.html#sec-mutate",
    "href": "03-wrangling.html#sec-mutate",
    "title": "3  Data Wrangling",
    "section": "3.5 mutate existing variables",
    "text": "3.5 mutate existing variables\n\n\n\nMutate diagram from Data Wrangling with dplyr and tidyr cheatsheet\n\n\nAnother common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius °C and not degrees Farenheit °F. The formula to convert temperatures from °F to °C is:\n\\[\n\\text{temp in C} = \\frac{\\text{temp in F} - 32}{1.8}\n\\]\nWe can apply this formula to the temp variable using the mutate() function, which takes existing variables and mutates them to create new ones.\n\nweather <- weather %>% \n  mutate(temp_in_C = (temp-32)/1.8)\n\nView(weather)\n\n\n\n\nNote that we have overwritten the original weather data frame with a new version that now includes the additional variable temp_in_C. In other words, the mutate() command outputs a new data frame which then gets saved over the original weather data frame. Furthermore, note how in mutate() we used temp_in_C = (temp-32)/1.8 to create a new variable temp_in_C.\nWhy did we overwrite the data frame weather instead of assigning the result to a new data frame like weather_new, but on the other hand why did we not overwrite temp, but instead created a new variable called temp_in_C? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames. On the other hand, had we used mutate(temp = (temp-32)/1.8) instead of mutate(temp_in_C = (temp-32)/1.8), we would have overwritten the original variable temp and lost its values.\nLet’s compute average monthly temperatures in both °F and °C using the similar group_by() and summarize() code as in the previous section.\n\nsummary_monthly_temp <- weather %>% \n  group_by(month) %>% \n  summarize(\n    mean_temp_in_F = mean(temp, na.rm = TRUE), \n    mean_temp_in_C = mean(temp_in_C, na.rm = TRUE)\n    )\n\nsummary_monthly_temp\n\n# A tibble: 12 × 3\n   month mean_temp_in_F mean_temp_in_C\n   <int>          <dbl>          <dbl>\n 1     1           35.6           2.02\n 2     2           34.3           1.26\n 3     3           39.9           4.38\n 4     4           51.7          11.0 \n 5     5           61.8          16.6 \n 6     6           72.2          22.3 \n 7     7           80.1          26.7 \n 8     8           74.5          23.6 \n 9     9           67.4          19.7 \n10    10           60.1          15.6 \n11    11           45.0           7.22\n12    12           38.4           3.58\n\n\nLet’s consider another example. Passengers are often frustrated when their flights depart late, but change their mood a bit if pilots can make up some time during the flight to get them to their destination close to the original arrival time. This is commonly referred to as “gain” and we will create this variable using the mutate() function.\n\nflights <- flights %>% \n  mutate(gain = dep_delay - arr_delay)\n\nLet’s take a look at dep_delay, arr_delay, and the resulting gain variables for the first 5 rows in our new flights data frame:\n\n\n# A tibble: 5 × 3\n  dep_delay arr_delay  gain\n      <dbl>     <dbl> <dbl>\n1         2        11    -9\n2         4        20   -16\n3         2        33   -31\n4        -1       -18    17\n5        -6       -25    19\n\n\nThe flight in the first row departed 2 minutes late but arrived 11 minutes late, so its “gained time in the air” is actually a loss of 9 minutes, hence its gain is -9. Contrast this to the flight in the fourth row which departed a minute early (dep_delay of -1) but arrived 18 minutes early (arr_delay of -18), so its “gained time in the air” is 17 minutes, hence its gain is +17.\nLet’s look at summary measures of this gain variable and even plot it in the form of a histogram:\n\ngain_summary <- flights %>% \n  summarize(\n    min = min(gain, na.rm = TRUE),\n    q1 = quantile(gain, 0.25, na.rm = TRUE),\n    median = quantile(gain, 0.5, na.rm = TRUE),\n    q3 = quantile(gain, 0.75, na.rm = TRUE),\n    max = max(gain, na.rm = TRUE),\n    mean = mean(gain, na.rm = TRUE),\n    sd = sd(gain, na.rm = TRUE),\n    missing = sum(is.na(gain))\n  )\n\ngain_summary\n\n\n\n\n\n\nmin\nq1\nmedian\nq3\nmax\nmean\nsd\nmissing\n\n\n\n\n-196\n-3\n7\n17\n109\n5.66\n18\n9430\n\n\n\n\n\nWe’ve recreated the summary function we saw in Chapter 2 here using the summarize function in dplyr.\n\nggplot(data = flights, mapping = aes(x = gain)) +\n  geom_histogram(color = \"white\", bins = 20)\n\n\n\n\nFigure 3.2: Histogram of gain variable\n\n\n\n\nWe can also create multiple columns at once and even refer to columns that were just created in a new column. Hadley and Garrett produce one such example in Chapter 5 of “R for Data Science” (Grolemund and Wickham 2016):\n\nflights <- flights %>% \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours\n  )\n\n\n\n\n\n\n\n🎯 Learning Check 3.11\n\n\n\n\n\nWhat do positive values of the gain variable in flights correspond to? What about negative values? And what about a zero value?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.12\n\n\n\n\n\nCould we create the dep_delay and arr_delay columns by simply subtracting dep_time from sched_dep_time and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in flights.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.13\n\n\n\n\n\nWhat can we say about the distribution of gain? Describe it in a few sentences using the plot and the gain_summary data frame values."
  },
  {
    "objectID": "03-wrangling.html#sec-arrange",
    "href": "03-wrangling.html#sec-arrange",
    "title": "3  Data Wrangling",
    "section": "3.6 arrange() and sort rows",
    "text": "3.6 arrange() and sort rows\nOne of the most common tasks people working with data would like to perform is sort the data frame’s rows in alphanumeric order of the values in a variable/column. For example, when calculating a median by hand requires you to first sort the data from the smallest to highest in value and then identify the “middle” value. The dplyr package has a function called arrange() that we will use to sort/reorder a data frame’s rows according to the values of the specified variable. This is often used after we have used the group_by() and summarize() functions as we will see.\nLet’s suppose we were interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013:\n\nfreq_dest <- flights %>% \n  group_by(dest) %>% \n  summarize(num_flights = n())\n\nfreq_dest\n\n# A tibble: 105 × 2\n   dest  num_flights\n   <chr>       <int>\n 1 ABQ           254\n 2 ACK           265\n 3 ALB           439\n 4 ANC             8\n 5 ATL         17215\n 6 AUS          2439\n 7 AVL           275\n 8 BDL           443\n 9 BGR           375\n10 BHM           297\n# … with 95 more rows\n\n\nObserve that by default the rows of the resulting freq_dest data frame are sorted in alphabetical order of dest destination. Say instead we would like to see the same data, but sorted from the most to the least number of flights num_flights instead:\n\nfreq_dest %>% \n  arrange(num_flights)\n\n# A tibble: 105 × 2\n   dest  num_flights\n   <chr>       <int>\n 1 LEX             1\n 2 LGA             1\n 3 ANC             8\n 4 SBN            10\n 5 HDN            15\n 6 MTJ            15\n 7 EYW            17\n 8 PSP            19\n 9 JAC            25\n10 BZN            36\n# … with 95 more rows\n\n\nThis is actually giving us the opposite of what we are looking for: the rows are sorted with the least frequent destination airports displayed first. To switch the ordering to be descending instead of ascending we use the desc() function, which is short for “descending”:\n\nfreq_dest %>% \n  arrange(desc(num_flights))\n\n# A tibble: 105 × 2\n   dest  num_flights\n   <chr>       <int>\n 1 ORD         17283\n 2 ATL         17215\n 3 LAX         16174\n 4 BOS         15508\n 5 MCO         14082\n 6 CLT         14064\n 7 SFO         13331\n 8 FLL         12055\n 9 MIA         11728\n10 DCA          9705\n# … with 95 more rows\n\n\nIn other words, arrange() sorts in ascending order by default unless you override this default behavior by using desc()."
  },
  {
    "objectID": "03-wrangling.html#sec-joins",
    "href": "03-wrangling.html#sec-joins",
    "title": "3  Data Wrangling",
    "section": "3.7 join data frames",
    "text": "3.7 join data frames\nAnother common data transformation task is “joining” or “merging” two different datasets. For example in the flights data frame the variable carrier lists the carrier code for the different flights. While the corresponding airline names for \"UA\" and \"AA\" might be somewhat easy to guess (United and American Airlines), what airlines have codes? \"VX\", \"HA\", and \"B6\"? This information is provided in a separate data frame airlines.\n\nView(airlines)\n\nWe see that in airlines, carrier is the carrier code while name is the full name of the airline company. Using this table, we can see that \"VX\", \"HA\", and \"B6\" correspond to Virgin America, Hawaiian Airlines, and JetBlue respectively. However, wouldn’t it be nice to have all this information in a single data frame instead of two separate data frames? We can do this by “joining” i.e. “merging” the flights and airlines data frames.\nNote that the values in the variable carrier in the flights data frame match the values in the variable carrier in the airlines data frame. In this case, we can use the variable carrier as a key variable to match the rows of the two data frames. Key variables are almost always identification variables that uniquely identify the observational units. This ensures that rows in both data frames are appropriately matched during the join. Hadley and Garrett (Grolemund and Wickham 2016) created the following diagram to help us understand how the different datasets are linked by various key variables:\n\n\n\nFigure 3.3: Data relationships in nycflights13 from R for Data Science\n\n\n\n3.7.1 Matching “key” variable names\nIn both the flights and airlines data frames, the key variable we want to join/merge/match the rows of the two data frames by have the same name: carriers. We make use of the inner_join() function to join the two data frames, where the rows will be matched by the variable carrier.\n\nflights_joined <- flights %>% \n  inner_join(airlines, by = \"carrier\")\n\nView(flights)\nView(flights_joined)\n\nObserve that the flights and flights_joined data frames are identical except that flights_joined has an additional variable name whose values correspond to the airline company names drawn from the airlines data frame.\nA visual representation of the inner_join() is given below (Grolemund and Wickham 2016). There are other types of joins available (such as left_join(), right_join(), outer_join(), and anti_join()), but the inner_join() will solve nearly all of the problems you’ll encounter in this book.\n\n\n\nFigure 3.4: Diagram of inner join from R for Data Science\n\n\n\n\n3.7.2 Different “key” variable names\nSay instead you are interested in the destinations of all domestic flights departing NYC in 2013 and ask yourself:\n\n“What cities are these airports in?”\n“Is \"ORD\" Orlando?”\n“Where is \"FLL\"?\n\nThe airports data frame contains airport codes:\n\nView(airports)\n\nHowever, considering the visual representation (Figure 3.3) of the relations between the datasets airports and flights, we see that:\n\nthe airports data frame the airport code is in the variable faa\nthe flights data frame the airport codes are in the variables origin and dest\n\nWe need to join these two data frames soo that we can identify the destination cities. For example, our inner_join() operation will use the by = c(\"dest\" = \"faa\") argument, which allows us to join two data frames where the key variable has a different name:\n\nflights_with_airport_names <-  flights %>% \n  inner_join(airports, by = c(\"dest\" = \"faa\"))\n\nView(flights_with_airport_names)\n\nLet’s construct the sequence of commands that computes the number of flights from NYC to each destination, but also includes information about each destination airport:\n\nnamed_dests <- flights %>%\n  group_by(dest) %>%\n  summarize(num_flights = n()) %>%\n  arrange(desc(num_flights)) %>%\n  inner_join(airports, by = c(\"dest\" = \"faa\")) %>%\n  rename(airport_name = name)\n\nnamed_dests\n\n# A tibble: 101 × 9\n   dest  num_flights airport_name             lat    lon   alt    tz dst   tzone\n   <chr>       <int> <chr>                  <dbl>  <dbl> <dbl> <dbl> <chr> <chr>\n 1 ORD         17283 Chicago Ohare Intl      42.0  -87.9   668    -6 A     Amer…\n 2 ATL         17215 Hartsfield Jackson At…  33.6  -84.4  1026    -5 A     Amer…\n 3 LAX         16174 Los Angeles Intl        33.9 -118.    126    -8 A     Amer…\n 4 BOS         15508 General Edward Lawren…  42.4  -71.0    19    -5 A     Amer…\n 5 MCO         14082 Orlando Intl            28.4  -81.3    96    -5 A     Amer…\n 6 CLT         14064 Charlotte Douglas Intl  35.2  -80.9   748    -5 A     Amer…\n 7 SFO         13331 San Francisco Intl      37.6 -122.     13    -8 A     Amer…\n 8 FLL         12055 Fort Lauderdale Holly…  26.1  -80.2     9    -5 A     Amer…\n 9 MIA         11728 Miami Intl              25.8  -80.3     8    -5 A     Amer…\n10 DCA          9705 Ronald Reagan Washing…  38.9  -77.0    15    -5 A     Amer…\n# … with 91 more rows\n\n\nIn case you didn’t know, \"ORD\" is the airport code of Chicago O’Hare airport and \"FLL\" is the main airport in Fort Lauderdale, Florida, which we can now see in the airport_name variable in the resulting named_dests data frame.\n\n\n3.7.3 Multiple “key” variables\nSay instead we are in a situation where we need to join by multiple variables. For example, in Figure 3.3 above we see that in order to join the flights and weather data frames, we need more than one key variable: year, month, day, hour, and origin. This is because the combination of these 5 variables act to uniquely identify each observational unit in the weather data frame: hourly weather recordings at each of the 3 NYC airports.\nWe achieve this by specifying a vector of key variables to join by using the c() function for “combine” or “concatenate” that we saw earlier:\n\nflights_weather_joined <- flights %>%\n  inner_join(weather, by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\"))\n\nView(flights_weather_joined)\n\n\n\n\n\n\n\n🎯 Learning Check 3.14\n\n\n\n\n\nLooking at Figure 3.3, when joining flights and weather (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of year, month, day, hour, and origin, and not just hour?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.15\n\n\n\n\n\nWhat surprises you about the top 10 destinations from NYC in 2013?\n\n\n\n\n\n3.7.4 Normal forms\nThe data frames included in the nycflights13 package are in a form that minimizes redundancy of data. For example, the flights data frame only saves the carrier code of the airline company; it does not include the actual name of the airline. For example the first row of flights has carrier equal to UA, but does it does not include the airline name “United Air Lines Inc.” The names of the airline companies are included in the name variable of the airlines data frame. In order to have the airline company name included in flights, we could join these two data frames as follows:\n\njoined_flights <- flights %>% \n  inner_join(airlines, by = \"carrier\")\n\nView(joined_flights)\n\nWe are capable of performing this join because each of the data frames have keys in common to relate one to another: the carrier variable in both the flights and airlines data frames. The key variable(s) that we join are often identification variables we mentioned previously.\nThis is an important property of what’s known as normal forms of data. The process of decomposing data frames into less redundant tables without losing information is called normalization. More information is available on Wikipedia.\n\n\n\n\n\n\n🎯 Learning Check 3.16\n\n\n\n\n\nWhat are some advantages of data in normal forms? What are some disadvantages?"
  },
  {
    "objectID": "03-wrangling.html#sec-other-verbs",
    "href": "03-wrangling.html#sec-other-verbs",
    "title": "3  Data Wrangling",
    "section": "3.8 Other verbs",
    "text": "3.8 Other verbs\nHere are some other useful data wrangling verbs that might come in handy:\n\nselect() only a subset of variables/columns\nrename() variables/columns to have new names\nReturn only the top_n() values of a variable\n\n\n3.8.1 select() variables\n\n\n\nSelect diagram from Data Wrangling with dplyr and tidyr cheatsheet\n\n\nWe’ve seen that the flights data frame in the nycflights13 package contains 19 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package:\n\nglimpse(flights)\n\nHowever, say you only need two of these variables, say carrier and flight. You can select() these two variables:\n\nflights %>% \n  select(carrier, flight)\n\nThis function makes exploring data frames with a very large number of variables easier for humans to process by restricting consideration to only those we care about, like our example with carrier and flight above. This might make viewing the dataset using the View() spreadsheet viewer more digestible. However, as far as the computer is concerned, it doesn’t care how many additional variables are in the data frame in question, so long as carrier and flight are included.\nLet’s say instead you want to drop i.e deselect certain variables. For example, take the variable year in the flights data frame. This variable isn’t quite a “variable” in the sense that all the values are 2013 i.e. it doesn’t change. Say you want to remove the year variable from the data frame; we can deselect year by using the - sign:\n\nflights_no_year <- flights %>% \n  select(-year)\n\nglimpse(flights_no_year)\n\nAnother way of selecting columns/variables is by specifying a range of columns:\n\nflight_arr_times <- flights %>% \n  select(month:day, arr_time:sched_arr_time)\n\nflight_arr_times\n\nThe select() function can also be used to reorder columns in combination with the everything() helper function. Let’s suppose we’d like the hour, minute, and time_hour variables, which appear at the end of the flights dataset, to appear immediately after the year, month, and day variables while keeping the rest of the variables. In the code below everything() picks up all remaining variables.\n\nflights_reorder <- flights %>% \n  select(year, month, day, hour, minute, time_hour, everything())\n\nglimpse(flights_reorder)\n\nLastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/column that match those conditions. For example:\n\nflights_begin_a <- flights %>% \n  select(starts_with(\"a\"))\n\nflights_begin_a\n\n\nflights_delays <- flights %>% \n  select(ends_with(\"delay\"))\n\nflights_delays\n\n\nflights_time <- flights %>% \n  select(contains(\"time\"))\n\nflights_time\n\n\n\n3.8.2 rename() variables\nAnother useful function is rename(), which as you may have guessed renames one column to another name. Suppose we want dep_time and arr_time to be departure_time and arrival_time instead in the flights_time data frame:\n\nflights_time <- flights %>% \n  select(contains(\"time\")) %>% \n  rename(departure_time = dep_time,\n         arrival_time = arr_time)\n\nglimpse(flights_time)\n\nNote that in this case we used a single = sign within the rename(), for example departure_time = dep_time. This is because we are not testing for equality like we would using ==, but instead we want to assign a new variable departure_time to have the same values as dep_time and then delete the variable dep_time. It’s easy to forget if the new name comes before or after the equals sign. I usually remember this as “New Before, Old After” or NBOA.\n\n\n3.8.3 slice() data by a variable\nWe can return observations with maximum or minimum values of a variable using the slice_mac() or slice_min(). For example, we can get the observations the top 10 destination airports using the example from Section 3.7.2. Observe that we set the number of values to return to n = 10 and wt = num_flights to indicate that we want the rows of corresponding to the top 10 values of num_flights. See the help file for top_n() by running ?top_n for more information.\n\nnamed_dests %>% \n  slice_max(n = 10, order_by =  num_flights)\n\nnamed_dests %>% \n  slice_min(n = 10, order_by =  num_flights)\n\n\n\n\n\n\n\n🎯 Learning Check 3.17\n\n\n\n\n\nWhat are some ways to select all three of the dest, air_time, and distance variables from flights? Give the code showing how to do this in at least three different ways.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.18\n\n\n\n\n\nHow could one use starts_with, ends_with, and contains to select columns from the flights data frame? Provide three different examples in total: one for starts_with, one for ends_with, and one for contains.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.19\n\n\n\n\n\nWhy might we want to use the select function on a data frame?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.20\n\n\n\n\n\nCreate a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013."
  },
  {
    "objectID": "03-wrangling.html#sec-wrangling-conclusion",
    "href": "03-wrangling.html#sec-wrangling-conclusion",
    "title": "3  Data Wrangling",
    "section": "3.9 Conclusion",
    "text": "3.9 Conclusion\n\n3.9.1 Summary table\nLet’s recap our data wrangling verbs in Table 3.1. Using these verbs and the pipe %>% operator from @sec-piping, you’ll be able to write easily legible code to perform almost all the data wrangling necessary for the rest of this book.\n\n\n\n\nTable 3.1: Summary of data wrangling verbs\n\n\n\n\n\n\nVerb\nData wrangling operation\n\n\n\n\nfilter()\nPick out a subset of rows\n\n\nsummarize()\nSummarize many values to one using a summary statistic function like mean(), median(), etc.\n\n\ngroup_by()\nAdd grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data\n\n\nmutate()\nCreate new variables by mutating existing ones\n\n\narrange()\nArrange rows of a data variable in ascending (default) or descending order\n\n\ninner_join()\nJoin/merge two data frames, matching rows by a key variable\n\n\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 3.21\n\n\n\n\n\nLet’s now put your newly acquired data wrangling skills to the test!\nAn airline industry measure of a passenger airline’s capacity is the available seat miles, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights. So for example say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be 2 \\(\\times\\) 10 \\(\\times\\) 500 \\(+\\) 3 \\(\\times\\) 20 \\(\\times\\) 1000 = 70,000 seat miles.\nUsing the datasets included in the nycflights13 package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints:\n\nCrucial: Unless you are very confident in what you are doing, it is worthwhile to not start coding right away, but rather first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level pseudocode that is informal yet detailed enough to articulate what you are doing. This way you won’t confuse what you are trying to do (the algorithm) with how you are going to do it (writing dplyr code).\nTake a close look at all the datasets using the View() function: flights, weather, planes, airports, and airlines to identify which variables are necessary to compute available seat miles.\nFigure 3.3 above showing how the various datasets can be joined will also be useful.\nConsider the data wrangling verbs in Table 3.1) as your toolbox!\n\n\n\n\n\n\n3.9.2 Additional resources\nIf you want to further unlock the power of the dplyr package for data wrangling, we suggest you that you check out RStudio’s “Data Transformation with dplyr” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular more-intermediate level and advanced data wrangling functions, while providing quick and easy to read visual descriptions.\nYou can access this cheatsheet by going to the RStudio Menu Bar -> Help -> Cheatsheets -> “Data Transformation with dplyr”:\n\n\n\nData Transformation with dplyr cheatsheat\n\n\nOn top of data wrangling verbs and examples we presented in this section, if you’d like to see more examples of using the dplyr package for data wrangling check out Chapter 5 of Garrett Grolemund and Hadley Wickham’s and Garrett’s book (Grolemund and Wickham 2016).\n\n\n3.9.3 What’s to come?\nSo far in this book, we’ve explored, visualized, and wrangled data saved in data frames that are in spreadsheet-type format: rectangular with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing the observations.\nWe’ll see in Chapter 4 that there are actually two ways to represent data in spreadsheet-type rectangular format: 1) “wide” format and 2) “tall/narrow” format also known in R circles as “tidy” format. While the distinction between “tidy” and non-“tidy” formatted data is very subtle, it has very important implications for whether or not we can use the ggplot2 package for data visualization and the dplyr package for data wrangling.\nFurthermore, we’ve only explored, visualized, and wrangled data saved within R packages. What if you have spreadsheet data saved in a Microsoft Excel, Google Sheets, or “Comma-Separated Values” (CSV) file that you would like to analyze? In Chapter 4, we’ll show you how to import this data into R using the readr package.\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. 2016. R for Data Science. http://r4ds.had.co.nz/."
  },
  {
    "objectID": "04-tidy.html",
    "href": "04-tidy.html",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "",
    "text": "In Subsection 1.2.2 we introduced the concept of a data frame: a rectangular spreadsheet-like representation of data in R where the rows correspond to observations and the columns correspond to variables describing each observation. In Section 1.4, we started exploring our first data frame: the flights data frame included in the nycflights13 package. In Chapter 2 we created visualizations based on the data included in flights and other data frames such as weather. In Chapter 3, we learned how to wrangle data, in other words take existing data frames and transform/ modify them to suit our analysis goals.\nIn this final chapter of the “Data Science via the tidyverse” portion of the book, we extend some of these ideas by discussing a type of data formatting called “tidy” data. You will see that having data stored in “tidy” format is about more than what the colloquial definition of the term “tidy” might suggest: having your data “neatly organized.” Instead, we define the term “tidy” in a more rigorous fashion, outlining a set of rules by which data can be stored, and the implications of these rules for analyses.\nAlthough knowledge of this type of data formatting was not necessary for our treatment of data visualization in Chapter 2 and data wrangling in Chapter 3 since all the data was already in “tidy” format, we’ll now see this format is actually essential to using the tools we covered in these two chapters. Furthermore, it will also be useful for all subsequent chapters in this book when we cover regression and statistical inference. First however, we’ll show you how to import spreadsheet data for use in R."
  },
  {
    "objectID": "04-tidy.html#packages-needed",
    "href": "04-tidy.html#packages-needed",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "Packages Needed",
    "text": "Packages Needed\nLet’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section @ref(packages) for information on how to install and load R packages.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(nycflights13)\nlibrary(fivethirtyeight)"
  },
  {
    "objectID": "04-tidy.html#sec-csv",
    "href": "04-tidy.html#sec-csv",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "4.1 Importing data",
    "text": "4.1 Importing data\nUp to this point, we’ve almost entirely used data stored inside of an R package. Say instead you have your own data saved on your computer or somewhere online? How can you analyze this data in R? Spreadsheet data is often saved in one of the following formats:\n\nA Comma Separated Values .csv file. You can think of a .csv file as a bare-bones spreadsheet where:\n\nEach line in the file corresponds to one row of data/one observation.\nValues for each line are separated with commas. In other words, the values of different variables are separated by commas.\nThe first line is often, but not always, a header row indicating the names of the columns/variables.\n\nAn Excel .xlsx file. This format is based on Microsoft’s proprietary Excel software. As opposed to a bare-bones .csv files, .xlsx Excel files contain a lot of meta-data, or put more simply, data about the data. (Recall we saw a previous example of meta-data in Section 3.4 when adding “group structure” meta-data to a data frame by using the group_by() verb.) Some examples of spreadsheet meta-data include the use of bold and italic fonts, colored cells, different column widths, and formula macros.\nA Google Sheets file, which is a “cloud” or online-based way to work with a spreadsheet. Google Sheets allows you to download your data in both comma separated values .csv and Excel .xlsx formats however: go to the Google Sheets menu bar -> File -> Download as -> Select “Microsoft Excel” or “Comma-separated values.”\n\nWe’ll cover two methods for importing .csv and .xlsx spreadsheet data in R: one using the R console and the other using RStudio’s graphical user interface, abbreviated a GUI.\n\n4.1.1 Using the console\nFirst, let’s import a Comma Separated Values .csv file of data directly off the internet. The .csv file dem_score.csv accessible at https://moderndive.com/data/dem_score.csv contains ratings of the level of democracy in different countries spanning 1952 to 1992. Let’s use the read_csv() function from the readr package to read it off the web, import it into R, and save it in a data frame called dem_score\n\nlibrary(readr)\ndem_score <- read_csv(\"https://moderndive.com/data/dem_score.csv\")\ndem_score\n\n\n\n# A tibble: 96 × 10\n   country    `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`\n   <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n 1 Albania        -9     -9     -9     -9     -9     -9     -9     -9      5\n 2 Argentina      -9     -1     -1     -9     -9     -9     -8      8      7\n 3 Armenia        -9     -7     -7     -7     -7     -7     -7     -7      7\n 4 Australia      10     10     10     10     10     10     10     10     10\n 5 Austria        10     10     10     10     10     10     10     10     10\n 6 Azerbaijan     -9     -7     -7     -7     -7     -7     -7     -7      1\n 7 Belarus        -9     -7     -7     -7     -7     -7     -7     -7      7\n 8 Belgium        10     10     10     10     10     10     10     10     10\n 9 Bhutan        -10    -10    -10    -10    -10    -10    -10    -10    -10\n10 Bolivia        -4     -3     -3     -4     -7     -7      8      9      9\n# … with 86 more rows\n\n\nIn this dem_score data frame, the minimum value of -10 corresponds to a highly autocratic nation whereas a value of 10 corresponds to a highly democratic nation. We’ll revisit the dem_score data frame in a case study in the upcoming Section 4.3.\nNote that the read_csv() function included in the readr package is different than the read.csv() function that comes installed with R by default. While the difference in the names might seem near meaningless (an _ instead of a .), the read_csv() function is in our opinion easier to use since it can more easily read data off the web and generally imports data at a much faster speed.\n\n\n4.1.2 Using RStudio’s interface\nLet’s read in the exact same data saved in Excel format, but this time via RStudio’s graphical interface instead of via the R console. First download the Excel file dem_score.xlsx by clicking here, then\n\nGo to the Files panel of RStudio.\nNavigate to the directory i.e. folder on your computer where the downloaded dem_score.xlsx Excel file is saved.\nClick on dem_score.xlsx.\nClick “Import Dataset…”\n\nAt this point you should see an image like this:\n\nAfter clicking on the “Import” button on the bottom right RStudio, RStudio will save this spreadsheet’s data in a data frame called dem_score and display its contents in the spreadsheet viewer. Furthermore, note in the bottom right of the above image there exists a “Code Preview”: you can copy and paste this code to reload your data again later automatically instead of repeating the above manual point-and-click process."
  },
  {
    "objectID": "04-tidy.html#sec-tidy-data-ex",
    "href": "04-tidy.html#sec-tidy-data-ex",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "4.2 Tidy data",
    "text": "4.2 Tidy data\nLet’s now switch gears and learn about the concept of “tidy” data format by starting with a motivating example. Let’s consider the drinks data frame included in the fivethirtyeight data. Run the following:\n\ndrinks\n\n# A tibble: 193 × 5\n   country           beer_servings spirit_servings wine_servings total_litres_…¹\n   <chr>                     <int>           <int>         <int>           <dbl>\n 1 Afghanistan                   0               0             0             0  \n 2 Albania                      89             132            54             4.9\n 3 Algeria                      25               0            14             0.7\n 4 Andorra                     245             138           312            12.4\n 5 Angola                      217              57            45             5.9\n 6 Antigua & Barbuda           102             128            45             4.9\n 7 Argentina                   193              25           221             8.3\n 8 Armenia                      21             179            11             3.8\n 9 Australia                   261              72           212            10.4\n10 Austria                     279              75           191             9.7\n# … with 183 more rows, and abbreviated variable name\n#   ¹​total_litres_of_pure_alcohol\n\n\nAfter reading the help file by running ?drinks, we see that drinks is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed for 193 countries. This data was originally reported on the data journalism website FiveThirtyEight.com in Mona Chalabi’s article “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”\nLet’s apply some of the data wrangling verbs we learned in Chapter 3 on the drinks data frame. Let’s\n\nfilter() the drinks data frame to only consider 4 countries (the United States, China, Italy, and Saudi Arabia) then\nselect() all columns except total_litres_of_pure_alcohol by using - sign, then\nrename() the variables beer_servings, spirit_servings, and wine_servings to beer, spirit, and wine respectively\n\nand save the resulting data frame in drinks_smaller.\n\ndrinks_smaller <- drinks %>% \n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) %>% \n  select(-total_litres_of_pure_alcohol) %>% \n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\n\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nUsing the drinks_smaller data frame, how would we create the side-by-side AKA dodged barplot in Figure 4.1? Recall we saw barplots displaying two categorical variables in Section 2.8.3.\n\n\n\n\n\nFigure 4.1: Alcohol consumption in 4 countries\n\n\n\n\nLet’s break down the Grammar of Graphics:\n\nThe categorical variable country with four levels (China, Italy, Saudi Arabia, USA) would have to be mapped to the x-position of the bars.\nThe numerical variable servings would have to be mapped to the y-position of the bars, in other words the height of the bars.\nThe categorical variable type with three levels (beer, spirit, wine) who have to be mapped to the fill color of the bars.\n\nObserve however that drinks_smaller has three separate variables for beer, spirit, and wine, whereas in order to recreate the side-by-side AKA dodged barplot in Figure 4.1 we would need a single variable type with three possible values: beer, spirit, and wine, which we would then map to the fill aesthetic. In other words, for us to be able to create the barplot in Figure 4.1, our data frame would have to look like this:\n\ndrinks_smaller_tidy\n\n# A tibble: 12 × 3\n   country      type   servings\n   <chr>        <chr>     <int>\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nLet’s compare the drinks_smaller_tidy with the drinks_smaller data frame from earlier:\n\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nObserve that while drinks_smaller and drinks_smaller_tidy are both rectangular in shape and contain the same 12 numerical values (3 alcohol types \\(\\times\\) 4 countries), they are formatted differently. drinks_smaller is formatted in what’s known as “wide” format, whereas drinks_smaller_tidy is formatted in what’s known as “long/narrow”. In the context of using R, long/narrow format is also known as “tidy” format. Furthermore, in order to use the ggplot2 and dplyr packages for data visualization and data wrangling, your input data frames must be in “tidy” format. So all non-“tidy” data must be converted to “tidy” format first.\nBefore we show you how to convert non-“tidy” data frames like drinks_smaller to “tidy” data frames like drinks_smaller_tidy, let’s go over the explicit definition of “tidy” data.\n\n4.2.1 Definition of “tidy” data\nYou have surely heard the word “tidy” in your life:\n\n“Tidy up your room!”\n“Please write your homework in a tidy way so that it is easier to grade and to provide feedback.”\nMarie Kondo’s best-selling book The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing and Netflix TV series Tidying Up with Marie Kondo.\n“I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant\n\nWhat does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized”, “tidy” in the context of data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of tidy data here (Wickham 2014):\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.\nTidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\nTidy data graphic from R for Data Science\n\n\nFor example, say you have the following table of stock prices in Table 4.1:\n\n\n\n\nTable 4.1: Stock Prices (Non-Tidy Format)\n\n\n\n\n\n\n\n\nDate\nBoeing Stock Price\nAmazon Stock Price\nGoogle Stock Price\n\n\n\n\n2009-01-01\n$173.55\n$174.90\n$174.34\n\n\n2009-01-02\n$172.61\n$171.42\n$170.04\n\n\n\n\n\n\nAlthough the data are neatly organized in a rectangular spreadsheet-type format, they are not in tidy format because while there are three variables corresponding to three unique pieces of information (Date, Stock Name, and Stock Price), there are not three columns. In “tidy” data format each variable should be its own column, as shown in Table 4.2). Notice that both tables present the same information, but in different formats.\n\n\n\n\nTable 4.2: Stock Prices (Tidy Format)\n\n\nDate\nStock Name\nStock Price\n\n\n\n\n2009-01-01\nBoeing\n$173.55\n\n\n2009-01-01\nAmazon\n$174.90\n\n\n2009-01-01\nGoogle\n$174.34\n\n\n2009-01-02\nBoeing\n$172.61\n\n\n2009-01-02\nAmazon\n$171.42\n\n\n2009-01-02\nGoogle\n$170.04\n\n\n\n\n\n\nNow we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider the data in Table @ref(tab:tidy-stocks-2).\n\n\n\n\nDate, Boeing Price, Weather Data\n \n  \n    Date \n    Boeing Price \n    Weather \n  \n \n\n  \n    2009-01-01 \n    $173.55 \n    Sunny \n  \n  \n    2009-01-02 \n    $172.61 \n    Overcast \n  \n\n\n\n\n\nIn this case, even though the variable “Boeing Price” occurs just like in our non-“tidy” data in Table 4.1), the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Boeing stock price, and the weather that particular day.\n\n\n\n\n\n\n🎯 Learning Check 4.1\n\n\n\n\n\nWhat are common characteristics of “tidy” data frames?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 4.2\n\n\n\n\n\nWhat makes “tidy” data frames useful for organizing data?\n\n\n\n\n\n4.2.2 Converting to “tidy” data\nIn this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore for the rest of this book, you’ll mostly only see data frames that are already in “tidy” format as well. This is not always the case however with data in the wild. If your original data frame is in wide i.e. non-“tidy” format and you would like to use the ggplot2 package for data visualization or the dplyr package for data wrangling, you will first have to convert it to “tidy” format using the pivot_longer() function in the tidyr package (Wickham and Girlich 2022).\nGoing back to our drinks_smaller data frame from earlier:\n\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nWe convert it to “tidy” format by using the pivot_longer() function from the tidyr package as follows:\n\n# tidy drinks_smaller\ndrinks_smaller_tidy <- drinks_smaller %>% \n  pivot_longer(\n    cols = -country, \n    names_to = \"type\", \n    values_to = \"servings\"\n  )\n\n# print\ndrinks_smaller_tidy\n\n# A tibble: 12 × 3\n   country      type   servings\n   <chr>        <chr>     <int>\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nWe set the arguments to pivot_longer() as follows:\n\nThe first argument, cols, are the columns you either want to or don’t want to tidy. Observe how we set this to -country indicating that we don’t want to tidy the country variable in drinks_smaller which leaves beer, spirit, and wine to be tidied.\nnames_to is the name of the column/variable in the new “tidy” frame that contains the column names of the original data frame that you want to tidy. Observe how we set names_to = \"type\" and in the resulting drinks_smaller_tidy the column type contains the three types of alcohol beer, spirit, and wine.\nvalues_to is the name of the column/variable in the “tidy” frame that contains the rows and columns of values in the original data frame you want to tidy. Observe how we set values_to = \"servings\" and in the resulting drinks_smaller_tidy the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values.\n\nThe first argument, cols, is a little nuanced, so let’s consider another example. Note the code below is very similar, but now the first argument species which columns we’d want to tidy c(beer, spirit, wine), instead of the columns we don’t want to tidy -country. Note the use of c() to create a vector of the columns in drinks_smaller that we’d like to tidy. If you run the code below, you’ll see that the result is as drinks_smaller_tidy.\n\n# tidy drinks_smaller\ndrinks_smaller %>% \n  pivot_longer(\n    cols = c(beer, spirit, wine),\n    names_to = \"type\", \n    values_to = \"servings\"\n    )\n\nWith our drinks_smaller_tidy “tidy” format data frame, we can now produce a side-by-side AKA dodged barplot using geom_col() and not geom_bar(), since we would like to map the servings variable to the y-aesthetic of the bars.\n\nggplot(drinks_smaller_tidy, aes(x=country, y=servings, fill=type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n\nConverting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice. For example, see the examples in the bottom of the help file for pivot_longer() by running ?pivot_longer. We’ll show another example of using pivot_longer() to convert a “wide” formatted data frame to “tidy” format in Section 4.3. For other examples of converting a dataset into “tidy” format, check out the different functions available for data tidying and a case study using data from the World Health Organization in R for Data Science (Grolemund and Wickham 2016).\n\n\n\n\n\n\n🎯 Learning Check 4.3\n\n\n\n\n\nTake a look the airline_safety data frame included in the fivethirtyeight data. Run the following:\n\nairline_safety\n\nAfter reading the help file by running ?airline_safety, we see that airline_safety is a data frame containing information on different airlines companies’ safety records. This data was originally reported on the data journalism website FiveThirtyEight.com in Nate Silver’s article “Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?”. Let’s ignore the incl_reg_subsidiaries and avail_seat_km_per_week variables for simplicity:\n\nairline_safety_smaller <- airline_safety %>% \n  select(-c(incl_reg_subsidiaries, avail_seat_km_per_week))\n\nairline_safety_smaller\n\n# A tibble: 56 × 7\n   airline               incidents_85_99 fatal…¹ fatal…² incid…³ fatal…⁴ fatal…⁵\n   <chr>                           <int>   <int>   <int>   <int>   <int>   <int>\n 1 Aer Lingus                          2       0       0       0       0       0\n 2 Aeroflot                           76      14     128       6       1      88\n 3 Aerolineas Argentinas               6       0       0       1       0       0\n 4 Aeromexico                          3       1      64       5       0       0\n 5 Air Canada                          2       0       0       2       0       0\n 6 Air France                         14       4      79       6       2     337\n 7 Air India                           2       1     329       4       1     158\n 8 Air New Zealand                     3       0       0       5       1       7\n 9 Alaska Airlines                     5       0       0       5       1      88\n10 Alitalia                            7       2      50       4       0       0\n# … with 46 more rows, and abbreviated variable names ¹​fatal_accidents_85_99,\n#   ²​fatalities_85_99, ³​incidents_00_14, ⁴​fatal_accidents_00_14,\n#   ⁵​fatalities_00_14\n\n\nThis data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, in particular so that it has a variable incident_type_years indicating the incident type/year and a variable count of the counts?\n\n\n\n\n\n4.2.3 nycflights13 package\nRecall the nycflights13 package with data about all domestic flights departing from New York City in 2013 that we introduced in Section 1.4 and used extensively in Chapter 2 on data visualization and Chapter 3 on data wrangling. Let’s revisit the flights data frame by running View(flights). We saw that flights has a rectangular shape with each of its 336,776 rows corresponding to a flight and each of its 19 columns corresponding to different characteristics/measurements of each flight. This matches exactly with our definition of “tidy” data from above.\n\nEach variable forms a column.\nEach observation forms a row.\n\nBut what about the third property of “tidy” data?\n\n\nEach type of observational unit forms a table.\n\n\nRecall that we also saw in Section 1.4.3 that the observational unit for the flights data frame is an individual flight. In other words, the rows of the flights data frame refer to characteristics/measurements of individual flights. Also included in the nycflights13 package are other data frames with their rows representing different observational units (Wickham 2021):\n\nairlines: translation between two letter IATA carrier codes and names (16 in total). i.e. the observational unit is an airline company.\nplanes: construction information about each of 3,322 planes used. i.e. the observational unit is an aircraft.\nweather: hourly meteorological data (about 8705 observations) for each of the three NYC airports. i.e. the observational unit is an hourly measurement.\nairports: airport names and locations. i.e. the observational unit is an airport.\n\nThe organization of the information into these five data frames follow the third “tidy” data property: observations corresponding to the same observational unit should be saved in the same table i.e. data frame. You could think of this property as the old English expression: “birds of a feather flock together.”"
  },
  {
    "objectID": "04-tidy.html#sec-case-study-tidy",
    "href": "04-tidy.html#sec-case-study-tidy",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "4.3 Case study: Democracy in Guatemala",
    "text": "4.3 Case study: Democracy in Guatemala\nIn this section, we’ll show you another example of how to convert a data frame that isn’t in “tidy” format i.e. “wide” format, to a data frame that is in “tidy” format i.e. “long/narrow” format. We’ll do this using the pivot_longer() function from the tidyr package again. Furthermore, we’ll make use of some of the ggplot2 data visualization and dplyr data wrangling tools you learned in Chapters Chapter 2 and Chapter 3.\nLet’s use the dem_score data frame we imported in Section 4.1, but focus on only data corresponding to Guatemala.\n\nguat_dem <- dem_score %>% \n  filter(country == \"Guatemala\")\n\nguat_dem\n\n# A tibble: 1 × 10\n  country   `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`\n  <chr>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Guatemala      2     -6     -5      3      1     -3     -7      3      3\n\n\nNow let’s produce a time-series plot showing how the democracy scores have changed over the 40 years from 1952 to 1992 for Guatemala. Recall that we saw time-series plot in Section 2.4 on creating linegraphs using geom_line(). Let’s lay out the Grammar of Graphics we saw in Section 2.1.\nFirst we know we need to set data = guat_dem and use a geom_line() layer, but what is the aesthetic mapping of variables. We’d like to see how the democracy score has changed over the years, so we need to map:\n\nyear to the x-position aesthetic and\ndemocracy_score to the y-position aesthetic\n\nNow we are stuck in a predicament, much like with our drinks_smaller example in sec-tidy-data-ex. We see that we have a variable named country, but its only value is \"Guatemala\". We have other variables denoted by different year values. Unfortunately, the guat_dem data frame is not “tidy” and hence is not in the appropriate format to apply the Grammar of Graphics and thus we cannot use the ggplot2 package. We need to take the values of the columns corresponding to years in guat_dem and convert them into a new “key” variable called year. Furthermore, we’d like to take the democracy scores on the inside of the table and turn them into a new “value” variable called democracy_score. Our resulting data frame will thus have three columns: country, year, and democracy_score.\nRecall that the pivot_longer() function in the tidyr package can complete this task for us:\n\nguat_dem_tidy <- guat_dem %>% \n  pivot_longer(\n    cols = -country, \n    names_to = \"year\", \n    values_to = \"democracy_score\"\n  ) \n\nguat_dem_tidy\n\n# A tibble: 9 × 3\n  country   year  democracy_score\n  <chr>     <chr>           <dbl>\n1 Guatemala 1952                2\n2 Guatemala 1957               -6\n3 Guatemala 1962               -5\n4 Guatemala 1967                3\n5 Guatemala 1972                1\n6 Guatemala 1977               -3\n7 Guatemala 1982               -7\n8 Guatemala 1987                3\n9 Guatemala 1992                3\n\n\nWe set the arguments to pivot_longer() as follows:\n\nThe first argument, cols, indicatesthe columns you either want to or don’t want to tidy. Observe how we set this to -country indicating that we don’t want to tidy the country variable in guat_dem which leaves 1952 through 1992 to be tidied.\nnames_to is the name of the column/variable in the new “tidy” frame that contains the column names of the original data frame that you want to tidy. Observe how we set names_to = \"year\" and in the resulting guat_dem_tidy the column year contains the years where the Guatemala’s democracy score were measured.\nvalues_to is the name of the column/variable in the “tidy” frame that contains the rows and columns of values in the original data frame you want to tidy. Observe how we set values_to = \"democracy_score\" and in the resulting guat_dem_tidy the column democracy_score contains the 1 \\(\\times\\) 9 = 9 democracy scores.\n\nHowever, observe in the output for guat_dem_tidy that the year variable is of type chr or character. Before we can plot this variable on the x-axis, we need to convert it into a numerical variable using the as.numeric() function within the mutate() function, which we saw in Section 3.5 on mutating existing variables to create new ones.\n\nguat_dem_tidy <- guat_dem_tidy %>% \n  mutate(year = as.numeric(year))\n\nWe can now create the plot to show how the democracy score of Guatemala changed from 1952 to 1992 using a geom_line():\n\nggplot(guat_dem_tidy, aes(x = year, y = democracy_score)) +\n  geom_line() +\n  labs(\n    x = \"Year\", \n    y = \"Democracy Score\",\n    title = \"Democracy score in Guatemala 1952-1992\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 4.4\n\n\n\n\n\nConvert the dem_score data frame into a tidy data frame and assign the name of dem_score_tidy to the resulting long-formatted data frame.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 4.5\n\n\n\n\n\nRead in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a tidy data frame."
  },
  {
    "objectID": "04-tidy.html#tidy-conclusion",
    "href": "04-tidy.html#tidy-conclusion",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\n\n4.4.1 tidyverse package\nNotice at the beginning of the chapter we loaded the following four packages, which are among the four of the most frequently used R packages for data science:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\n\nThere is a much quicker way to load these packages than by individually loading them as we did above: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you. So after installing the tidyverse package as you would a normal package, running this:\n\nlibrary(tidyverse)\n\nwould be the same as running this:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\n\nYou’ve seen the first 4 of the these packages: ggplot2 for data visualization, dplyr for data wrangling, tidyr for converting data to “tidy” format, and readr for importing spreadsheet data into R. The remaining packages (purrr, tibble, stringr, and forcats) are left for a more advanced book; check out R for Data Science to learn about these packages.\nThe tidyverse “umbrella” package gets its name from the fact that all functions in all its constituent packages are designed to that all inputs/argument data frames are in “tidy” format and all output data frames are in “tidy” format as well. This standardization of input and output data frames makes transitions between the various functions in these packages as seamless as possible.\n\n\n4.4.2 Additional resources\nIf you want to learn more about using the readr and tidyr package, we suggest you that you check out RStudio’s “Data Import” cheatsheet. You can access this cheatsheet by going to RStudio’s cheatsheet page and searching for “Data Import Cheat Sheet”.\n\n\n\nData Import cheatsheat\n\n\n\n\n4.4.3 What’s to come?\nCongratulations! We’ve completed the “Data Science via the tidyverse” portion of this book! We’ll now move to the “data modeling” portion in Chapters -Chapter 5 and -Chapter 6, where you’ll leverage your data visualization and wrangling skills to model relationships between different variables in data frames.\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. 2016. R for Data Science. http://r4ds.had.co.nz/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software Volume 59 (Issue 10). https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf.\n\n\n———. 2021. Nycflights13: Flights That Departed NYC in 2013. https://github.com/hadley/nycflights13.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  },
  {
    "objectID": "05-regression.html",
    "href": "05-regression.html",
    "title": "5  Basic Regression",
    "section": "",
    "text": "Now that we are equipped with data visualization skills from Chapter 2, an understanding of the “tidy” data format from Chapter 4, and data wrangling skills from Chapter 3, we now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between:\nAnother way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) as a function of the explanatory/predictor variable \\(x\\). Why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? That’s because roughly speaking data modeling can be used for two purposes:\nData modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks and deep learning, and supervised learning. In this chapter, we’ll focus on one particular technique: linear regression, one of the most commonly-used and easy-to-understand approaches to modeling. Recall our discussion in Subsection 1.4.3 on numerical and categorical variables. Linear regression involves:\nWith linear regression there is always only one numerical outcome variable \\(y\\) but we have choices on both the number and the type of explanatory variables to use. We’re going to cover the following regression scenarios:\nWe’ll study all four of these regression scenarios using real data, all easily accessible via R packages!"
  },
  {
    "objectID": "05-regression.html#packages-needed",
    "href": "05-regression.html#packages-needed",
    "title": "5  Basic Regression",
    "section": "Packages Needed",
    "text": "Packages Needed\nLet’s now load all the packages needed for this chapter (this assumes you’ve already installed them). In this chapter we introduce some new packages:\n\nThe tidyverse “umbrella” package. Recall from our discussion in Section 4.4.1 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once:\n\nggplot2 for data visualization\ndplyr for data wrangling\ntidyr for converting data to “tidy” format\nreadr for importing spreadsheet data into R\nAs well as the more advanced purrr, tibble, stringr, and forcats packages\n\nThe skimr (Waring et al. 2022) package, which provides a simple-to-use function to quickly compute a wide array of commonly-used summary statistics.\nThe gapminder package, which provides excerpts of data available from Gapminder.org\nThe moderndive package, which includes data sets we will analyze\n\nIf needed, read Section 1.3 for information on how to install and load R packages.\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(gapminder)\nlibrary(moderndive)"
  },
  {
    "objectID": "05-regression.html#sec-model1",
    "href": "05-regression.html#sec-model1",
    "title": "5  Basic Regression",
    "section": "5.1 One numerical explanatory variable",
    "text": "5.1 One numerical explanatory variable\nWhy do some professors and instructors at universities and colleges get high teaching evaluations from students while others don’t? What factors can explain these differences? Are there biases? These are questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which professors and instructors should get promotions. Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer this question: what factors can explain differences in instructor’s teaching evaluation scores? To this end, they collected information on \\(n = 463\\) instructors. A full description of the study can be found at openintro.org.\nWe’ll keep things simple for now and try to explain differences in instructor evaluation scores as a function of one numerical variable: their “beauty score.” The specifics on how this score was calculated will be described shortly.\nCould it be that instructors with higher beauty scores also have higher teaching evaluations? Could it be instead that instructors with higher beauty scores tend to have lower teaching evaluations? Or could it be there is no relationship between beauty score and teaching evaluations?\nWe’ll achieve ways to address these questions by modeling the relationship between these two variables with a particular kind of linear regression called simple linear regression. Simple linear regression is the most basic form of linear regression. With it we have\n\nA numerical outcome variable \\(y\\). In this case, an instructor’s teaching score.\nA single numerical explanatory variable \\(x\\). In this case, an instructor’s beauty score.\n\n\n5.1.1 Exploratory data analysis\nA crucial step before doing any kind of modeling or analysis is performing an exploratory data analysis, or EDA, of all our data. Exploratory data analysis can give you a sense of the distribution of the data and whether there are outliers and/or missing values. Most importantly, it can inform how to build your model. There are many approaches to exploratory data analysis; here are three:\n\nMost fundamentally: just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step!\nComputing summary statistics like means, medians, and standard deviations.\nCreating data visualizations.\n\nLet’s load the evals data (which is built into the moderndive package), select only a subset of the variables, and look at the raw values. Recall you can look at the raw values by running View() in the console in RStudio to pop-up the spreadsheet viewer with the data frame of interest as the argument to View(). Here, however, we present only a snapshot of five randomly chosen rows:\n\nevals_ch5 <- evals %>%\n  select(score, bty_avg, age)\n\n\nevals_ch5 %>% \n  slice_sample(n = 5)\n\n\n\n\nRandom sample of 5 instructors\n\n\nscore\nbty_avg\nage\n\n\n\n\n3.7\n3.00\n62\n\n\n4.7\n4.33\n46\n\n\n4.8\n5.50\n62\n\n\n2.8\n2.00\n62\n\n\n4.0\n2.33\n64\n\n\n\n\n\nWhile a full description of each of these variables can be found at openintro.org, let’s summarize what each of these variables represents.\n\nscore: Numerical variable of the average teaching score based on students’ evaluations between 1 and 5. This is the outcome variable \\(y\\) of interest.\nbty_avg: Numerical variable of average “beauty” rating based on a panel of 6 students’ scores between 1 and 10. This is the numerical explanatory variable \\(x\\) of interest. Here 1 corresponds to a low beauty rating and 10 to a high beauty rating.\nage: A numerical variable of age in years as an integer value.\n\nAn alternative way to look at the raw data values is by choosing a random sample of the rows in evals_ch5 by piping it into the slice_sample() function from the dplyr package. Here we set the n argument to be 5, indicating that we want a random sample of 5 rows. We display the results in Table 5.1. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.\n\nevals_ch5 %>%\n  slice_sample(n = 5)\n\n\n\n\n\nTable 5.1: A random sample of 5 out of the 463 courses at UT Austin\n\n\nscore\nbty_avg\nage\n\n\n\n\n4.4\n5.67\n57\n\n\n3.1\n7.00\n33\n\n\n4.1\n4.17\n45\n\n\n5.0\n4.33\n46\n\n\n4.8\n4.83\n52\n\n\n\n\n\n\nNow that we’ve looked at the raw values in our evals_ch5 data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s start by computing the mean and median of our numerical outcome variable score and our numerical explanatory variable “beauty” score denoted as bty_avg. We’ll do this by using the summarize() function from dplyr along with the mean() and median() summary functions we saw in Section 3.3.\n\nevals_ch5 %>%\n  summarize(\n    mean_bty_avg = mean(bty_avg),\n    mean_score = mean(score),\n    median_bty_avg = median(bty_avg), \n    median_score = median(score)\n    )\n\n# A tibble: 1 × 4\n  mean_bty_avg mean_score median_bty_avg median_score\n         <dbl>      <dbl>          <dbl>        <dbl>\n1         4.42       4.17           4.33          4.3\n\n\nHowever, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? Typing out all these summary statistic functions in summarize() would be long and tedious. Instead, let’s use the convenient skim() function from the skimr package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our evals_ch5 data frame, select() only the outcome and explanatory variables teaching score and bty_avg, and pipe them into the skim() function:\n\n\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr."
  },
  {
    "objectID": "06-multiple-regression.html",
    "href": "06-multiple-regression.html",
    "title": "6  Multiple Regression",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "07-causality.html",
    "href": "07-causality.html",
    "title": "7  Randomization and Causality",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "08-populations.html",
    "href": "08-populations.html",
    "title": "8  Populations and Generalizability",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "09-sampling-distributions.html",
    "href": "09-sampling-distributions.html",
    "title": "9  Sampling Distributions",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "10-confidence-intervals.html",
    "href": "10-confidence-intervals.html",
    "title": "10  Confidence Intervals",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "11-p-values.html",
    "href": "11-p-values.html",
    "title": "11  P-values",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "12-hypothesis-tests.html",
    "href": "12-hypothesis-tests.html",
    "title": "12  Hypothesis tests",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "13-putting-together.html",
    "href": "13-putting-together.html",
    "title": "13  Putting it all together",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garrett, and Hadley Wickham. 2016. R for Data\nScience. http://r4ds.had.co.nz/.\n\n\nIsmay, Chester. 2016. Getting Used to r, RStudio, and r\nMarkdown. http://ismayc.github.io/rbasics-book.\n\n\nRobbins, Naomi. 2013. Creating More Effective Graphs. Chart\nHouse.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software Volume 59 (Issue 10). https://www.jstatsoft.org/index.php/jss/article/view/v059i10/v59i10.pdf.\n\n\n———. 2021. Nycflights13: Flights That Departed NYC in 2013. https://github.com/hadley/nycflights13.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. Tidyr: Tidy Messy\nData. https://CRAN.R-project.org/package=tidyr.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and\nComputing). Secaucus, NJ, USA: Springer-Verlag New York, Inc."
  },
  {
    "objectID": "statistical-background.html",
    "href": "statistical-background.html",
    "title": "Appendix A — Statistical Background",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  }
]