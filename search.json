[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistics and Data Science",
    "section": "",
    "text": "This is the website for Introduction to Statistics and Data Science. This book starts you down the path of learning how to think with data using R. You’ll learn the basics of how to engage, explore, and examine many types of data arising from several contexts. Hopefully you’ll have fun and see how valuable it is to be able to critically think with data.\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that this is a “development version” of this book for the new design of STAT 202. Meaning this is a work in progress being edited and updated as we go.\nWe would appreciate any feedback on typos and errors.\n\n\nThis open textbook is produced with support from Northwestern University Libraries and The Alumnae of Northwestern University."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Statistics and Data Science",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Zero v1.0 Universal License. If you’d like to give back, please consider reporting a typo or leaving a pull request at github.com/NUstat/intro-stat-data-sci."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Help! I’m new to R and RStudio and I need to learn about them! However, I’m completely new to coding! What do I do?\nIf you’re asking yourself this question, then you’ve come to the right place! Start with our “Introduction for Students”."
  },
  {
    "objectID": "preface.html#introduction-for-students",
    "href": "preface.html#introduction-for-students",
    "title": "Preface",
    "section": "Introduction for students",
    "text": "Introduction for students\nThis book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way statisticians, data scientists, data journalists, and other researchers would.\nIn Figure 1 we present a flowchart of what you’ll cover in this book. You’ll first get started with data in Chapter 1, where you’ll learn about the difference between R and RStudio, start coding in R, understand what R packages are, and explore your first dataset: all domestic departure flights from a New York City airport in 2013. Then\n\nData Exploration: You’ll assemble your data science toolbox using tidyverse packages. In particular:\n\nCh. 2: Visualizing data via the ggplot2 package.\nCh. 3: Wrangling data via the dplyr package.\nCh. 4: Understanding the concept of “tidy” data as a standardized data input format for all packages in the tidyverse\n\nData Modeling: Using these data science tools, you’ll start performing data modeling. In particular:\n\nCh. 5: Constructing basic regression models.\nCh. 6: Constructing multiple regression models.\n\nStatistical Theory: Now you’ll learn about the role of randomization in making inferences and the general frameworks used to make inferences in statistics. In particular:\n\nCh. 7: Randomization and causality.\nCh. 8: Populations and generalizability.\nCh. 9: Sampling distributions.\n\nStatistical Inference: You’ll learn to combine your newly acquired data analysis and modeling skills with statistical theory to make inferences. In particular:\n\nCh. 10: Building confidence intervals.\nCh. 11: Calculating p-values.\nCh. 12: Conducting hypothesis tests.\n\n\n\n\n\nFigure 1: Course Flowchart\n\n\n\nWhat you will learn from this book\nWe hope that by the end of this book, you’ll have learned\n\nHow to use R to explore data.\nHow to generate research questions and hypotheses.\nHow to think like a statistician and the role of chance in your data.\nHow to answer statistical questions using tools like confidence intervals and hypothesis tests.\nHow to effectively create “data stories” using these tools.\n\nWhat do we mean by data stories? We mean any analysis involving data that engages the reader in answering questions with careful visuals and thoughtful discussion, such as How strong is the relationship between per capita income and crime in Chicago neighborhoods? and How many f**ks does Quentin Tarantino give (as measured by the amount of swearing in his films)?. Further discussions on data stories can be found in this Think With Google article.\nFor other examples of data stories constructed by students like yourselves, look at the final projects for two courses that have previously used a version of this book:\n\nMiddlebury College MATH 116 Introduction to Statistical and Data Sciences using student collected data.\nPacific University SOC 301 Social Statistics using data from the fivethirtyeight R package.\n\nThis book will help you develop your “data science toolbox”, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. With these tools, you’ll be able to perform the entirety of the “data/science pipeline” while building data communication skills.\nIn particular, this book will lean heavily on data visualization. In today’s world, we are bombarded with graphics that attempt to convey ideas. We will explore what makes a good graphic and what the standard ways are to convey relationships with data. You’ll also see the use of visualization to introduce concepts like mean, median, standard deviation, distributions, etc. In general, we’ll use visualization as a way of building almost all of the ideas in this book.\nTo impart the statistical lessons in this book, we have intentionally minimized the number of mathematical formulas used and instead have focused on developing a conceptual understanding via data visualization, statistical computing, and simulations. We hope this is a more intuitive experience than the way statistics has traditionally been taught in the past and how it is commonly perceived.\nFinally, you’ll learn the importance of literate programming. By this we mean you’ll learn how to write code that is useful not just for a computer to execute but also for readers to understand exactly what your analysis is doing and how you did it. This is part of a greater effort to encourage reproducible research (see subsection Reproducible research for more details). Hal Abelson coined the phrase that we will follow throughout this book:\n\n“Programs must be written for people to read, and only incidentally for machines to execute.”\n\nWe understand that there may be challenging moments as you learn to program. We still continue to struggle and find ourselves often using web searches to find answers and reach out to colleagues for help. In the long run though, we all can solve problems faster and more elegantly via programming. We wrote this book as our way to help you get started and you should know that there is a huge community of R users that are always happy to help everyone along as well. This community exists in particular on the internet on various forums and websites such as stackoverflow.com.\n\n\nData/science pipeline\nYou may think of statistics as just being a bunch of numbers. We commonly hear the phrase “statistician” when listening to broadcasts of sporting events. Statistics (in particular, data analysis), in addition to describing numbers like with baseball batting averages, plays a vital role in all of the sciences. You’ll commonly hear the phrase “statistically significant” thrown around in the media. You’ll see articles that say “Science now shows that chocolate is good for you.” Underpinning these claims is data analysis and a theoretical model relating the data collected in a sample to a larger population. By the end of this book, you’ll be able to better understand whether these claims should be trusted or whether we should be wary. Inside data analysis are many sub-fields that we will discuss throughout this book (though not necessarily in this order):\n\ndata collection\ndata wrangling\ndata visualization\ndata modeling\nstatistical inference\ncorrelation and regression\ninterpretation of results\ndata communication/storytelling\n\nThese sub-fields are summarized in what Grolemund and Wickham term the “Data/Science Pipeline” in Figure 2.\n\n\n\nFigure 2: Data/Science Pipeline\n\n\nWe will begin by digging into the gray Understand portion of the cycle with data visualization, then with a discussion on what is meant by tidy data and data wrangling, and then conclude by talking about interpreting and discussing the results of our models via Communication. These steps are vital to any statistical analysis. But why should you care about statistics? “Why did they make me take this class?”\nThere’s a reason so many fields require a statistics course. Scientific knowledge grows through an understanding of statistical significance and data analysis. You needn’t be intimidated by statistics. It’s not the beast that it used to be and, paired with computation, you’ll see how reproducible research in the sciences particularly increases scientific knowledge.\n\n\nReproducible research\n\n“The most important tool is the mindset, when starting, that the end product will be reproducible.” – Keith Baggerly\n\nAnother goal of this book is to help readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their analyses reproducible from the very beginning. This means we’ll be trying to help you build new habits. This will take practice and be difficult at times. You’ll see just why it is so important for you to keep track of your code and well-document it to help yourself later and any potential collaborators as well.\nCopying and pasting results from one program into a word processor is not the way that efficient and effective scientific research is conducted. It’s much more important for time to be spent on data collection and data analysis and not on copying and pasting plots back and forth across a variety of programs.\nIn a traditional analysis if an error was made with the original data, we’d need to step through the entire process again: recreate the plots and copy and paste all of the new plots and our statistical analysis into your document. This is error prone and a frustrating use of time. We’ll see how to use R Markdown to get away from this tedious activity so that we can spend more time doing science.\n\n“We are talking about computational reproducibility.” - Yihui Xie\n\nReproducibility means a lot of things in terms of different scientific fields. Are experiments conducted in a way that another researcher could follow the steps and get similar results? In this book, we will focus on what is known as computational reproducibility. This refers to being able to pass all of one’s data analysis, data-sets, and conclusions to someone else and have them get exactly the same results on their machine. This allows for time to be spent interpreting results and considering assumptions instead of the more error prone way of starting from scratch or following a list of steps that may be different from machine to machine."
  },
  {
    "objectID": "01-getting-started.html",
    "href": "01-getting-started.html",
    "title": "1  Getting Started with Data in R",
    "section": "",
    "text": "Before we can start exploring data in R, there are some key concepts to understand first:\nWe’ll introduce these concepts in upcoming Sections 1.1 - 1.3 If you are already somewhat familiar with these concepts, feel free to skip to Section 1.4 where we’ll introduce our first data set: all domestic flights departing a New York City airport in 2013. This is a dataset we will explore in depth in this book."
  },
  {
    "objectID": "01-getting-started.html#sec-r-rstudio",
    "href": "01-getting-started.html#sec-r-rstudio",
    "title": "1  Getting Started with Data in R",
    "section": "1.1 What are R and RStudio?",
    "text": "1.1 What are R and RStudio?\nFor much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest:\n\nR is like a car’s engine.\nRStudio is like a car’s dashboard.\n\n\n\n\n\n\n\n\nR: Engine\nRStudio: Dashboard\n\n\n\n\n\n\n\n\n\nMore precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well.\n\n1.1.1 Using RStudio Cloud\nRStudio Cloud (https://rstudio.cloud) is a hosted version of RStudio that allows you to begin coding directly from your browser - there is no software to install and nothing to configure on your computer.\nTo begin using RStudio Cloud use the link provided by your instructor to gain access to the classroom workspace. You will be prompted to create a free account or log in if you have an existing account.\nAfter you open RStudio Cloud, you should now have access to the classroom under ‘Spaces’ on the left hand side (in this case ‘Stat 202’).\n\nThroughout class you will be working on various activities. Once the instructor has made an activity available you will click on the classroom Workspace (Stat 202) to access the available projects. To begin working on an activity click ‘Start’. Once that activity project is open navigate to the ‘File’ pane and open the R Markdown ‘.Rmd’ file.\n\nYou can use RStudio Cloud for personal use as well by creating projects in ‘Your Workspace’. However, RStudio Cloud limits the number of projects and amount of accessible time so it is recommended that you later install the software on your own computer.\n\n\n1.1.2 Installing R and RStudio on your personal computer\n\nNote about RStudio Server or RStudio Cloud: If your instructor has provided you with a link and access to RStudio Server or RStudio Cloud, then you can skip this section. We do recommend after a few months of working on RStudio Server/Cloud that you return to these instructions to install this software on your own computer though. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio second.\n\n\nYou must do this first: Download and install R.\n\nIf you are a Windows user: Click on “Download R for Windows”, then click on “base”, then click on the Download link.\nIf you are macOS user: Click on “Download R for (Mac) OS X”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of August 10, 2019 was R-3.6.1.\n\nYou must do this second: Download and install RStudio.\n\nScroll down to “Installers for Supported Platforms” near the bottom of the page.\nClick on the download link corresponding to your computer’s operating system.\n\n\n\n\n1.1.3 Using R via RStudio\nRecall our car analogy from above. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs AKA applications you can open. We will always work in RStudio and not R. In other words:\n\n\n\n\n\n\n\nR: Do not open this\nRStudio: Open this\n\n\n\n\n\n\n\n\n\nAfter you open RStudio, you should see the following:\n\nNote the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, you’ll come to learn what purpose each of these panes serve."
  },
  {
    "objectID": "01-getting-started.html#sec-code",
    "href": "01-getting-started.html#sec-code",
    "title": "1  Getting Started with Data in R",
    "section": "1.2 How do I code in R?",
    "text": "1.2 How do I code in R?\nNow that you’re set up with R and RStudio, you are probably asking yourself “OK. Now how do I use R?” The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book.\nWhile it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively.\n\n1.2.1 Creating your first R Markdown document\nR Markdown allows you to easily create a document which combines your code, the results from your code, as well as any text that accompanies the analysis. To create a new R Markdown file, in R Studio select File>New File>R Markdown. Then, you will see a window pop-up titled New R Markdown. Here, you specify the type of file you wish to create. HTML is generally the recommended document type since it does not have traditional page separators like PDF and Word do. You can also choose a title and author for your document using their respective fields. Finally, select Ok to create your new R Markdown file. You will see it appear as a tab in your R Studio session. Click the save icon to save your new document.\nThe following is an example of an R Markdown document:\n\n\nSave your document.\nClick knit to compile your R Markdown into the document file type that you specified. The file will be saved in your Files pane. This will also save your document.\nInsert a new code chunk in your document where the cursor is located. You will often have many code chunks in your document.\nRun the current code chunk.\n\nWhen you create your Markdown file and knit it into a document, the chunks are run in order and any output from them is shown in the document, in the order and location that their respective chunk appears. Sometimes you may wish to type code or analyze data without it printing in the document. If that is the case, you type the code in the Console rather than in the .Rmd file.\nWhile you read through this book, it will be helpful to have an RMarkdown document open so you can copy code provided and paste it into a code chunk to run.\n\n\n1.2.2 Basic programming concepts and terminology\nWe now introduce some basic programming concepts and terminology. Instead of asking you to learn all these concepts and terminology right now, we’ll guide you so that you’ll “learn by doing.” Note that in this book we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, “learning by doing” and lots of repetition.\n\nBasics:\n\nConsole: Where you enter in commands. \nRunning code: The act of telling R to perform an action by giving it commands in the console.\nObjects: Where values are saved in R. In order to do useful and interesting things in R, we will want to assign a name to an object. For example we could do the following assignments: x <- 44 - 20 and three <- 3. This would allow us to run x + three which would return 27.\nData types: Integers, doubles/numerics, logicals, and characters.\n\n\nIn R Studio try typing the following code into the console or code chunk.\n\nx <- 44-20\nthree <- 3\nx+three\n\n[1] 27\n\n\nYou should see x and three appear as stored objects in the Environment pane. Anything you store in the Environment pane can be referenced and used later. R can also be used as a calculator, notice how it evaluates x+three.\n\nVectors: A series of values. These are created using the c() function, where c() stands for “combine” or “concatenate”. For example: c(6, 11, 13, 31, 90, 92).\nFactors: Categorical data are represented in R as factors.\nData frames: Data frames are like rectangular spreadsheets: they are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section Section 1.4.\nConditionals:\n\nTesting for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error.\nBoolean algebra: TRUE/FALSE statements and mathematical operators such as < (less than), <= (less than or equal), and != (not equal to).\nLogical operators: & representing “and” as well as | representing “or.” Ex: (2 + 1 == 3) & (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE.\n\nFunctions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values.\n\nThis list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. We feel that you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.\n\n\n1.2.3 Errors, warnings, and messages\nOne thing that intimidates new R and RStudio users is how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad.\nR will show red text in the console pane in three different situations:\n\nErrors: When the red text is a legitimate error, it will be prefaced with “Error in…” and try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 1.3.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first.\nWarnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter 2 if you create a scatterplot based on a dataset where one of the values is missing, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining values, but it is warning you that one of the points isn’t there.\nMessages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 1.3.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Chapter 4. These are helpful diagnostic messages and they don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages().\n\nRemember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather:\n\nIf the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong!\nIf the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention.\nOtherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine.\n\n\n\n1.2.4 Tips on learning to code\nLearning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn.\nHere are a few useful tips to keep in mind as you learn to program:\n\nRemember that computers are not actually that smart: You may think your computer or smartphone are “smart,” but really people spent a lot of time and energy designing them to appear “smart.” Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way.\nTake the “copy, paste, and tweak” approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Don’t be afraid to play around!\nThe best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in.\nPractice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Don’t worry however; we’ll give you plenty of opportunities to do so!"
  },
  {
    "objectID": "01-getting-started.html#sec-packages",
    "href": "01-getting-started.html#sec-packages",
    "title": "1  Getting Started with Data in R",
    "section": "1.3 What are R packages?",
    "text": "1.3 What are R packages?\nAnother point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization in Chapter 2, the dplyr package for data wrangling in Chapter 3, and the moderndive package that accompanies this book.\nA good analogy for R packages is they are like apps you can download onto a mobile phone:\n\n\n\n\n\n\n\nR: A new phone\nR Packages: Apps you can download\n\n\n\n\n\n\n\n\n\nSo R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play.\nLet’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to:\n\nInstall the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set. You might do this again in the future any time there is an update to the app.\nOpen the app: After you’ve installed Instagram, you need to open the app.\n\nOnce Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to:\n\nInstall the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version.\n“Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio.\n\nLet’s now show you how to perform these two steps for the ggplot2 package for data visualization.\n\n1.3.1 Package installation\n\nNote about RStudio Server: If your instructor has provided you with a link and access to RStudio Server, you probably will not need to install packages, as they have likely been pre-installed for you by your instructor. That being said, it is still a good idea to know this process for later on when you are not using RStudio Server, but rather RStudio Desktop on your own computer.\n\nThere are two ways to install an R package. For example, to install the ggplot2 package:\n\nEasy way: In the Files pane of RStudio:\n\nClick on the “Packages” tab\nClick on “Install”\nType the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2\nClick “Install”\n\n\nSlightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the Console pane of RStudio and hitting enter. Note you must include the quotation marks.\n\nMuch like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps.\n\n\n\n\n\n\n🎯 Learning Check 1.1\n\n\n\n\n\nRepeat the above installing steps for the dplyr, nycflights13, and knitr packages. This will install the earlier mentioned dplyr package, the nycflights13 package containing data on all domestic flights leaving a NYC airport in 2013, and the knitr package for writing reports in R.\n\n\n\n\n\n1.3.2 Package loading\nRecall that after you’ve installed a package, you need to “load” it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by “run the following code”? Either type or copy & paste the following code into the Console pane and then hit the enter key.\n\nlibrary(ggplot2)\n\nIf after running the above code, a blinking cursor returns next to the > “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red “error message” that reads…\nError in library(ggplot2) : there is no package called ‘ggplot2’\n… it means that you didn’t successfully install it. In that case, go back to the previous subsection “Package installation” and install it.\n\n\n\n\n\n\n🎯 Learning Check 1.2\n\n\n\n\n\n“Load” the dplyr, nycflights13, and knitr packages as well by repeating the above steps.\n\n\n\n\n\n1.3.3 Package use\nOne extremely common mistake new R users make when wanting to use particular packages is that they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to:\nError: could not find function\nR is telling you that you are trying to use a function in a package that has not yet been “loaded.” Almost all new users forget do this when starting out, and it is a little annoying to get used to. However, you’ll remember with practice."
  },
  {
    "objectID": "01-getting-started.html#sec-nycflights13",
    "href": "01-getting-started.html#sec-nycflights13",
    "title": "1  Getting Started with Data in R",
    "section": "1.4 Explore your first dataset",
    "text": "1.4 Explore your first dataset\nLet’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format; this is probably the most common way data are collected and saved in many fields. Remember from Subsection 1.2.2 that these “spreadsheet”-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this book.\nLet’s first load all the packages needed for this chapter, assuming you’ve already installed them. Read Section 1.3 for information on how to install and load R packages if you haven’t already.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(knitr)\n\nAt the beginning of all subsequent chapters in this text, we’ll always have a list of packages that you should have installed and loaded to work with that chapter’s R code.\n\n1.4.1 nycflights13 package\nMany of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect in many people’s lives. If you live in or are visiting a relatively large city and you walk around that city’s airport, you see gates showing flight information from many different airlines. And you will frequently see that some flights are delayed because of a variety of conditions. Are there ways that we can avoid having to deal with these flight delays?\nWe’d all like to arrive at our destinations on time whenever possible. (Unless you secretly love hanging out at airports. If you are one of these people, pretend for the moment that you are very much anticipating being at your final destination.) Throughout this book, we’re going to analyze data related to flights contained in the nycflights13 package (Wickham 2021). Specifically, this package contains five data sets saved in five separate data frames with information about all domestic flights departing from New York City in 2013. These include Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia (LGA) airports:\n\nflights: Information on all 336,776 flights\nairlines: A table matching airline names and their two letter IATA airline codes (also known as carrier codes) for 16 airline companies\nplanes: Information about each of 3,322 physical aircraft used.\nweather: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughtly corresponding to the 365 \\(\\times\\) 24 \\(\\times\\) 3 = 26,280 possible hourly measurements one can observe at three locations over the course of a year.\nairports: Airport names, codes, and locations for 1,458 destination airports.\n\n\n\n1.4.2 flights data frame\nWe will begin by exploring the flights data frame that is included in the nycflights13 package and getting an idea of its structure. Run the following code in your console (either by typing it or cutting & pasting it): it loads in the flights dataset into your Console. Note depending on the size of your monitor, the output may vary slightly.\n\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nLet’s unpack this output:\n\nA tibble: 336,776 x 19: A tibble is a kind of data frame used in R. This particular data frame has\n\n336,776 rows\n19 columns corresponding to 19 variables describing each observation\n\nyear month day dep_time sched_dep_time dep_delay arr_time are different columns, in other words variables, of this data frame.\nWe then have the first 10 rows of observations corresponding to 10 flights.\n... with 336,766 more rows, and 11 more variables: indicating to us that 336,766 more rows of data and 11 more variables could not fit in this screen.\n\nUnfortunately, this output does not allow us to explore the data very well. Let’s look at different tools to explore data frames.\n\n\n1.4.3 Exploring data frames\nAmong the many ways of getting a feel for the data contained in a data frame such as flights, we present three functions that take as their “argument”, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame:\n\nUsing the View() function built for use in RStudio. We will use this the most.\nUsing the glimpse() function, which is included in the dplyr package.\nUsing the kable() function, which is included in the knitr package.\nUsing the $ operator to view a single variable in a data frame.\n\n1. View():\nRun View(flights) in your Console in RStudio, either by typing it or cutting & pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital “V” in View. R is case-sensitive so you’ll receive an error is you run view(flights) instead of View(flights).\n\n\n\n\n\n\n🎯 Learning Check 1.3\n\n\n\n\n\nWhat does any ONE row in this flights dataset refer to?\n\nData on an airline\nData on a flight\nData on an airport\nData on multiple flights\n\n\n\n\nBy running View(flights), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like distance, day, and arr_delay are what we will call quantitative variables. These variables are numerical in nature. Other variables here are categorical.\nNote that if you look in the leftmost column of the View(flights) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. In other words, this will allow you to identify what object is being referred to in a given row. This is often called the observational unit. The observational unit in this example is an individual flight departing New York City in 2013. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables.\n2. glimpse():\nThe second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame:\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nWe see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type (see Subsection 1.2.2) of the variable is given immediately after each variable’s name inside < >. Here, int and dbl refer to “integer” and “double”, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to “character”, which is computer terminology for text data. Text data, such as the carrier or origin of a flight, are categorical variables. The time_hour variable is an example of one more type of data type: dttm. As you may suspect, this variable corresponds to a specific date and time of day. However, we won’t work with dates in this class and leave it to a more advanced book on data science.\n\n\n\n\n\n\n🎯 Learning Check 1.4\n\n\n\n\n\nWhat are some examples in this dataset of categorical variables? What makes them different than quantitative variables?\n\n\n\n3. kable():\nThe another way to explore the entirety of a data frame is using the kable() function from the knitr package. Let’s explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in your Console:\n\nairlines\nkable(airlines)\n\nAt first glance, it may not appear that there is much difference in the outputs. However when using tools for document production such as R Markdown, the latter code produces output that is much more legible and reader-friendly.\n4. $ operator\nLastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console\n\nairlines\nairlines$name\n\nWe used the $ operator to extract only the name variable and return it as a vector of length 16. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions.\n\n\n1.4.4 Help files\nAnother nice feature of R is the help system. You can get help in R by entering a ? before the name of a function or data frame in question and you will be presented with a page showing the documentation. For example, let’s look at the help file for the flights data frame:\n\n?flights\n\nA help file should pop-up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away."
  },
  {
    "objectID": "01-getting-started.html#sec-gs-conclusion",
    "href": "01-getting-started.html#sec-gs-conclusion",
    "title": "1  Getting Started with Data in R",
    "section": "1.5 Conclusion",
    "text": "1.5 Conclusion\nWe’ve given you what we feel are the most essential concepts to know before you can start exploring data in R. Is this chapter exhaustive? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful!\n\n1.5.1 Additional resources\nIf you are completely new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out Chester Ismay’s short book Getting used to R, RStudio, and R Markdown (Ismay 2016), which includes screencast recordings that you can follow along and pause as you learn. Furthermore, there is an introduction to R Markdown, a tool used for reproducible research in R.\n\n\n\n\n\nIsmay, Chester. 2016. Getting Used to r, RStudio, and r Markdown. http://ismayc.github.io/rbasics-book.\n\n\nWickham, Hadley. 2021. Nycflights13: Flights That Departed NYC in 2013. https://github.com/hadley/nycflights13."
  },
  {
    "objectID": "02-visualization.html",
    "href": "02-visualization.html",
    "title": "2  Data Visualization",
    "section": "",
    "text": "We begin the development of your data science toolbox with data visualization. By visualizing our data, we gain valuable insights that we couldn’t initially see from just looking at the raw data in spreadsheet form. We will use the ggplot2 package as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as The Grammar of Graphics (Wilkinson 2005).\nAt the most basic level, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way for us to get a sense for how quantitative variables compare in terms of their center (where the values tend to be located) and their spread (how they vary around the center). Graphics should be designed to emphasize the findings and insight you want your audience to understand. This does however require a balancing act. On the one hand, you want to highlight as many meaningful relationships and interesting findings as possible; on the other you don’t want to include so many as to overwhelm your audience.\nAs we will see, plots/graphics also help us to identify patterns and outliers in our data. We will see that a common extension of these ideas is to compare the distribution of one quantitative variable (i.e., what the spread of a variable looks like or how the variable is distributed in terms of its values) as we go across the levels of a different categorical variable."
  },
  {
    "objectID": "02-visualization.html#packages-needed",
    "href": "02-visualization.html#packages-needed",
    "title": "2  Data Visualization",
    "section": "Packages Needed",
    "text": "Packages Needed\nLet’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section 1.3 for information on how to install and load R packages.\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "02-visualization.html#sec-grammarofgraphics",
    "href": "02-visualization.html#sec-grammarofgraphics",
    "title": "2  Data Visualization",
    "section": "2.1 The Grammar of Graphics",
    "text": "2.1 The Grammar of Graphics\nWe begin with a discussion of a theoretical framework for data visualization known as “The Grammar of Graphics,” which serves as the foundation for the ggplot2 package. Think of how we construct sentences in English to form sentences by combining different elements, like nouns, verbs, particles, subjects, objects, etc. However, we can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “The Grammar of Graphics” define a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2005) and has been implemented in a variety of data visualization software including R.\n\n2.1.1 Components of the Grammar\nIn short, the grammar tells us that:\n\nA statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects.\n\nSpecifically, we can break a graphic into three essential components:\n\ndata: the data set composed of variables that we map.\ngeom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.\naes: aesthetic attributes of the geometric object. For example, x-position, y-position, color, shape, and size. Each assigned aesthetic attribute can be mapped to a variable in our data set.\n\nYou might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example.\n\n\n2.1.2 Gapminder data\n\n\n\nIn February 2006, a statistician named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for the 142 countries included from 2007, let’s consider only the first 6 countries when listed alphabetically in Table 2.1.\n\n\n\n\nTable 2.1: Gapminder 2007 Data: First 6 of 142 countries\n\n\nCountry\nContinent\nLife Expectancy\nPopulation\nGDP per Capita\n\n\n\n\nAfghanistan\nAsia\n43.8\n31889923\n975\n\n\nAlbania\nEurope\n76.4\n3600523\n5937\n\n\nAlgeria\nAfrica\n72.3\n33333216\n6223\n\n\nAngola\nAfrica\n42.7\n12420476\n4797\n\n\nArgentina\nAmericas\n75.3\n40301927\n12779\n\n\nAustralia\nOceania\n81.2\n20434176\n34435\n\n\n\n\n\n\nEach row in this table corresponds to a country in 2007. For each row, we have 5 columns:\n\nCountry: Name of country.\nContinent: Which of the five continents the country is part of. (Note that “Americas” includes countries in both North and South America and that Antarctica is excluded.)\nLife Expectancy: Life expectancy in years.\nPopulation: Number of people living in the country.\nGDP per Capita: Gross domestic product (in US dollars).\n\nNow consider Figure 2.1, which plots this data for all 142 countries in the data.\n\n\n\n\n\nFigure 2.1: Life Expectancy over GDP per Capita in 2007\n\n\n\n\nLet’s view this plot through the grammar of graphics:\n\nThe data variable GDP per Capita gets mapped to the x-position aesthetic of the points.\nThe data variable Life Expectancy gets mapped to the y-position aesthetic of the points.\nThe data variable Population gets mapped to the size aesthetic of the points.\nThe data variable Continent gets mapped to the color aesthetic of the points.\n\nWe’ll see shortly that data corresponds to the particular data frame where our data is saved and a “data variable” corresponds to a particular column in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. Other plots involve lines while others involve bars.\nLet’s summarize the three essential components of the Grammar in Table 2.2.\n\n\n\n\nTable 2.2: Summary of Grammar of Graphics for this plot\n\n\ndata variable\naes\ngeom\n\n\n\n\nGDP per Capita\nx\npoint\n\n\nLife Expectancy\ny\npoint\n\n\nPopulation\nsize\npoint\n\n\nContinent\ncolor\npoint\n\n\n\n\n\n\n\n\n2.1.3 Other components\nThere are other components of the Grammar of Graphics we can control as well. As you start to delve deeper into the Grammar of Graphics, you’ll start to encounter these topics more frequently. In this book however, we’ll keep things simple and only work with the two additional components listed below:\n\nfaceting breaks up a plot into small multiples corresponding to the levels of another variable (Section 2.6)\nposition adjustments for barplots (Section 2.8)\n\nOther more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (Grolemund and Wickham 2016). Generally speaking, the Grammar of Graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them.\n\n\n2.1.4 ggplot2 package\nIn this book, we will be using the ggplot2 package for data visualization, which is an implementation of the Grammar of Graphics for R (Wickham et al. 2022). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the Grammar of Graphics are specified in the ggplot() function included in the ggplot2 package, which expects at a minimum as arguments (i.e. inputs):\n\nThe data frame where the variables exist: the data argument.\nThe mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved.\n\nAfter we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include layers specifying the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 2.6.\nLet’s now put the theory of the Grammar of Graphics into practice."
  },
  {
    "objectID": "02-visualization.html#sec-five-ng",
    "href": "02-visualization.html#sec-five-ng",
    "title": "2  Data Visualization",
    "section": "2.2 Five Named Graphs - The 5NG",
    "text": "2.2 Five Named Graphs - The 5NG\nIn order to keep things simple, we will only focus on five types of graphics in this book, each with a commonly given name. We term these “five named graphs” the 5NG:\n\nscatterplots\nlinegraphs\nboxplots\nhistograms\nbarplots\n\nWe will discuss some variations of these plots, but with this basic repertoire of graphics in your toolbox you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables and while others are only appropriate for quantitative variables. You’ll want to quiz yourself often as we go along on which plot makes sense a given a particular problem or data set."
  },
  {
    "objectID": "02-visualization.html#sec-scatterplots",
    "href": "02-visualization.html#sec-scatterplots",
    "title": "2  Data Visualization",
    "section": "2.3 5NG#1: Scatterplots",
    "text": "2.3 5NG#1: Scatterplots\nThe simplest of the 5NG are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the Grammar of Graphics. Specifically, we will visualize the relationship between the following two numerical variables in the flights data frame included in the nycflights13 package:\n\ndep_delay: departure delay on the horizontal “x” axis and\narr_delay: arrival delay on the vertical “y” axis\n\nfor Alaska Airlines flights leaving NYC in 2013. This requires paring down the data from all 336,776 flights that left NYC in 2013, to only the 714 Alaska Airlines flights that left NYC in 2013.\nWhat this means computationally is: we’ll take the flights data frame, extract only the 714 rows corresponding to Alaska Airlines flights, and save this in a new data frame called alaska_flights. Run the code below to do this:\n\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\n\nFor now we suggest you ignore how this code works; we’ll explain this in detail in Chapter 3 when we cover data wrangling. However, convince yourself that this code does what it is supposed to by running View(alaska_flights): it creates a new data frame alaska_flights consisting of only the 714 Alaska Airlines flights.\nWe’ll see later in Chapter 3 on data wrangling that this code uses the dplyr package for data wrangling to achieve our goal: it takes the flights data frame and filters it to only return the rows where carrier is equal to \"AS\", Alaska Airlines’ carrier code. Other examples of carrier codes include “AA” for American Airlines and “UA” for United Airlines. Recall from Section 1.2 that testing for equality is specified with == and not =. Fasten your seat belts and sit tight for now however, we’ll introduce these ideas more fully in Chapter 3.\n\n\n\n\n\n\n🎯 Learning Check 2.1\n\n\n\n\n\nTake a look at both the flights and alaska_flights data frames by running View(flights) and View(alaska_flights). In what respect do these data frames differ?\n\n\n\n\n2.3.1 Scatterplots via geom_point\nLet’s now go over the code that will create the desired scatterplot, keeping in mind our discussion on the Grammar of Graphics in Section 2.1. We’ll be using the ggplot() function included in the ggplot2 package.\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()\n\nLet’s break this down piece-by-piece:\n\nWithin the ggplot() function, we specify two of the components of the Grammar of Graphics as arguments (i.e. inputs):\n\nThe data frame to be alaska_flights by setting data = alaska_flights.\nThe aesthetic mapping by setting aes(x = dep_delay, y = arr_delay). Specifically:\n\nthe variable dep_delay maps to the x position aesthetic\nthe variable arr_delay maps to the y position aesthetic\n\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case the geometric object are points, set by specifying geom_point().\n\nAfter running the above code, you’ll notice two outputs: a warning message and the graphic shown in Figure 2.2. Let’s first unpack the warning message:\n\n\nWarning: Removed 5 rows containing missing values (geom_point).\n\n\n\n\n\nFigure 2.2: Arrival Delays vs Departure Delays for Alaska Airlines flights from NYC in 2013\n\n\n\n\nAfter running the above code, R returns a warning message alerting us to the fact that 5 rows were ignored due to them being missing. For 5 rows either the value for dep_delay or arr_delay or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Turning our attention to the resulting scatterplot in Figure 2.2, we see that a positive relationship exists between dep_delay and arr_delay: as departure delays increase, arrival delays tend to also increase. We also note the large mass of points clustered near (0, 0).\nBefore we continue, let’s consider a few more notes on the layers in the above code that generated the scatterplot:\n\nNote that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning.\nWhen adding layers to a plot, you are encouraged to start a new line after the + so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code.\nTo stress the importance of adding layers in particular the layer specifying the geometric object, consider Figure 2.3 where no layers are added. A not very useful plot!\n\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay))\n\n\n\n\nFigure 2.3: Plot with no layers\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.2\n\n\n\n\n\nWhat are some practical reasons why dep_delay and arr_delay have a positive relationship?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.3\n\n\n\n\n\nWhat variables (not necessarily in the flights data frame) would you expect to have a negative correlation (i.e. a negative relationship) with dep_delay? Why? Remember that we are focusing on numerical variables here.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.4\n\n\n\n\n\nWhy do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaskan flights?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.5\n\n\n\n\n\nWhat are some other features of the plot that stand out to you?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.6\n\n\n\n\n\nCreate a new scatterplot using different variables in the alaska_flights data frame by modifying the example above.\n\n\n\n\n\n2.3.2 Over-plotting\nThe large mass of points near (0, 0) in Figure 2.2 can cause some confusion as it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to values being plotted on top of each other over and over again. It is often difficult to know just how many values are plotted in this way when looking at a basic scatterplot as we have here. There are two methods to address the issue of overplotting:\n\nBy adjusting the transparency of the points.\nBy adding a little random “jitter”, or random “nudges”, to each of the points.\n\nMethod 1: Changing the transparency\nThe first way of addressing overplotting is by changing the transparency of the points by using the alpha argument in geom_point(). By default, this value is set to 1. We can change this to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. Note how the following code is identical to the code in Section 2.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point():\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point(alpha = 0.2)\n\n\n\n\nFigure 2.4: Delay scatterplot with alpha = 0.2\n\n\n\n\nThe key feature to note in Figure 2.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line above to read geom_point(aes(alpha = 0.2)).\nMethod 2: Jittering the points\nThe second way of addressing overplotting is by jittering all the points, in other words give each point a small nudge in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame jitter_example with 4 rows of identical value 0 for both x and y:\n\n\n# A tibble: 4 × 2\n      x     y\n  <dbl> <dbl>\n1     0     0\n2     0     0\n3     0     0\n4     0     0\n\n\nWe display the resulting scatterplot in Figure 2.5; observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others.\n\n\n\n\n\nFigure 2.5: Regular scatterplot of jitter example data\n\n\n\n\nIn Figure 2.6 we instead display a jittered scatterplot where each point is given a random “nudge.” It is now plainly evident that this plot involves four points. Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in jitter_example remain unchanged.\n\n\n\n\n\nFigure 2.6: Jittered scatterplot of jitter example data\n\n\n\n\nTo create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). To specify how much jitter to add, we adjust the width and height arguments. This corresponds to how hard you’d like to shake the plot in units corresponding to those for both the horizontal and vertical variables (in this case minutes).\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_jitter(width = 30, height = 30)\n\n\n\n\nFigure 2.7: Jittered delay scatterplot\n\n\n\n\nObserve how the above code is identical to the code that created the scatterplot with overplotting in Subsection 2.3.1, but with geom_point() replaced with geom_jitter().\nThe resulting plot in Figure 2.7 helps us a little bit in getting a sense for the overplotting, but with a relatively large data set like this one (714 flights), it can be argued that changing the transparency of the points by setting alpha proved more effective. In terms of how much jitter one should add using the width and height arguments, it is important to add just enough jitter to break any overlap in points, but not so much that we completely alter the overall pattern in points.\n\n\n\n\n\n\n🎯 Learning Check 2.7\n\n\n\n\n\nWhy is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.8\n\n\n\n\n\nAfter viewing the Figure 2.4 above, give an approximate range of arrival delays and departure delays that occur the most frequently. How has that region changed compared to when you observed the same plot without the alpha = 0.2 set in Figure 2.2?\n\n\n\n\n\n2.3.3 Summary\nScatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful!\nWith medium to large data sets, you may need to play around with the different modifications one can make to a scatterplot. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships come about as you make subtle changes to your plots."
  },
  {
    "objectID": "02-visualization.html#sec-linegraphs",
    "href": "02-visualization.html#sec-linegraphs",
    "title": "2  Data Visualization",
    "section": "2.4 5NG#2: Linegraphs",
    "text": "2.4 5NG#2: Linegraphs\nThe next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature; in other words there is an inherent ordering to the variable. The most common example of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots. Linegraphs should be avoided when there is not a clear sequential ordering to the variable on the x-axis. Let’s illustrate linegraphs using another data set in the nycflights13 package: the weather data frame.\nLet’s get a sense for the weather data frame:\n\nExplore the weather data by running View(weather).\nRun ?weather to bring up the help file.\n\nWe can see that there is a variable called temp of hourly temperature recordings in Fahrenheit at weather stations near all three airports in New York City: Newark (origin code EWR), JFK, and La Guardia (LGA). Instead of considering hourly temperatures for all days in 2013 for all three airports however, for simplicity let’s only consider hourly temperatures at only Newark airport for the first 15 days in January.\nRecall in Section 2.3 we used the filter() function to only choose the subset of rows of flights corresponding to Alaska Airlines flights. We similarly use filter() here, but by using the & operator we only choose the subset of rows of weather where\n\nThe origin is \"EWR\" and\nthe month is January and\nthe day is between 1 and 15\n\n\nearly_january_weather <- weather %>% \n  filter(origin == \"EWR\" & month == 1 & day <= 15)\n\n\n\n\n\n\n\n🎯 Learning Check 2.9\n\n\n\n\n\nTake a look at both the weather and early_january_weather data frames by running View(weather) and View(early_january_weather). In what respect do these data frames differ?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.10\n\n\n\n\n\nView() the flights data frame again. Why does the time_hour variable uniquely identify the hour of the measurement whereas the hour variable does not?\n\n\n\n\n2.4.1 Linegraphs via geom_line\nLet’s plot a linegraph of hourly temperatures in early_january_weather by using geom_line() instead of geom_point() like we did for scatterplots:\n\nggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) +\n  geom_line()\n\n\n\n\nFigure 2.8: Hourly Temperature in Newark for January 1-15, 2013\n\n\n\n\nMuch as with the ggplot() code that created the scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 2.2, let’s break down the above code piece-by-piece in terms of the Grammar of Graphics:\n\nWithin the ggplot() function call, we specify two of the components of the Grammar of Graphics as arguments:\n\nThe data frame to be early_january_weather by setting data = early_january_weather\nThe aesthetic mapping by setting aes(x = time_hour, y = temp). Specifically:\n\nthe variable time_hour maps to the x position aesthetic.\nthe variable temp maps to the y position aesthetic\n\n\nWe add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object in question. In this case the geometric object is a line, set by specifying geom_line().\n\n\n\n\n\n\n\n🎯 Learning Check 2.11\n\n\n\n\n\nWhy should linegraphs be avoided when there is not a clear ordering of the horizontal axis?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.12\n\n\n\n\n\nWhy are linegraphs frequently used when time is the explanatory variable on the x-axis?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.13\n\n\n\n\n\nPlot a time series of a variable other than temp for Newark Airport in the first 15 days of January 2013.\n\n\n\n\n\n2.4.2 Summary\nLinegraphs, just like scatterplots, display the relationship between two numerical variables. However it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e. the explanatory variable) has an inherent ordering, like some notion of time."
  },
  {
    "objectID": "02-visualization.html#sec-histograms",
    "href": "02-visualization.html#sec-histograms",
    "title": "2  Data Visualization",
    "section": "2.5 5NG#3: Histograms",
    "text": "2.5 5NG#3: Histograms\nLet’s consider the temp variable in the weather data frame once again, but unlike with the linegraphs in Section 2.4, let’s say we don’t care about the relationship of temperature to time, but rather we only care about how the values of temp distribute. In other words:\n\nWhat are the smallest and largest values?\nWhat is the “center” value?\nHow do the values spread out?\nWhat are frequent and infrequent values?\n\nOne way to visualize this distribution of this single variable temp is to plot them on a horizontal line as we do in Figure 2.9:\n\n\n\n\n\nFigure 2.9: Plot of Hourly Temperature Recordings from NYC in 2013\n\n\n\n\nThis gives us a general idea of how the values of temp distribute: observe that temperatures vary from around 11°F up to 100°F. Furthermore, there appear to be more recorded temperatures between 40°F and 60°F than outside this range. However, because of the high degree of overlap in the points, it’s hard to get a sense of exactly how many values are between, say, 50°F and 55°F.\nWhat is commonly produced instead of the above plot is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows:\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nThen for each bin, we draw a bar whose height marks the corresponding count.\n\nLet’s drill-down on an example of a histogram, shown in @fig-histogramexample.\n\n\n\n\n\nFigure 2.10: Example histogram\n\n\n\n\nObserve that there are three bins of equal width between 30°F and 60°F, thus we have three bins of width 10°F each: one bin for the 30-40°F range, another bin for the 40-50°F range, and another bin for the 50-60°F range. Since:\n\nThe bin for the 30-40°F range has a height of around 5000, this histogram is telling us that around 5000 of the hourly temperature recordings are between 30°F and 40°F.\nThe bin for the 40-50°F range has a height of around 4300, this histogram is telling us that around 4300 of the hourly temperature recordings are between 40°F and 50°F.\nThe bin for the 50-60°F range has a height of around 3500, this histogram is telling us that around 3500 of the hourly temperature recordings are between 50°F and 60°F.\n\nThe remaining bins all have a similar interpretation.\n\n2.5.1 Histograms via geom_histogram\nLet’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram()\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 rows containing non-finite values (stat_bin).\n\n\n\n\n\nFigure 2.11: Histogram of hourly temperatures at three NYC airports\n\n\n\n\nLet’s unpack the messages R sent us first. The first message is telling us that the histogram was constructed using bins = 30, in other words 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change this default number of bins. The second message is telling us something similar to the warning message we received when we ran the code to create a scatterplot of departure and arrival delays for Alaska Airlines flights in Figure 2.2: that because one row has a missing NA value for temp, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case.\nNow’s let’s unpack the resulting histogram in Figure 2.11. Observe that values less than 25°F as well as values above 80°F are rather rare. However, because of the large number of bins, its hard to get a sense for which range of temperatures is covered by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram():\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(color = \"white\")\n\n\n\n\nFigure 2.12: Histogram of hourly temperatures at three NYC airports with white borders\n\n\n\n\nWe can now better associate ranges of temperatures to each of the bins. We can also vary the color of the bars by setting the fill argument. Run colors() to see all 657 possible choice of colors!\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\")\n\n\n\n\nFigure 2.13: Histogram of hourly temperatures at three NYC airports with white borders\n\n\n\n\n\n\n2.5.2 Adjusting the bins\nObserve in both Figure 2.12 and Figure 2.13 that in the 50-75°F range there appear to be roughly 8 bins. Thus each bin has width 25 divided by 8, or roughly 3.12°F which is not a very easily interpretable range to work with. Let’s now adjust the number of bins in our histogram in one of two methods:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nUsing the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows:\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(bins = 40, color = \"white\")\n\n\n\n\nFigure 2.14: Histogram with 40 bins\n\n\n\n\nUsing the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 10°F.\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 10, color = \"white\")\n\n\n\n\nFigure 2.15: Histogram with binwidth 10\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.14\n\n\n\n\n\nWhat does changing the number of bins from 30 to 40 tell us about the distribution of temperatures?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.15\n\n\n\n\n\nWould you classify the distribution of temperatures as symmetric or skewed?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.16\n\n\n\n\n\nWhat would you guess is the “center” value in this distribution? Why did you make that choice?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.17\n\n\n\n\n\nIs this data spread out greatly from the center or is it close? Why?\n\n\n\n\n\n2.5.3 Summary\nHistograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question."
  },
  {
    "objectID": "02-visualization.html#sec-facets",
    "href": "02-visualization.html#sec-facets",
    "title": "2  Data Visualization",
    "section": "2.6 Facets",
    "text": "2.6 Facets\nBefore continuing the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization of variables by another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ.\nFor example, suppose we were interested in looking at how the histogram of hourly temperature recordings at the three NYC airports we saw in Section 2.5 differed by month. We would “split” this histogram by the 12 possible months in a given year, in other words plot histograms of temp for each month. We do this by adding facet_wrap(~ month) layer.\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  facet_wrap(~ month)\n\n\n\n\nFigure 2.16: Faceted histogram\n\n\n\n\nNote the use of the tilde ~ before month in facet_wrap(). The tilde is required and you’ll receive the error Error in as.quoted(facets) : object 'month' not found if you don’t include it before month here. We can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted plot to have 4 rows instead of 3. Add the nrow = 4 argument to facet_wrap(~ month)\n\nggplot(data = weather, mapping = aes(x = temp)) +\n  geom_histogram(binwidth = 5, color = \"white\") +\n  facet_wrap(~ month, nrow = 4)\n\n\n\n\nFigure 2.17: Faceted histogram with 4 instead of 3 rows\n\n\n\n\nObserve in both Figure 2.16 and Figure 2.17 that as we might expect in the Northern Hemisphere, temperatures tend to be higher in the summer months, while they tend to be lower in the winter.\n\n\n\n\n\n\n🎯 Learning Check 2.18\n\n\n\n\n\nWhat other things do you notice about the faceted plot above? How does a faceted plot help us see relationships between two variables?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.19\n\n\n\n\n\nWhat do the numbers 1-12 correspond to in the plot above? What about 25, 50, 75, 100?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.20\n\n\n\n\n\nFor which types of data sets would these types of faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.21\n\n\n\n\n\nDoes the temp variable in the weather data set have a lot of variability? Why do you say that?"
  },
  {
    "objectID": "02-visualization.html#sec-boxplots",
    "href": "02-visualization.html#sec-boxplots",
    "title": "2  Data Visualization",
    "section": "2.7 5NG#4: Boxplots",
    "text": "2.7 5NG#4: Boxplots\nWhile faceted histograms are one visualization that allows us to compare distributions of a numerical variable split by another variable, another visualization that achieves this same goal are side-by-side boxplots. A boxplot is constructed from the information provided in the five-number summary of a numerical variable (see Appendix A). To keep things simple for now, let’s only consider hourly temperature recordings for the month of November in Figure 2.18.\n\n\n\n\n\nFigure 2.18: November temperatures\n\n\n\n\nThese 2141 observations have the following five-number summary:\n\nMinimum: 21.02°F\nFirst quartile AKA 25th percentile: 35.96°F\nMedian AKA second quartile AKA 50th percentile: 44.96°F\nThird quartile AKA 75th percentile: 51.98°F\nMaximum: 71.06°F\n\nLet’s mark these 5 values with dashed horizontal lines in Figure 2.19.\n\n\n\n\n\nFigure 2.19: November temperatures\n\n\n\n\nLet’s add the boxplot underneath these points and dashed horizontal lines in Figure 2.20.\n\n\n\n\n\nFigure 2.20: November temperatures\n\n\n\n\nWhat the boxplot does summarize the 2141 points by emphasizing that:\n\n25% of points (about 534 observations) fall below the bottom edge of the box, which is the first quartile of 35.96°F. In other words 25% of observations were colder than 35.96°F.\n25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 44.96°F. In other words 25% of observations were between 35.96 and 44.96°F and 50% of observations were colder than 44.96°F.\n25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 51.98°F. In other words 25% of observations were between 44.96 and 51.98°F and 75% of observations were colder than 51.98°F.\n25% of points fall over the top edge of the box. In other words 25% of observations were warmer than 51.98°F.\nThe middle 50% of points lie within the interquartile range between the first and third quartile of 51.98 - 35.96 = 16.02°F.\n\nLastly, for clarity’s sake let’s remove the points but keep the dashed horizontal lines in Figure 2.21.\n\n\n\n\n\nFigure 2.21: November temperatures\n\n\n\n\nWe can now better see the whiskers of the boxplot. They stick out from either end of the box all the way to the minimum and maximum observed temperatures of 21.02°F and 71.06°F respectively. However, the whiskers don’t always extend to the smallest and largest observed values. They in fact can extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box, in this case 1.5 \\(\\times\\) 16.02°F = 24.03°F from either end of the box. Any observed values outside this whiskers get marked with points called outliers, which we’ll see in the next section.\n\n2.7.1 Boxplots via geom_boxplot\nLet’s now create a side-by-side boxplot of hourly temperatures split by the 12 months as we did above with the faceted histograms. We do this by mapping the month variable to the x-position aesthetic, the temp variable to the y-position aesthetic, and by adding a geom_boxplot() layer:\n\nggplot(data = weather, mapping = aes(x = month, y = temp)) +\n  geom_boxplot()\n\n\n\n\nFigure 2.22: Invalid boxplot specification\n\n\n\n\nWarning messages:\n1: Continuous x aesthetic -- did you forget aes(group=...)? \n2: Removed 1 rows containing non-finite values (stat_boxplot). \nObserve in Figure 2.22 that this plot does not provide information about temperature separated by month. The warning messages clue us in as to why. The second warning message is identical to the warning message when plotting a histogram of hourly temperatures: that one of the values was recorded as NA missing. However, the first warning message is telling us that we have a “continuous”, or numerical variable, on the x-position aesthetic. Boxplots however require a categorical variable on the x-axis.\nWe can convert the numerical variable month into a categorical variable by using the factor() function. So after applying factor(month), month goes from having numerical values 1, 2, …, 12 to having labels “1”, “2”, …, “12.”\n\nggplot(data = weather, mapping = aes(x = factor(month), y = temp)) +\n  geom_boxplot()\n\n\n\n\nFigure 2.23: Temp by month boxplot\n\n\n\n\nThe resulting Figure 2.23 shows 12 separate “box and whiskers” plots with the features we saw earlier focusing only on November:\n\nThe “box” portions of this visualization represent the 1st quartile, the median AKA the 2nd quartile, and the 3rd quartile.\nThe “length” of each box, i.e. the value of the 3rd quartile minus the value of the 1st quartile, is the interquartile range. It is a measure of spread of the middle 50% of values, with longer boxes indicating more variability.\nThe “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed temperatures. The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability.\nThe dots representing values falling outside the whiskers are called outliers. These can be thought of as anomalous values.\n\nIt is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long. Looking at this plot we can see, as expected, that summer months (6 through 8) have higher median temperatures as evidenced by the higher solid lines in the middle of the boxes. We can easily compare temperatures across months by drawing imaginary horizontal lines across the plot. Furthermore, the height of the 12 boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of temperatures recorded in a given month.\n\n\n\n\n\n\n🎯 Learning Check 2.22\n\n\n\n\n\nWhat does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.23\n\n\n\n\n\nWhich months have the highest variability in temperature? What reasons can you give for this?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.24\n\n\n\n\n\nWe looked at the distribution of the numerical variable temp split by the numerical variable month that we converted to a categorical variable using the factor() function. Why would a boxplot of temp split by the numerical variable pressure similarly converted to a categorical variable using the factor() not be informative?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.25\n\n\n\n\n\nBoxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?\n\n\n\n\n\n2.7.2 Summary\nSide-by-side boxplots provide us with a way to compare and contrast the distribution of a quantitative variable across multiple levels of another categorical variable. One can see where the median falls across the different groups by looking at the center line in the boxes. To see how spread out the variable is across the different groups, look at both the width of the box and also how far the whiskers stretch out away from the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with points."
  },
  {
    "objectID": "02-visualization.html#sec-geombar",
    "href": "02-visualization.html#sec-geombar",
    "title": "2  Data Visualization",
    "section": "2.8 5NG#5: Barplots",
    "text": "2.8 5NG#5: Barplots\nBoth histograms and boxplots are tools to visualize the distribution of numerical variables. Another common task is visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories, also known as levels, of a categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot (also known as a barchart). One complication, however, is how your data is represented: is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges.\n\nfruits <- tibble(\n  fruit = c(\"apple\", \"apple\", \"orange\", \"apple\", \"orange\")\n  )\n\nfruits_counted <- tibble(\n  fruit = c(\"apple\", \"orange\"),\n  number = c(3, 2)\n  )\n\nWe see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually…\n\n\n# A tibble: 5 × 1\n  fruit \n  <chr> \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange\n\n\n… fruits_counted has a variable number which represents pre-counted values of each fruit.\n\n\n# A tibble: 2 × 2\n  fruit  number\n  <chr>   <dbl>\n1 apple       3\n2 orange      2\n\n\nDepending on how your categorical data is represented, you’ll need to use add a different geom layer to your ggplot() to create a barplot, as we now explore.\n\n2.8.1 Barplots via geom_bar or geom_col\nLet’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer.\n\nggplot(data = fruits, mapping = aes(x = fruit)) +\n  geom_bar()\n\n\n\n\nFigure 2.24: Barplot when counts are not pre-counted\n\n\n\n\nHowever, using the fruits_counted data frame where the fruit have been “pre-counted”, we map the fruit variable to the x-position aesthetic as with geom_bar(), but we also map the count variable to the y-position aesthetic, and add a geom_col() layer.\n\nggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) +\n  geom_col()\n\n\n\n\nFigure 2.25: Barplot when counts are pre-counted\n\n\n\n\nCompare the barplots in Figure 2.24 and Figure 2.25. They are identical because they reflect count of the same 5 fruit. However depending on how our data is saved, either pre-counted or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize is:\n\nIs not pre-counted in your data frame: use geom_bar().\nIs pre-counted in your data frame, use geom_col() with the y-position aesthetic mapped to the variable that has the counts.\n\nLet’s now go back to the flights data frame in the nycflights13 package and visualize the distribution of the categorical variable carrier. In other words, let’s visualize the number of domestic flights out of the three New York City airports each airline company flew in 2013. Recall from Section 1.4.3 when you first explored the flights data frame you saw that each row corresponds to a flight. In other words the flights data frame is more like the fruits data frame than the fruits_counted data frame above, and thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable carrier gets mapped to the x-position.\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\n\n\n\nFigure 2.26: Number of flights departing NYC in 2013 by airline using geom_bar()\n\n\n\n\nObserve in Figure 2.26 that United Air Lines (UA), JetBlue Airways (B6), and ExpressJet Airlines (EV) had the most flights depart New York City in 2013. If you don’t know which airlines correspond to which carrier codes, then run View(airlines) to see a directory of airlines. For example: AA is American Airlines; B6 is JetBlue Airways; DL is Delta Airlines; EV is ExpressJet Airlines; MQ is Envoy Air; while UA is United Airlines.\nAlternatively, say you had a data frame flights_counted where the number of flights for each carrier was pre-counted like in Table 2.3.\n\n\n\n\nTable 2.3: Number of flights pre-counted for each carrier\n\n\ncarrier\nnumber\n\n\n\n\nUA\n58665\n\n\nB6\n54635\n\n\nEV\n54173\n\n\nDL\n48110\n\n\nAA\n32729\n\n\nMQ\n26397\n\n\nUS\n20536\n\n\n9E\n18460\n\n\nWN\n12275\n\n\nVX\n5162\n\n\nFL\n3260\n\n\nAS\n714\n\n\nF9\n685\n\n\nYV\n601\n\n\nHA\n342\n\n\nOO\n32\n\n\n\n\n\n\nIn order to create a barplot visualizing the distribution of the categorical variable carrier in this case, we would use geom_col() instead with x mapped to carrier and y mapped to number as seen below. The resulting barplot would be identical to Figure 2.26.\n\nggplot(data = flights_table, mapping = aes(x = carrier, y = number)) +\n  geom_col()\n\n\n\n\n\n\n\n🎯 Learning Check 2.26\n\n\n\n\n\nWhy are histograms inappropriate for visualizing categorical variables?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.27\n\n\n\n\n\nWhat is the difference between histograms and barplots?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.28\n\n\n\n\n\nHow many Envoy Air flights departed NYC in 2013?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.29\n\n\n\n\n\nWhat was the seventh highest airline in terms of departed flights from NYC in 2013? How could we better present the table to get this answer quickly?\n\n\n\n\n\n2.8.2 Must avoid pie charts!\nUnfortunately, one of the most common plots seen today for categorical data is the pie chart. While they may seem harmless enough, they actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book “Creating More Effective Graphs” (Robbins 2013), we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine relative size of one piece of the pie compared to another.\nLet’s examine the same data used in our previous barplot of the number of flights departing NYC by airline in Figure 2.26, but this time we will use a pie chart in Figure 2.27.\n\n\n\n\n\nFigure 2.27: The dreaded pie chart\n\n\n\n\nTry to answer the following questions:\n\nHow much larger the portion of the pie is for ExpressJet Airlines (EV) compared to US Airways (US),\nWhat the third largest carrier is in terms of departing flights, and\nHow many carriers have fewer flights than United Airlines (UA)?\n\nWhile it is quite difficult to answer these questions when looking at the pie chart in Figure 2.27, we can much more easily answer these questions using the barchart in Figure Figure 2.26. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons between categories must be made by comparing angles.\nThere may be one exception of a pie chart not to avoid courtesy Nathan Yau at FlowingData.com, but we will leave this for the reader to decide:\n\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.30\n\n\n\n\n\nWhy should pie charts be avoided and replaced by barplots?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.31\n\n\n\n\n\nWhy do you think people continue to use pie charts?\n\n\n\n\n\n2.8.3 Two categorical variables\nBarplots are the go-to way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of outgoing domestic flights from NYC by carrier and origin, or in other words the number of flights for each carrier and origin combination. For example, the number of WestJet flights from JFK, the number of WestJet flights from LGA, the number of WestJet flights from EWR, the number of American Airlines flights from JFK, and so on. Recall the ggplot() code that created the barplot of carrier frequency in Figure 2.26:\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe can now map the additional variable origin by adding a fill = origin inside the aes() aesthetic mapping; the fill aesthetic of any bar corresponds to the color used to fill the bars.\n\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar()\n\n\n\n\nFigure 2.28: Stacked barplot comparing the number of flights by carrier and origin\n\n\n\n\nFigure 2.28 is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of flights from each origin airport between the carriers.\nBefore we continue, let’s address some common points of confusion amongst new R users. First, note that fill is another aesthetic mapping much like x-position; thus it must be included within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will yield an error. This is a fairly common error that new ggplot users make:\n\nggplot(data = flights, mapping = aes(x = carrier), fill = origin) +\n  geom_bar()\n\nSecond, the fill aesthetic corresponds to the color used to fill the bars, while the color aesthetic corresponds to the color of the outline of the bars. Observe in Figure 2.29 that mapping origin to color and not fill yields grey bars with different colored outlines.\n\nggplot(data = flights, mapping = aes(x = carrier, color = origin)) +\n  geom_bar()\n\n\n\n\nFigure 2.29: Stacked barplot with color aesthetic used instead of fill\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.32\n\n\n\n\n\nWhat kinds of questions are not easily answered by looking at the above figure?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.33\n\n\n\n\n\nWhat can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?\n\n\n\nAnother alternative to stacked barplots are side-by-side barplots, also known as a dodged barplot. The code to created a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot.\n\nggplot(data = flights, mapping = aes(x = carrier, fill = origin)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nFigure 2.30: Side-by-side AKA dodged barplot comparing the number of flights by carrier and origin\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.34\n\n\n\n\n\nWhy might the side-by-side (AKA dodged) barplot be preferable to a stacked barplot in this case?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.35\n\n\n\n\n\nWhat are the disadvantages of using a side-by-side (AKA dodged) barplot, in general?\n\n\n\nLastly, another type of barplot is a faceted barplot. Recall in Section 2.6 we visualized the distribution of hourly temperatures at the 3 NYC airports split by month using facets. We apply the same principle to our barplot visualizing the frequency of carrier split by origin: instead of mapping origin\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar() +\n  facet_wrap(~ origin, ncol = 1)\n\n\n\n\nFigure 2.31: Faceted barplot comparing the number of flights by carrier and origin\n\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.36\n\n\n\n\n\nWhy is the faceted barplot preferred to the side-by-side and stacked barplots in this case?\n\n\n\n\n\n\n\n\n\n🎯 Learning Check 2.37\n\n\n\n\n\nWhat information about the different carriers at different airports is more easily seen in the faceted barplot?\n\n\n\n\n\n2.8.4 Summary\nBarplots are the preferred way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories called levels occur. They are easy to understand and make it easy to make comparisons across levels. When trying to visualize two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the joint distribution you are trying to emphasize, you will need to make a choice between these three types of barplots."
  },
  {
    "objectID": "02-visualization.html#sec-data-vis-conclusion",
    "href": "02-visualization.html#sec-data-vis-conclusion",
    "title": "2  Data Visualization",
    "section": "2.9 Conclusion",
    "text": "2.9 Conclusion\n\n2.9.1 Summary table\nLet’s recap all five of the Five Named Graphs (5NG) in Table 2.4 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package.\n\n\n\n\nTable 2.4: Summary of 5NG\n\n\n\n\n\n\n\n\nNamed graph\nShows\nGeometric object\nNotes\n\n\n\n\nScatterplot\nRelationship between 2 numerical variables\ngeom_point()\n\n\n\nLinegraph\nRelationship between 2 numerical variables\ngeom_line()\nUsed when there is a sequential order to x-variable e.g. time\n\n\nHistogram\nDistribution of 1 numerical variable\ngeom_histogram()\nFacetted histograms show the distribution of 1 numerical variable split by the values of another variable\n\n\nBoxplot\nDistribution of 1 numerical variable split by the values of another variable\ngeom_boxplot()\n\n\n\nBarplot\nDistribution of 1 categorical variable\ngeom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted\nStacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables\n\n\n\n\n\n\n\n\n2.9.2 Argument specification\nRun the following two segments of code. First this:\n\nggplot(data = flights, mapping = aes(x = carrier)) +\n  geom_bar()\n\nthen this:\n\nggplot(flights, aes(x = carrier)) +\n  geom_bar()\n\nYou’ll notice that that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() by default assumes that the data argument comes first and the mapping argument comes second. So as long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =.\nGoing forward for the rest of this book, all ggplot() will be like the second segment above: with the data = and mapping = explicit naming of the argument omitted and the default ordering of arguments respected.\n\n\n2.9.3 Additional resources\nIf you want to further unlock the power of the ggplot2 package for data visualization, we suggest you that you check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, in particular the many more than the 5 geom geometric objects we covered in this Chapter, while providing quick and easy to read visual descriptions.\nYou can access this cheatsheet by going to the RStudio Menu Bar -> Help -> Cheatsheets -> “Data Visualization with ggplot2”:\n\n\n\n\n\n\n\n2.9.4 What’s to come\nRecall in Figure 2.2 in Section 2.3 we visualized the relationship between departure delay and arrival delay for Alaska Airlines flights. This necessitated paring or filtering down the flights data frame to a new data frame alaska_flights consisting of only carrier == AS flights first:\n\nalaska_flights <- flights %>% \n  filter(carrier == \"AS\")\n\nggplot(data = alaska_flights, mapping = aes(x = dep_delay, y = arr_delay)) + \n  geom_point()\n\nFurthermore recall in Figure 2.8 in Section 2.4 we visualized hourly temperature recordings at Newark airport only for the first 15 days of January 2013. This necessitated paring or fitlering down the weather data frame to a new data frame early_january_weather consisting of hourly temperature recordings only for origin == \"EWR\", month == 1, and day less than or equal to 15 first:\n\nearly_january_weather <- weather %>% \n  filter(origin == \"EWR\" & month == 1 & day <= 15)\n\nggplot(data = early_january_weather, mapping = aes(x = time_hour, y = temp)) +\n  geom_line()\n\nThese two code segments were a preview of Chapter 3 on data wrangling where we’ll delve further into the dplyr package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, the two code segments used the filter() function to create new data frames (alaska_flights and early_january_weather) by choosing only a subset of rows of existing data frames (flights and weather). In this next chapter, we’ll formally introduce the filter() and other data wrangling functions as well as the pipe operator %>% which allows you to combine multiple data wrangling actions into a single sequential chain of actions. On to Chapter 3 on data wrangling!\n\n\n\n\nGrolemund, Garrett, and Hadley Wickham. 2016. R for Data Science. http://r4ds.had.co.nz/.\n\n\nRobbins, Naomi. 2013. Creating More Effective Graphs. Chart House.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and Computing). Secaucus, NJ, USA: Springer-Verlag New York, Inc."
  },
  {
    "objectID": "03-wrangling.html",
    "href": "03-wrangling.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "04-tidy.html",
    "href": "04-tidy.html",
    "title": "4  Data Importing & “Tidy Data”",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "05-regression.html",
    "href": "05-regression.html",
    "title": "5  Basic Regression",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "06-multiple-regression.html",
    "href": "06-multiple-regression.html",
    "title": "6  Multiple Regression",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "07-causality.html",
    "href": "07-causality.html",
    "title": "7  Randomization and Causality",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "08-populations.html",
    "href": "08-populations.html",
    "title": "8  Populations and Generalizability",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "09-sampling-distributions.html",
    "href": "09-sampling-distributions.html",
    "title": "9  Sampling Distributions",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "10-confidence-intervals.html",
    "href": "10-confidence-intervals.html",
    "title": "10  Confidence Intervals",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "11-p-values.html",
    "href": "11-p-values.html",
    "title": "11  P-values",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "12-hypothesis-tests.html",
    "href": "12-hypothesis-tests.html",
    "title": "12  Hypothesis tests",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "13-putting-together.html",
    "href": "13-putting-together.html",
    "title": "13  Putting it all together",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grolemund, Garrett, and Hadley Wickham. 2016. R for Data\nScience. http://r4ds.had.co.nz/.\n\n\nIsmay, Chester. 2016. Getting Used to r, RStudio, and r\nMarkdown. http://ismayc.github.io/rbasics-book.\n\n\nRobbins, Naomi. 2013. Creating More Effective Graphs. Chart\nHouse.\n\n\nWickham, Hadley. 2021. Nycflights13: Flights That Departed NYC in\n2013. https://github.com/hadley/nycflights13.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics (Statistics and\nComputing). Secaucus, NJ, USA: Springer-Verlag New York, Inc."
  },
  {
    "objectID": "statistical-background.html",
    "href": "statistical-background.html",
    "title": "Appendix A — Statistical Background",
    "section": "",
    "text": "Under Construction\n\n\n\nCurrently working on content transfer from previous version of the book."
  }
]